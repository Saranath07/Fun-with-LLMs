Training Log - Started at 2024-11-17 15:55:58.146206
Model: LLaMA-2 1B with LoRA
Task: Proposal Generation Fine-tuning
==================================================


Epoch 1/250
==================================================
[00:01:08] Step 50/125000:
  Training Loss: 2.827
  Perplexity: 16.891
  Learning Rate: 2.00e-04
  GPU Memory Used: 12151MB

[00:02:17] Step 100/125000:
  Training Loss: 2.822
  Perplexity: 16.817
  Learning Rate: 2.00e-04
  GPU Memory Used: 12017MB

[00:03:27] Step 150/125000:
  Training Loss: 2.790
  Perplexity: 16.285
  Learning Rate: 2.00e-04
  GPU Memory Used: 11954MB

[00:04:35] Step 200/125000:
  Training Loss: 2.712
  Perplexity: 15.054
  Learning Rate: 2.00e-04
  GPU Memory Used: 12149MB

[00:05:42] Step 250/125000:
  Training Loss: 2.560
  Perplexity: 12.936
  Learning Rate: 2.00e-04
  GPU Memory Used: 11840MB

[00:06:53] Step 300/125000:
  Training Loss: 2.841
  Perplexity: 17.138
  Learning Rate: 2.00e-04
  GPU Memory Used: 12142MB

[00:08:02] Step 350/125000:
  Training Loss: 3.029
  Perplexity: 20.671
  Learning Rate: 2.00e-04
  GPU Memory Used: 11916MB

[00:09:12] Step 400/125000:
  Training Loss: 2.875
  Perplexity: 17.733
  Learning Rate: 2.00e-04
  GPU Memory Used: 12086MB

[00:10:20] Step 450/125000:
  Training Loss: 2.859
  Perplexity: 17.449
  Learning Rate: 2.00e-04
  GPU Memory Used: 11896MB

[00:11:27] Step 500/125000:
  Training Loss: 2.908
  Perplexity: 18.319
  Learning Rate: 2.00e-04
  GPU Memory Used: 11823MB


Evaluation Results:
  Validation Loss: 2.754
  Validation Perplexity: 15.698

Saving model checkpoint: proposal_model/checkpoint-500


Epoch 2/250
==================================================
[00:12:38] Step 550/125000:
  Training Loss: 2.850
  Perplexity: 17.283
  Learning Rate: 2.00e-04
  GPU Memory Used: 12054MB

[00:13:47] Step 600/125000:
  Training Loss: 2.871
  Perplexity: 17.654
  Learning Rate: 2.00e-04
  GPU Memory Used: 11961MB

[00:14:55] Step 650/125000:
  Training Loss: 2.902
  Perplexity: 18.213
  Learning Rate: 2.00e-04
  GPU Memory Used: 11824MB

[00:16:06] Step 700/125000:
  Training Loss: 2.817
  Perplexity: 16.731
  Learning Rate: 2.00e-04
  GPU Memory Used: 12106MB

[00:17:14] Step 750/125000:
  Training Loss: 2.665
  Perplexity: 14.366
  Learning Rate: 2.00e-04
  GPU Memory Used: 12089MB

[00:18:24] Step 800/125000:
  Training Loss: 3.006
  Perplexity: 20.203
  Learning Rate: 2.00e-04
  GPU Memory Used: 11998MB

[00:19:33] Step 850/125000:
  Training Loss: 2.858
  Perplexity: 17.428
  Learning Rate: 2.00e-04
  GPU Memory Used: 12150MB

[00:20:43] Step 900/125000:
  Training Loss: 2.907
  Perplexity: 18.305
  Learning Rate: 2.00e-04
  GPU Memory Used: 11992MB

[00:21:54] Step 950/125000:
  Training Loss: 2.892
  Perplexity: 18.024
  Learning Rate: 2.00e-04
  GPU Memory Used: 11826MB

[00:23:05] Step 1000/125000:
  Training Loss: 2.884
  Perplexity: 17.881
  Learning Rate: 2.00e-04
  GPU Memory Used: 12166MB


Evaluation Results:
  Validation Loss: 2.875
  Validation Perplexity: 17.719

Saving model checkpoint: proposal_model/checkpoint-1000


Epoch 3/250
==================================================
[00:24:14] Step 1050/125000:
  Training Loss: 2.844
  Perplexity: 17.184
  Learning Rate: 2.00e-04
  GPU Memory Used: 11863MB

[00:25:24] Step 1100/125000:
  Training Loss: 2.703
  Perplexity: 14.927
  Learning Rate: 2.00e-04
  GPU Memory Used: 11932MB

[00:26:34] Step 1150/125000:
  Training Loss: 3.003
  Perplexity: 20.146
  Learning Rate: 2.00e-04
  GPU Memory Used: 11928MB

[00:27:42] Step 1200/125000:
  Training Loss: 2.768
  Perplexity: 15.932
  Learning Rate: 2.00e-04
  GPU Memory Used: 12063MB

[00:28:49] Step 1250/125000:
  Training Loss: 2.736
  Perplexity: 15.418
  Learning Rate: 2.00e-04
  GPU Memory Used: 11931MB

[00:29:59] Step 1300/125000:
  Training Loss: 2.931
  Perplexity: 18.741
  Learning Rate: 2.00e-04
  GPU Memory Used: 11844MB

[00:31:08] Step 1350/125000:
  Training Loss: 2.880
  Perplexity: 17.807
  Learning Rate: 2.00e-04
  GPU Memory Used: 12129MB

[00:32:18] Step 1400/125000:
  Training Loss: 2.848
  Perplexity: 17.260
  Learning Rate: 2.00e-04
  GPU Memory Used: 12069MB

[00:33:27] Step 1450/125000:
  Training Loss: 2.861
  Perplexity: 17.485
  Learning Rate: 2.00e-04
  GPU Memory Used: 12059MB

[00:34:34] Step 1500/125000:
  Training Loss: 2.727
  Perplexity: 15.282
  Learning Rate: 2.00e-04
  GPU Memory Used: 12033MB


Evaluation Results:
  Validation Loss: 2.949
  Validation Perplexity: 19.087

Saving model checkpoint: proposal_model/checkpoint-1500


Epoch 4/250
==================================================
[00:35:44] Step 1550/125000:
  Training Loss: 2.993
  Perplexity: 19.947
  Learning Rate: 2.00e-04
  GPU Memory Used: 12091MB

[00:36:54] Step 1600/125000:
  Training Loss: 2.901
  Perplexity: 18.199
  Learning Rate: 2.00e-04
  GPU Memory Used: 11841MB

[00:38:01] Step 1650/125000:
  Training Loss: 2.871
  Perplexity: 17.659
  Learning Rate: 2.00e-04
  GPU Memory Used: 12068MB

[00:39:09] Step 1700/125000:
  Training Loss: 2.935
  Perplexity: 18.830
  Learning Rate: 2.00e-04
  GPU Memory Used: 12171MB

[00:40:17] Step 1750/125000:
  Training Loss: 2.974
  Perplexity: 19.568
  Learning Rate: 2.00e-04
  GPU Memory Used: 11964MB

[00:41:26] Step 1800/125000:
  Training Loss: 2.811
  Perplexity: 16.629
  Learning Rate: 2.00e-04
  GPU Memory Used: 12158MB

[00:42:33] Step 1850/125000:
  Training Loss: 2.689
  Perplexity: 14.719
  Learning Rate: 2.00e-04
  GPU Memory Used: 12066MB

[00:43:42] Step 1900/125000:
  Training Loss: 2.960
  Perplexity: 19.303
  Learning Rate: 2.00e-04
  GPU Memory Used: 12017MB

[00:44:50] Step 1950/125000:
  Training Loss: 2.599
  Perplexity: 13.449
  Learning Rate: 2.00e-04
  GPU Memory Used: 12082MB

[00:45:59] Step 2000/125000:
  Training Loss: 2.871
  Perplexity: 17.649
  Learning Rate: 2.00e-04
  GPU Memory Used: 12178MB


Evaluation Results:
  Validation Loss: 2.795
  Validation Perplexity: 16.371

Saving model checkpoint: proposal_model/checkpoint-2000


Epoch 5/250
==================================================
[00:47:10] Step 2050/125000:
  Training Loss: 2.728
  Perplexity: 15.303
  Learning Rate: 2.00e-04
  GPU Memory Used: 12167MB

[00:48:19] Step 2100/125000:
  Training Loss: 2.705
  Perplexity: 14.961
  Learning Rate: 2.00e-04
  GPU Memory Used: 11843MB

[00:49:30] Step 2150/125000:
  Training Loss: 2.652
  Perplexity: 14.188
  Learning Rate: 2.00e-04
  GPU Memory Used: 12178MB

[00:50:39] Step 2200/125000:
  Training Loss: 2.812
  Perplexity: 16.644
  Learning Rate: 2.00e-04
  GPU Memory Used: 12118MB

[00:51:48] Step 2250/125000:
  Training Loss: 2.723
  Perplexity: 15.221
  Learning Rate: 2.00e-04
  GPU Memory Used: 12109MB

[00:52:57] Step 2300/125000:
  Training Loss: 2.825
  Perplexity: 16.857
  Learning Rate: 2.00e-04
  GPU Memory Used: 12196MB

[00:54:07] Step 2350/125000:
  Training Loss: 2.755
  Perplexity: 15.719
  Learning Rate: 2.00e-04
  GPU Memory Used: 12114MB

[00:55:16] Step 2400/125000:
  Training Loss: 2.692
  Perplexity: 14.758
  Learning Rate: 2.00e-04
  GPU Memory Used: 12033MB

[00:56:26] Step 2450/125000:
  Training Loss: 2.801
  Perplexity: 16.454
  Learning Rate: 2.00e-04
  GPU Memory Used: 12054MB

[00:57:35] Step 2500/125000:
  Training Loss: 2.817
  Perplexity: 16.728
  Learning Rate: 2.00e-04
  GPU Memory Used: 12090MB


Evaluation Results:
  Validation Loss: 2.663
  Validation Perplexity: 14.339

Saving model checkpoint: proposal_model/checkpoint-2500


Epoch 6/250
==================================================
[00:58:44] Step 2550/125000:
  Training Loss: 2.789
  Perplexity: 16.270
  Learning Rate: 2.00e-04
  GPU Memory Used: 11901MB

[00:59:52] Step 2600/125000:
  Training Loss: 2.788
  Perplexity: 16.250
  Learning Rate: 2.00e-04
  GPU Memory Used: 12033MB

[01:01:00] Step 2650/125000:
  Training Loss: 2.783
  Perplexity: 16.167
  Learning Rate: 2.00e-04
  GPU Memory Used: 12045MB

[01:02:11] Step 2700/125000:
  Training Loss: 2.873
  Perplexity: 17.682
  Learning Rate: 2.00e-04
  GPU Memory Used: 11852MB

[01:03:20] Step 2750/125000:
  Training Loss: 2.699
  Perplexity: 14.870
  Learning Rate: 2.00e-04
  GPU Memory Used: 12102MB

[01:04:27] Step 2800/125000:
  Training Loss: 2.771
  Perplexity: 15.982
  Learning Rate: 2.00e-04
  GPU Memory Used: 11867MB

[01:05:37] Step 2850/125000:
  Training Loss: 2.810
  Perplexity: 16.612
  Learning Rate: 2.00e-04
  GPU Memory Used: 12141MB

[01:06:44] Step 2900/125000:
  Training Loss: 2.736
  Perplexity: 15.418
  Learning Rate: 2.00e-04
  GPU Memory Used: 11872MB

[01:07:52] Step 2950/125000:
  Training Loss: 2.586
  Perplexity: 13.276
  Learning Rate: 2.00e-04
  GPU Memory Used: 11943MB

[01:09:02] Step 3000/125000:
  Training Loss: 2.706
  Perplexity: 14.963
  Learning Rate: 2.00e-04
  GPU Memory Used: 11961MB


Evaluation Results:
  Validation Loss: 3.011
  Validation Perplexity: 20.305

Saving model checkpoint: proposal_model/checkpoint-3000


Epoch 7/250
==================================================
[01:10:10] Step 3050/125000:
  Training Loss: 2.778
  Perplexity: 16.094
  Learning Rate: 2.00e-04
  GPU Memory Used: 12019MB

[01:11:20] Step 3100/125000:
  Training Loss: 2.785
  Perplexity: 16.193
  Learning Rate: 2.00e-04
  GPU Memory Used: 12067MB

[01:12:29] Step 3150/125000:
  Training Loss: 2.894
  Perplexity: 18.074
  Learning Rate: 2.00e-04
  GPU Memory Used: 11920MB

[01:13:39] Step 3200/125000:
  Training Loss: 2.776
  Perplexity: 16.052
  Learning Rate: 2.00e-04
  GPU Memory Used: 11916MB

[01:14:48] Step 3250/125000:
  Training Loss: 2.791
  Perplexity: 16.296
  Learning Rate: 2.00e-04
  GPU Memory Used: 11846MB

[01:15:56] Step 3300/125000:
  Training Loss: 2.741
  Perplexity: 15.505
  Learning Rate: 2.00e-04
  GPU Memory Used: 11961MB

[01:17:05] Step 3350/125000:
  Training Loss: 2.782
  Perplexity: 16.156
  Learning Rate: 2.00e-04
  GPU Memory Used: 11926MB

[01:18:13] Step 3400/125000:
  Training Loss: 2.569
  Perplexity: 13.055
  Learning Rate: 2.00e-04
  GPU Memory Used: 12121MB

[01:19:23] Step 3450/125000:
  Training Loss: 2.753
  Perplexity: 15.686
  Learning Rate: 2.00e-04
  GPU Memory Used: 12145MB

[01:20:30] Step 3500/125000:
  Training Loss: 2.691
  Perplexity: 14.741
  Learning Rate: 2.00e-04
  GPU Memory Used: 11819MB


Evaluation Results:
  Validation Loss: 2.891
  Validation Perplexity: 18.005

Saving model checkpoint: proposal_model/checkpoint-3500


Epoch 8/250
==================================================
[01:21:39] Step 3550/125000:
  Training Loss: 2.915
  Perplexity: 18.449
  Learning Rate: 2.00e-04
  GPU Memory Used: 11976MB

[01:22:47] Step 3600/125000:
  Training Loss: 2.560
  Perplexity: 12.939
  Learning Rate: 2.00e-04
  GPU Memory Used: 11943MB

[01:23:55] Step 3650/125000:
  Training Loss: 2.917
  Perplexity: 18.490
  Learning Rate: 2.00e-04
  GPU Memory Used: 11942MB

[01:25:05] Step 3700/125000:
  Training Loss: 2.917
  Perplexity: 18.484
  Learning Rate: 2.00e-04
  GPU Memory Used: 12110MB

[01:26:15] Step 3750/125000:
  Training Loss: 2.818
  Perplexity: 16.744
  Learning Rate: 2.00e-04
  GPU Memory Used: 11933MB

[01:27:22] Step 3800/125000:
  Training Loss: 2.516
  Perplexity: 12.379
  Learning Rate: 2.00e-04
  GPU Memory Used: 12004MB

[01:28:31] Step 3850/125000:
  Training Loss: 2.843
  Perplexity: 17.161
  Learning Rate: 2.00e-04
  GPU Memory Used: 12111MB

[01:29:39] Step 3900/125000:
  Training Loss: 2.604
  Perplexity: 13.520
  Learning Rate: 2.00e-04
  GPU Memory Used: 12050MB

[01:30:48] Step 3950/125000:
  Training Loss: 2.663
  Perplexity: 14.343
  Learning Rate: 2.00e-04
  GPU Memory Used: 12110MB

[01:31:58] Step 4000/125000:
  Training Loss: 2.718
  Perplexity: 15.146
  Learning Rate: 1.99e-04
  GPU Memory Used: 12086MB


Evaluation Results:
  Validation Loss: 2.885
  Validation Perplexity: 17.905

Saving model checkpoint: proposal_model/checkpoint-4000


Epoch 9/250
==================================================
[01:33:04] Step 4050/125000:
  Training Loss: 2.751
  Perplexity: 15.651
  Learning Rate: 1.99e-04
  GPU Memory Used: 11880MB

[01:34:13] Step 4100/125000:
  Training Loss: 2.711
  Perplexity: 15.042
  Learning Rate: 1.99e-04
  GPU Memory Used: 11982MB

[01:35:22] Step 4150/125000:
  Training Loss: 2.651
  Perplexity: 14.164
  Learning Rate: 1.99e-04
  GPU Memory Used: 12115MB

[01:36:31] Step 4200/125000:
  Training Loss: 2.625
  Perplexity: 13.804
  Learning Rate: 1.99e-04
  GPU Memory Used: 12107MB

[01:37:39] Step 4250/125000:
  Training Loss: 2.534
  Perplexity: 12.607
  Learning Rate: 1.99e-04
  GPU Memory Used: 11821MB

[01:38:49] Step 4300/125000:
  Training Loss: 2.692
  Perplexity: 14.763
  Learning Rate: 1.99e-04
  GPU Memory Used: 11920MB

[01:40:00] Step 4350/125000:
  Training Loss: 2.597
  Perplexity: 13.423
  Learning Rate: 1.99e-04
  GPU Memory Used: 12115MB

[01:41:10] Step 4400/125000:
  Training Loss: 2.589
  Perplexity: 13.310
  Learning Rate: 1.99e-04
  GPU Memory Used: 11831MB

[01:42:19] Step 4450/125000:
  Training Loss: 2.826
  Perplexity: 16.871
  Learning Rate: 1.99e-04
  GPU Memory Used: 11893MB

[01:43:28] Step 4500/125000:
  Training Loss: 2.706
  Perplexity: 14.966
  Learning Rate: 1.99e-04
  GPU Memory Used: 11822MB


Evaluation Results:
  Validation Loss: 2.854
  Validation Perplexity: 17.353

Saving model checkpoint: proposal_model/checkpoint-4500


Epoch 10/250
==================================================
[01:44:38] Step 4550/125000:
  Training Loss: 2.759
  Perplexity: 15.783
  Learning Rate: 1.99e-04
  GPU Memory Used: 12097MB

[01:45:46] Step 4600/125000:
  Training Loss: 2.416
  Perplexity: 11.205
  Learning Rate: 1.99e-04
  GPU Memory Used: 11966MB

[01:46:55] Step 4650/125000:
  Training Loss: 2.601
  Perplexity: 13.477
  Learning Rate: 1.99e-04
  GPU Memory Used: 12093MB

[01:48:06] Step 4700/125000:
  Training Loss: 2.749
  Perplexity: 15.623
  Learning Rate: 1.99e-04
  GPU Memory Used: 12056MB

[01:49:16] Step 4750/125000:
  Training Loss: 2.603
  Perplexity: 13.506
  Learning Rate: 1.99e-04
  GPU Memory Used: 11908MB

[01:50:25] Step 4800/125000:
  Training Loss: 2.602
  Perplexity: 13.492
  Learning Rate: 1.99e-04
  GPU Memory Used: 12182MB

[01:51:35] Step 4850/125000:
  Training Loss: 2.819
  Perplexity: 16.768
  Learning Rate: 1.99e-04
  GPU Memory Used: 12006MB

[01:52:46] Step 4900/125000:
  Training Loss: 2.636
  Perplexity: 13.956
  Learning Rate: 1.99e-04
  GPU Memory Used: 12064MB

[01:53:54] Step 4950/125000:
  Training Loss: 2.623
  Perplexity: 13.783
  Learning Rate: 1.99e-04
  GPU Memory Used: 11993MB

[01:55:02] Step 5000/125000:
  Training Loss: 2.743
  Perplexity: 15.526
  Learning Rate: 1.99e-04
  GPU Memory Used: 12148MB


Evaluation Results:
  Validation Loss: 2.760
  Validation Perplexity: 15.802

Saving model checkpoint: proposal_model/checkpoint-5000


Epoch 11/250
==================================================
[01:56:12] Step 5050/125000:
  Training Loss: 2.669
  Perplexity: 14.428
  Learning Rate: 1.99e-04
  GPU Memory Used: 12103MB

[01:57:20] Step 5100/125000:
  Training Loss: 2.770
  Perplexity: 15.955
  Learning Rate: 1.99e-04
  GPU Memory Used: 12102MB

[01:58:30] Step 5150/125000:
  Training Loss: 2.725
  Perplexity: 15.251
  Learning Rate: 1.99e-04
  GPU Memory Used: 11936MB

[01:59:40] Step 5200/125000:
  Training Loss: 2.718
  Perplexity: 15.150
  Learning Rate: 1.99e-04
  GPU Memory Used: 11882MB

[02:00:48] Step 5250/125000:
  Training Loss: 2.709
  Perplexity: 15.009
  Learning Rate: 1.99e-04
  GPU Memory Used: 12032MB

[02:01:58] Step 5300/125000:
  Training Loss: 2.604
  Perplexity: 13.512
  Learning Rate: 1.99e-04
  GPU Memory Used: 11863MB

[02:03:06] Step 5350/125000:
  Training Loss: 2.656
  Perplexity: 14.233
  Learning Rate: 1.99e-04
  GPU Memory Used: 12176MB

[02:04:15] Step 5400/125000:
  Training Loss: 2.505
  Perplexity: 12.248
  Learning Rate: 1.99e-04
  GPU Memory Used: 12049MB

[02:05:24] Step 5450/125000:
  Training Loss: 2.665
  Perplexity: 14.370
  Learning Rate: 1.99e-04
  GPU Memory Used: 12041MB

[02:06:35] Step 5500/125000:
  Training Loss: 2.800
  Perplexity: 16.446
  Learning Rate: 1.99e-04
  GPU Memory Used: 11890MB


Evaluation Results:
  Validation Loss: 2.935
  Validation Perplexity: 18.814

Saving model checkpoint: proposal_model/checkpoint-5500


Epoch 12/250
==================================================
[02:07:44] Step 5550/125000:
  Training Loss: 2.779
  Perplexity: 16.100
  Learning Rate: 1.99e-04
  GPU Memory Used: 12034MB

[02:08:52] Step 5600/125000:
  Training Loss: 2.656
  Perplexity: 14.245
  Learning Rate: 1.99e-04
  GPU Memory Used: 12080MB

[02:10:00] Step 5650/125000:
  Training Loss: 2.574
  Perplexity: 13.123
  Learning Rate: 1.99e-04
  GPU Memory Used: 12171MB

[02:11:10] Step 5700/125000:
  Training Loss: 2.536
  Perplexity: 12.624
  Learning Rate: 1.99e-04
  GPU Memory Used: 12094MB

[02:12:21] Step 5750/125000:
  Training Loss: 2.596
  Perplexity: 13.409
  Learning Rate: 1.99e-04
  GPU Memory Used: 11963MB

[02:13:30] Step 5800/125000:
  Training Loss: 2.875
  Perplexity: 17.717
  Learning Rate: 1.99e-04
  GPU Memory Used: 11994MB

[02:14:38] Step 5850/125000:
  Training Loss: 2.572
  Perplexity: 13.090
  Learning Rate: 1.99e-04
  GPU Memory Used: 12176MB

[02:15:46] Step 5900/125000:
  Training Loss: 2.612
  Perplexity: 13.629
  Learning Rate: 1.99e-04
  GPU Memory Used: 11944MB

[02:16:52] Step 5950/125000:
  Training Loss: 2.526
  Perplexity: 12.506
  Learning Rate: 1.99e-04
  GPU Memory Used: 11964MB

[02:18:02] Step 6000/125000:
  Training Loss: 2.685
  Perplexity: 14.658
  Learning Rate: 1.99e-04
  GPU Memory Used: 11800MB


Evaluation Results:
  Validation Loss: 2.687
  Validation Perplexity: 14.692

Saving model checkpoint: proposal_model/checkpoint-6000


Epoch 13/250
==================================================
[02:19:12] Step 6050/125000:
  Training Loss: 2.640
  Perplexity: 14.020
  Learning Rate: 1.99e-04
  GPU Memory Used: 11837MB

[02:20:19] Step 6100/125000:
  Training Loss: 2.821
  Perplexity: 16.802
  Learning Rate: 1.99e-04
  GPU Memory Used: 11900MB

[02:21:29] Step 6150/125000:
  Training Loss: 2.506
  Perplexity: 12.251
  Learning Rate: 1.99e-04
  GPU Memory Used: 12181MB

[02:22:37] Step 6200/125000:
  Training Loss: 2.795
  Perplexity: 16.357
  Learning Rate: 1.99e-04
  GPU Memory Used: 12063MB

[02:23:46] Step 6250/125000:
  Training Loss: 2.586
  Perplexity: 13.276
  Learning Rate: 1.99e-04
  GPU Memory Used: 12010MB

[02:24:56] Step 6300/125000:
  Training Loss: 2.577
  Perplexity: 13.163
  Learning Rate: 1.99e-04
  GPU Memory Used: 11821MB

[02:26:04] Step 6350/125000:
  Training Loss: 2.497
  Perplexity: 12.141
  Learning Rate: 1.99e-04
  GPU Memory Used: 11962MB

[02:27:13] Step 6400/125000:
  Training Loss: 2.772
  Perplexity: 15.994
  Learning Rate: 1.99e-04
  GPU Memory Used: 11953MB

[02:28:20] Step 6450/125000:
  Training Loss: 2.585
  Perplexity: 13.267
  Learning Rate: 1.99e-04
  GPU Memory Used: 11812MB

[02:29:29] Step 6500/125000:
  Training Loss: 2.769
  Perplexity: 15.942
  Learning Rate: 1.99e-04
  GPU Memory Used: 12074MB


Evaluation Results:
  Validation Loss: 2.593
  Validation Perplexity: 13.374

Saving model checkpoint: proposal_model/checkpoint-6500


Epoch 14/250
==================================================
[02:30:38] Step 6550/125000:
  Training Loss: 2.577
  Perplexity: 13.155
  Learning Rate: 1.99e-04
  GPU Memory Used: 12186MB

[02:31:46] Step 6600/125000:
  Training Loss: 2.520
  Perplexity: 12.432
  Learning Rate: 1.99e-04
  GPU Memory Used: 11991MB

[02:32:58] Step 6650/125000:
  Training Loss: 2.565
  Perplexity: 13.001
  Learning Rate: 1.99e-04
  GPU Memory Used: 11841MB

[02:34:05] Step 6700/125000:
  Training Loss: 2.616
  Perplexity: 13.683
  Learning Rate: 1.99e-04
  GPU Memory Used: 11878MB

[02:35:14] Step 6750/125000:
  Training Loss: 2.562
  Perplexity: 12.957
  Learning Rate: 1.99e-04
  GPU Memory Used: 11834MB

[02:36:23] Step 6800/125000:
  Training Loss: 2.631
  Perplexity: 13.894
  Learning Rate: 1.99e-04
  GPU Memory Used: 11807MB

[02:37:33] Step 6850/125000:
  Training Loss: 2.593
  Perplexity: 13.369
  Learning Rate: 1.99e-04
  GPU Memory Used: 12196MB

[02:38:41] Step 6900/125000:
  Training Loss: 2.505
  Perplexity: 12.246
  Learning Rate: 1.99e-04
  GPU Memory Used: 11986MB

[02:39:50] Step 6950/125000:
  Training Loss: 2.562
  Perplexity: 12.963
  Learning Rate: 1.98e-04
  GPU Memory Used: 11842MB

[02:40:59] Step 7000/125000:
  Training Loss: 2.662
  Perplexity: 14.330
  Learning Rate: 1.98e-04
  GPU Memory Used: 11840MB


Evaluation Results:
  Validation Loss: 2.624
  Validation Perplexity: 13.787

Saving model checkpoint: proposal_model/checkpoint-7000


Epoch 15/250
==================================================
[02:42:09] Step 7050/125000:
  Training Loss: 2.595
  Perplexity: 13.390
  Learning Rate: 1.98e-04
  GPU Memory Used: 12162MB

[02:43:18] Step 7100/125000:
  Training Loss: 2.546
  Perplexity: 12.752
  Learning Rate: 1.98e-04
  GPU Memory Used: 12036MB

[02:44:28] Step 7150/125000:
  Training Loss: 2.412
  Perplexity: 11.158
  Learning Rate: 1.98e-04
  GPU Memory Used: 12130MB

[02:45:36] Step 7200/125000:
  Training Loss: 2.563
  Perplexity: 12.970
  Learning Rate: 1.98e-04
  GPU Memory Used: 11911MB

[02:46:45] Step 7250/125000:
  Training Loss: 2.387
  Perplexity: 10.876
  Learning Rate: 1.98e-04
  GPU Memory Used: 12041MB

[02:47:52] Step 7300/125000:
  Training Loss: 2.647
  Perplexity: 14.117
  Learning Rate: 1.98e-04
  GPU Memory Used: 12108MB

[02:49:01] Step 7350/125000:
  Training Loss: 2.642
  Perplexity: 14.036
  Learning Rate: 1.98e-04
  GPU Memory Used: 11839MB

[02:50:10] Step 7400/125000:
  Training Loss: 2.634
  Perplexity: 13.935
  Learning Rate: 1.98e-04
  GPU Memory Used: 12091MB

[02:51:18] Step 7450/125000:
  Training Loss: 2.513
  Perplexity: 12.343
  Learning Rate: 1.98e-04
  GPU Memory Used: 11843MB

[02:52:26] Step 7500/125000:
  Training Loss: 2.534
  Perplexity: 12.598
  Learning Rate: 1.98e-04
  GPU Memory Used: 12171MB


Evaluation Results:
  Validation Loss: 2.776
  Validation Perplexity: 16.059

Saving model checkpoint: proposal_model/checkpoint-7500


Epoch 16/250
==================================================
[02:53:36] Step 7550/125000:
  Training Loss: 2.614
  Perplexity: 13.653
  Learning Rate: 1.98e-04
  GPU Memory Used: 12143MB

[02:54:45] Step 7600/125000:
  Training Loss: 2.520
  Perplexity: 12.428
  Learning Rate: 1.98e-04
  GPU Memory Used: 11998MB

[02:55:52] Step 7650/125000:
  Training Loss: 2.579
  Perplexity: 13.181
  Learning Rate: 1.98e-04
  GPU Memory Used: 12056MB

[02:57:03] Step 7700/125000:
  Training Loss: 2.603
  Perplexity: 13.505
  Learning Rate: 1.98e-04
  GPU Memory Used: 11968MB

[02:58:11] Step 7750/125000:
  Training Loss: 2.745
  Perplexity: 15.558
  Learning Rate: 1.98e-04
  GPU Memory Used: 12103MB

[02:59:19] Step 7800/125000:
  Training Loss: 2.595
  Perplexity: 13.400
  Learning Rate: 1.98e-04
  GPU Memory Used: 12109MB

[03:00:28] Step 7850/125000:
  Training Loss: 2.549
  Perplexity: 12.795
  Learning Rate: 1.98e-04
  GPU Memory Used: 12078MB

[03:01:38] Step 7900/125000:
  Training Loss: 2.553
  Perplexity: 12.839
  Learning Rate: 1.98e-04
  GPU Memory Used: 12123MB

[03:02:49] Step 7950/125000:
  Training Loss: 2.384
  Perplexity: 10.843
  Learning Rate: 1.98e-04
  GPU Memory Used: 12101MB

[03:03:57] Step 8000/125000:
  Training Loss: 2.558
  Perplexity: 12.910
  Learning Rate: 1.98e-04
  GPU Memory Used: 11855MB


Evaluation Results:
  Validation Loss: 2.606
  Validation Perplexity: 13.545

Saving model checkpoint: proposal_model/checkpoint-8000


Epoch 17/250
==================================================
[03:05:06] Step 8050/125000:
  Training Loss: 2.470
  Perplexity: 11.820
  Learning Rate: 1.98e-04
  GPU Memory Used: 11884MB

[03:06:17] Step 8100/125000:
  Training Loss: 2.398
  Perplexity: 10.998
  Learning Rate: 1.98e-04
  GPU Memory Used: 11893MB

[03:07:25] Step 8150/125000:
  Training Loss: 2.457
  Perplexity: 11.673
  Learning Rate: 1.98e-04
  GPU Memory Used: 11968MB

[03:08:34] Step 8200/125000:
  Training Loss: 2.522
  Perplexity: 12.456
  Learning Rate: 1.98e-04
  GPU Memory Used: 12154MB

[03:09:43] Step 8250/125000:
  Training Loss: 2.504
  Perplexity: 12.235
  Learning Rate: 1.98e-04
  GPU Memory Used: 12178MB

[03:10:51] Step 8300/125000:
  Training Loss: 2.587
  Perplexity: 13.294
  Learning Rate: 1.98e-04
  GPU Memory Used: 11893MB

[03:11:59] Step 8350/125000:
  Training Loss: 2.685
  Perplexity: 14.651
  Learning Rate: 1.98e-04
  GPU Memory Used: 11992MB

[03:13:07] Step 8400/125000:
  Training Loss: 2.646
  Perplexity: 14.103
  Learning Rate: 1.98e-04
  GPU Memory Used: 12024MB

[03:14:16] Step 8450/125000:
  Training Loss: 2.640
  Perplexity: 14.019
  Learning Rate: 1.98e-04
  GPU Memory Used: 12171MB

[03:15:26] Step 8500/125000:
  Training Loss: 2.614
  Perplexity: 13.652
  Learning Rate: 1.98e-04
  GPU Memory Used: 12045MB


Evaluation Results:
  Validation Loss: 2.740
  Validation Perplexity: 15.486

Saving model checkpoint: proposal_model/checkpoint-8500


Epoch 18/250
==================================================
[03:16:36] Step 8550/125000:
  Training Loss: 2.712
  Perplexity: 15.062
  Learning Rate: 1.98e-04
  GPU Memory Used: 12160MB

[03:17:44] Step 8600/125000:
  Training Loss: 2.627
  Perplexity: 13.827
  Learning Rate: 1.98e-04
  GPU Memory Used: 12197MB

[03:18:52] Step 8650/125000:
  Training Loss: 2.616
  Perplexity: 13.676
  Learning Rate: 1.98e-04
  GPU Memory Used: 11993MB

[03:20:02] Step 8700/125000:
  Training Loss: 2.525
  Perplexity: 12.489
  Learning Rate: 1.98e-04
  GPU Memory Used: 12183MB

[03:21:11] Step 8750/125000:
  Training Loss: 2.630
  Perplexity: 13.870
  Learning Rate: 1.98e-04
  GPU Memory Used: 11855MB

[03:22:22] Step 8800/125000:
  Training Loss: 2.311
  Perplexity: 10.080
  Learning Rate: 1.98e-04
  GPU Memory Used: 11928MB

[03:23:29] Step 8850/125000:
  Training Loss: 2.485
  Perplexity: 11.997
  Learning Rate: 1.98e-04
  GPU Memory Used: 11963MB

[03:24:39] Step 8900/125000:
  Training Loss: 2.587
  Perplexity: 13.291
  Learning Rate: 1.98e-04
  GPU Memory Used: 12023MB

[03:25:48] Step 8950/125000:
  Training Loss: 2.542
  Perplexity: 12.703
  Learning Rate: 1.97e-04
  GPU Memory Used: 11843MB

[03:26:57] Step 9000/125000:
  Training Loss: 2.604
  Perplexity: 13.520
  Learning Rate: 1.97e-04
  GPU Memory Used: 12156MB


Evaluation Results:
  Validation Loss: 2.795
  Validation Perplexity: 16.367

Saving model checkpoint: proposal_model/checkpoint-9000


Epoch 19/250
==================================================
[03:28:08] Step 9050/125000:
  Training Loss: 2.581
  Perplexity: 13.209
  Learning Rate: 1.97e-04
  GPU Memory Used: 11850MB

[03:29:16] Step 9100/125000:
  Training Loss: 2.610
  Perplexity: 13.593
  Learning Rate: 1.97e-04
  GPU Memory Used: 11861MB

[03:30:23] Step 9150/125000:
  Training Loss: 2.581
  Perplexity: 13.215
  Learning Rate: 1.97e-04
  GPU Memory Used: 11866MB

[03:31:32] Step 9200/125000:
  Training Loss: 2.389
  Perplexity: 10.898
  Learning Rate: 1.97e-04
  GPU Memory Used: 12108MB

[03:32:42] Step 9250/125000:
  Training Loss: 2.456
  Perplexity: 11.655
  Learning Rate: 1.97e-04
  GPU Memory Used: 12118MB

[03:33:50] Step 9300/125000:
  Training Loss: 2.665
  Perplexity: 14.371
  Learning Rate: 1.97e-04
  GPU Memory Used: 11942MB

[03:34:59] Step 9350/125000:
  Training Loss: 2.560
  Perplexity: 12.932
  Learning Rate: 1.97e-04
  GPU Memory Used: 11838MB

[03:36:09] Step 9400/125000:
  Training Loss: 2.564
  Perplexity: 12.985
  Learning Rate: 1.97e-04
  GPU Memory Used: 12006MB

[03:37:19] Step 9450/125000:
  Training Loss: 2.419
  Perplexity: 11.239
  Learning Rate: 1.97e-04
  GPU Memory Used: 12053MB

[03:38:28] Step 9500/125000:
  Training Loss: 2.661
  Perplexity: 14.312
  Learning Rate: 1.97e-04
  GPU Memory Used: 11855MB


Evaluation Results:
  Validation Loss: 2.761
  Validation Perplexity: 15.812

Saving model checkpoint: proposal_model/checkpoint-9500


Epoch 20/250
==================================================
[03:39:36] Step 9550/125000:
  Training Loss: 2.428
  Perplexity: 11.341
  Learning Rate: 1.97e-04
  GPU Memory Used: 11966MB

[03:40:45] Step 9600/125000:
  Training Loss: 2.462
  Perplexity: 11.730
  Learning Rate: 1.97e-04
  GPU Memory Used: 11852MB

[03:41:55] Step 9650/125000:
  Training Loss: 2.273
  Perplexity: 9.710
  Learning Rate: 1.97e-04
  GPU Memory Used: 12063MB

[03:43:01] Step 9700/125000:
  Training Loss: 2.345
  Perplexity: 10.437
  Learning Rate: 1.97e-04
  GPU Memory Used: 11806MB

[03:44:12] Step 9750/125000:
  Training Loss: 2.390
  Perplexity: 10.909
  Learning Rate: 1.97e-04
  GPU Memory Used: 12064MB

[03:45:20] Step 9800/125000:
  Training Loss: 2.518
  Perplexity: 12.404
  Learning Rate: 1.97e-04
  GPU Memory Used: 11936MB

[03:46:31] Step 9850/125000:
  Training Loss: 2.603
  Perplexity: 13.502
  Learning Rate: 1.97e-04
  GPU Memory Used: 11885MB

[03:47:42] Step 9900/125000:
  Training Loss: 2.401
  Perplexity: 11.034
  Learning Rate: 1.97e-04
  GPU Memory Used: 11805MB

[03:48:52] Step 9950/125000:
  Training Loss: 2.588
  Perplexity: 13.299
  Learning Rate: 1.97e-04
  GPU Memory Used: 11826MB

[03:50:02] Step 10000/125000:
  Training Loss: 2.409
  Perplexity: 11.120
  Learning Rate: 1.97e-04
  GPU Memory Used: 12131MB


Evaluation Results:
  Validation Loss: 2.411
  Validation Perplexity: 11.150

Saving model checkpoint: proposal_model/checkpoint-10000


Epoch 21/250
==================================================
[03:51:12] Step 10050/125000:
  Training Loss: 2.539
  Perplexity: 12.664
  Learning Rate: 1.97e-04
  GPU Memory Used: 11941MB

[03:52:23] Step 10100/125000:
  Training Loss: 2.601
  Perplexity: 13.482
  Learning Rate: 1.97e-04
  GPU Memory Used: 12078MB

[03:53:31] Step 10150/125000:
  Training Loss: 2.467
  Perplexity: 11.784
  Learning Rate: 1.97e-04
  GPU Memory Used: 11826MB

[03:54:39] Step 10200/125000:
  Training Loss: 2.549
  Perplexity: 12.789
  Learning Rate: 1.97e-04
  GPU Memory Used: 11851MB

[03:55:48] Step 10250/125000:
  Training Loss: 2.499
  Perplexity: 12.171
  Learning Rate: 1.97e-04
  GPU Memory Used: 12135MB

[03:56:59] Step 10300/125000:
  Training Loss: 2.498
  Perplexity: 12.154
  Learning Rate: 1.97e-04
  GPU Memory Used: 12090MB

[03:58:07] Step 10350/125000:
  Training Loss: 2.434
  Perplexity: 11.401
  Learning Rate: 1.97e-04
  GPU Memory Used: 11867MB

[03:59:17] Step 10400/125000:
  Training Loss: 2.304
  Perplexity: 10.016
  Learning Rate: 1.97e-04
  GPU Memory Used: 12161MB

[04:00:26] Step 10450/125000:
  Training Loss: 2.455
  Perplexity: 11.651
  Learning Rate: 1.97e-04
  GPU Memory Used: 11928MB

[04:01:34] Step 10500/125000:
  Training Loss: 2.461
  Perplexity: 11.721
  Learning Rate: 1.97e-04
  GPU Memory Used: 12098MB


Evaluation Results:
  Validation Loss: 2.535
  Validation Perplexity: 12.617

Saving model checkpoint: proposal_model/checkpoint-10500


Epoch 22/250
==================================================
[04:02:44] Step 10550/125000:
  Training Loss: 2.437
  Perplexity: 11.443
  Learning Rate: 1.97e-04
  GPU Memory Used: 12030MB

[04:03:55] Step 10600/125000:
  Training Loss: 2.627
  Perplexity: 13.827
  Learning Rate: 1.96e-04
  GPU Memory Used: 11883MB

[04:05:05] Step 10650/125000:
  Training Loss: 2.558
  Perplexity: 12.909
  Learning Rate: 1.96e-04
  GPU Memory Used: 12009MB

[04:06:14] Step 10700/125000:
  Training Loss: 2.580
  Perplexity: 13.191
  Learning Rate: 1.96e-04
  GPU Memory Used: 11837MB

[04:07:22] Step 10750/125000:
  Training Loss: 2.482
  Perplexity: 11.968
  Learning Rate: 1.96e-04
  GPU Memory Used: 12153MB

[04:08:31] Step 10800/125000:
  Training Loss: 2.423
  Perplexity: 11.284
  Learning Rate: 1.96e-04
  GPU Memory Used: 12043MB

[04:09:41] Step 10850/125000:
  Training Loss: 2.370
  Perplexity: 10.694
  Learning Rate: 1.96e-04
  GPU Memory Used: 12157MB

[04:10:51] Step 10900/125000:
  Training Loss: 2.343
  Perplexity: 10.408
  Learning Rate: 1.96e-04
  GPU Memory Used: 11989MB

[04:11:59] Step 10950/125000:
  Training Loss: 2.494
  Perplexity: 12.108
  Learning Rate: 1.96e-04
  GPU Memory Used: 11923MB

[04:13:08] Step 11000/125000:
  Training Loss: 2.577
  Perplexity: 13.160
  Learning Rate: 1.96e-04
  GPU Memory Used: 12133MB


Evaluation Results:
  Validation Loss: 2.581
  Validation Perplexity: 13.204

Saving model checkpoint: proposal_model/checkpoint-11000


Epoch 23/250
==================================================
[04:14:16] Step 11050/125000:
  Training Loss: 2.382
  Perplexity: 10.821
  Learning Rate: 1.96e-04
  GPU Memory Used: 12164MB

[04:15:26] Step 11100/125000:
  Training Loss: 2.352
  Perplexity: 10.501
  Learning Rate: 1.96e-04
  GPU Memory Used: 11942MB

[04:16:36] Step 11150/125000:
  Training Loss: 2.314
  Perplexity: 10.110
  Learning Rate: 1.96e-04
  GPU Memory Used: 11998MB

[04:17:45] Step 11200/125000:
  Training Loss: 2.462
  Perplexity: 11.732
  Learning Rate: 1.96e-04
  GPU Memory Used: 11888MB

[04:18:55] Step 11250/125000:
  Training Loss: 2.495
  Perplexity: 12.120
  Learning Rate: 1.96e-04
  GPU Memory Used: 11918MB

[04:20:02] Step 11300/125000:
  Training Loss: 2.338
  Perplexity: 10.358
  Learning Rate: 1.96e-04
  GPU Memory Used: 12178MB

[04:21:10] Step 11350/125000:
  Training Loss: 2.405
  Perplexity: 11.084
  Learning Rate: 1.96e-04
  GPU Memory Used: 11955MB

[04:22:18] Step 11400/125000:
  Training Loss: 2.232
  Perplexity: 9.317
  Learning Rate: 1.96e-04
  GPU Memory Used: 12127MB

[04:23:26] Step 11450/125000:
  Training Loss: 2.436
  Perplexity: 11.428
  Learning Rate: 1.96e-04
  GPU Memory Used: 12117MB

[04:24:34] Step 11500/125000:
  Training Loss: 2.373
  Perplexity: 10.731
  Learning Rate: 1.96e-04
  GPU Memory Used: 12065MB


Evaluation Results:
  Validation Loss: 2.491
  Validation Perplexity: 12.074

Saving model checkpoint: proposal_model/checkpoint-11500


Epoch 24/250
==================================================
[04:25:42] Step 11550/125000:
  Training Loss: 2.632
  Perplexity: 13.898
  Learning Rate: 1.96e-04
  GPU Memory Used: 11862MB

[04:26:51] Step 11600/125000:
  Training Loss: 2.460
  Perplexity: 11.700
  Learning Rate: 1.96e-04
  GPU Memory Used: 12020MB

[04:28:01] Step 11650/125000:
  Training Loss: 2.440
  Perplexity: 11.474
  Learning Rate: 1.96e-04
  GPU Memory Used: 12178MB

[04:29:11] Step 11700/125000:
  Training Loss: 2.332
  Perplexity: 10.301
  Learning Rate: 1.96e-04
  GPU Memory Used: 11984MB

[04:30:21] Step 11750/125000:
  Training Loss: 2.629
  Perplexity: 13.856
  Learning Rate: 1.96e-04
  GPU Memory Used: 11956MB

[04:31:32] Step 11800/125000:
  Training Loss: 2.472
  Perplexity: 11.851
  Learning Rate: 1.96e-04
  GPU Memory Used: 11988MB

[04:32:41] Step 11850/125000:
  Training Loss: 2.250
  Perplexity: 9.486
  Learning Rate: 1.96e-04
  GPU Memory Used: 11870MB

[04:33:51] Step 11900/125000:
  Training Loss: 2.345
  Perplexity: 10.430
  Learning Rate: 1.96e-04
  GPU Memory Used: 11977MB

[04:34:58] Step 11950/125000:
  Training Loss: 2.457
  Perplexity: 11.673
  Learning Rate: 1.96e-04
  GPU Memory Used: 12158MB

[04:36:07] Step 12000/125000:
  Training Loss: 2.359
  Perplexity: 10.581
  Learning Rate: 1.95e-04
  GPU Memory Used: 11952MB


Evaluation Results:
  Validation Loss: 2.607
  Validation Perplexity: 13.559

Saving model checkpoint: proposal_model/checkpoint-12000


Epoch 25/250
==================================================
[04:37:16] Step 12050/125000:
  Training Loss: 2.516
  Perplexity: 12.384
  Learning Rate: 1.95e-04
  GPU Memory Used: 11952MB

[04:38:25] Step 12100/125000:
  Training Loss: 2.418
  Perplexity: 11.224
  Learning Rate: 1.95e-04
  GPU Memory Used: 11928MB

[04:39:34] Step 12150/125000:
  Training Loss: 2.521
  Perplexity: 12.446
  Learning Rate: 1.95e-04
  GPU Memory Used: 12159MB

[04:40:43] Step 12200/125000:
  Training Loss: 2.489
  Perplexity: 12.046
  Learning Rate: 1.95e-04
  GPU Memory Used: 12187MB

[04:41:52] Step 12250/125000:
  Training Loss: 2.442
  Perplexity: 11.496
  Learning Rate: 1.95e-04
  GPU Memory Used: 11971MB

[04:43:00] Step 12300/125000:
  Training Loss: 2.369
  Perplexity: 10.691
  Learning Rate: 1.95e-04
  GPU Memory Used: 11866MB

[04:44:09] Step 12350/125000:
  Training Loss: 2.353
  Perplexity: 10.513
  Learning Rate: 1.95e-04
  GPU Memory Used: 11860MB

[04:45:16] Step 12400/125000:
  Training Loss: 2.374
  Perplexity: 10.741
  Learning Rate: 1.95e-04
  GPU Memory Used: 12013MB

[04:46:24] Step 12450/125000:
  Training Loss: 2.436
  Perplexity: 11.425
  Learning Rate: 1.95e-04
  GPU Memory Used: 11933MB

[04:47:33] Step 12500/125000:
  Training Loss: 2.501
  Perplexity: 12.195
  Learning Rate: 1.95e-04
  GPU Memory Used: 11983MB


Evaluation Results:
  Validation Loss: 2.487
  Validation Perplexity: 12.025

Saving model checkpoint: proposal_model/checkpoint-12500


Epoch 26/250
==================================================
[04:48:41] Step 12550/125000:
  Training Loss: 2.506
  Perplexity: 12.252
  Learning Rate: 1.95e-04
  GPU Memory Used: 11963MB

[04:49:47] Step 12600/125000:
  Training Loss: 2.458
  Perplexity: 11.679
  Learning Rate: 1.95e-04
  GPU Memory Used: 11891MB

[04:50:56] Step 12650/125000:
  Training Loss: 2.232
  Perplexity: 9.315
  Learning Rate: 1.95e-04
  GPU Memory Used: 11836MB

[04:52:06] Step 12700/125000:
  Training Loss: 2.412
  Perplexity: 11.156
  Learning Rate: 1.95e-04
  GPU Memory Used: 11931MB

[04:53:14] Step 12750/125000:
  Training Loss: 2.532
  Perplexity: 12.575
  Learning Rate: 1.95e-04
  GPU Memory Used: 12184MB

[04:54:23] Step 12800/125000:
  Training Loss: 2.339
  Perplexity: 10.367
  Learning Rate: 1.95e-04
  GPU Memory Used: 12050MB

[04:55:33] Step 12850/125000:
  Training Loss: 2.318
  Perplexity: 10.159
  Learning Rate: 1.95e-04
  GPU Memory Used: 12024MB

[04:56:45] Step 12900/125000:
  Training Loss: 2.346
  Perplexity: 10.442
  Learning Rate: 1.95e-04
  GPU Memory Used: 11955MB

[04:57:54] Step 12950/125000:
  Training Loss: 2.474
  Perplexity: 11.865
  Learning Rate: 1.95e-04
  GPU Memory Used: 12007MB

[04:59:02] Step 13000/125000:
  Training Loss: 2.325
  Perplexity: 10.226
  Learning Rate: 1.95e-04
  GPU Memory Used: 12063MB


Evaluation Results:
  Validation Loss: 2.432
  Validation Perplexity: 11.382

Saving model checkpoint: proposal_model/checkpoint-13000


Epoch 27/250
==================================================
[05:00:12] Step 13050/125000:
  Training Loss: 2.388
  Perplexity: 10.891
  Learning Rate: 1.95e-04
  GPU Memory Used: 12154MB

[05:01:21] Step 13100/125000:
  Training Loss: 2.373
  Perplexity: 10.731
  Learning Rate: 1.95e-04
  GPU Memory Used: 12144MB

[05:02:31] Step 13150/125000:
  Training Loss: 2.343
  Perplexity: 10.416
  Learning Rate: 1.95e-04
  GPU Memory Used: 12058MB

[05:03:41] Step 13200/125000:
  Training Loss: 2.423
  Perplexity: 11.285
  Learning Rate: 1.95e-04
  GPU Memory Used: 11983MB

[05:04:50] Step 13250/125000:
  Training Loss: 2.471
  Perplexity: 11.835
  Learning Rate: 1.95e-04
  GPU Memory Used: 12013MB

[05:05:59] Step 13300/125000:
  Training Loss: 2.417
  Perplexity: 11.207
  Learning Rate: 1.94e-04
  GPU Memory Used: 12139MB

[05:07:10] Step 13350/125000:
  Training Loss: 2.144
  Perplexity: 8.532
  Learning Rate: 1.94e-04
  GPU Memory Used: 11823MB

[05:08:20] Step 13400/125000:
  Training Loss: 2.491
  Perplexity: 12.070
  Learning Rate: 1.94e-04
  GPU Memory Used: 11952MB

[05:09:28] Step 13450/125000:
  Training Loss: 2.539
  Perplexity: 12.673
  Learning Rate: 1.94e-04
  GPU Memory Used: 12103MB

[05:10:37] Step 13500/125000:
  Training Loss: 2.438
  Perplexity: 11.455
  Learning Rate: 1.94e-04
  GPU Memory Used: 12190MB


Evaluation Results:
  Validation Loss: 2.444
  Validation Perplexity: 11.521

Saving model checkpoint: proposal_model/checkpoint-13500


Epoch 28/250
==================================================
[05:11:46] Step 13550/125000:
  Training Loss: 2.383
  Perplexity: 10.839
  Learning Rate: 1.94e-04
  GPU Memory Used: 11986MB

[05:12:55] Step 13600/125000:
  Training Loss: 2.229
  Perplexity: 9.293
  Learning Rate: 1.94e-04
  GPU Memory Used: 11807MB

[05:14:03] Step 13650/125000:
  Training Loss: 2.424
  Perplexity: 11.292
  Learning Rate: 1.94e-04
  GPU Memory Used: 11979MB

[05:15:12] Step 13700/125000:
  Training Loss: 2.440
  Perplexity: 11.477
  Learning Rate: 1.94e-04
  GPU Memory Used: 12125MB

[05:16:18] Step 13750/125000:
  Training Loss: 2.456
  Perplexity: 11.659
  Learning Rate: 1.94e-04
  GPU Memory Used: 12121MB

[05:17:25] Step 13800/125000:
  Training Loss: 2.379
  Perplexity: 10.798
  Learning Rate: 1.94e-04
  GPU Memory Used: 12015MB

[05:18:35] Step 13850/125000:
  Training Loss: 2.410
  Perplexity: 11.132
  Learning Rate: 1.94e-04
  GPU Memory Used: 11863MB

[05:19:44] Step 13900/125000:
  Training Loss: 2.377
  Perplexity: 10.776
  Learning Rate: 1.94e-04
  GPU Memory Used: 12026MB

[05:20:54] Step 13950/125000:
  Training Loss: 2.536
  Perplexity: 12.632
  Learning Rate: 1.94e-04
  GPU Memory Used: 11862MB

[05:22:05] Step 14000/125000:
  Training Loss: 2.393
  Perplexity: 10.942
  Learning Rate: 1.94e-04
  GPU Memory Used: 12118MB


Evaluation Results:
  Validation Loss: 2.463
  Validation Perplexity: 11.741

Saving model checkpoint: proposal_model/checkpoint-14000


Epoch 29/250
==================================================
[05:23:16] Step 14050/125000:
  Training Loss: 2.354
  Perplexity: 10.531
  Learning Rate: 1.94e-04
  GPU Memory Used: 12076MB

[05:24:24] Step 14100/125000:
  Training Loss: 2.357
  Perplexity: 10.559
  Learning Rate: 1.94e-04
  GPU Memory Used: 11890MB

[05:25:33] Step 14150/125000:
  Training Loss: 2.271
  Perplexity: 9.693
  Learning Rate: 1.94e-04
  GPU Memory Used: 11843MB

[05:26:43] Step 14200/125000:
  Training Loss: 2.269
  Perplexity: 9.670
  Learning Rate: 1.94e-04
  GPU Memory Used: 12129MB

[05:27:54] Step 14250/125000:
  Training Loss: 2.347
  Perplexity: 10.449
  Learning Rate: 1.94e-04
  GPU Memory Used: 12069MB

[05:29:03] Step 14300/125000:
  Training Loss: 2.332
  Perplexity: 10.297
  Learning Rate: 1.94e-04
  GPU Memory Used: 11854MB

[05:30:11] Step 14350/125000:
  Training Loss: 2.437
  Perplexity: 11.443
  Learning Rate: 1.94e-04
  GPU Memory Used: 12010MB

[05:31:21] Step 14400/125000:
  Training Loss: 2.265
  Perplexity: 9.633
  Learning Rate: 1.94e-04
  GPU Memory Used: 12173MB

[05:32:28] Step 14450/125000:
  Training Loss: 2.343
  Perplexity: 10.417
  Learning Rate: 1.93e-04
  GPU Memory Used: 11987MB

[05:33:38] Step 14500/125000:
  Training Loss: 2.404
  Perplexity: 11.066
  Learning Rate: 1.93e-04
  GPU Memory Used: 12141MB


Evaluation Results:
  Validation Loss: 2.352
  Validation Perplexity: 10.506

Saving model checkpoint: proposal_model/checkpoint-14500


Epoch 30/250
==================================================
[05:34:47] Step 14550/125000:
  Training Loss: 2.392
  Perplexity: 10.940
  Learning Rate: 1.93e-04
  GPU Memory Used: 11860MB

[05:35:57] Step 14600/125000:
  Training Loss: 2.315
  Perplexity: 10.130
  Learning Rate: 1.93e-04
  GPU Memory Used: 12085MB

[05:37:06] Step 14650/125000:
  Training Loss: 2.401
  Perplexity: 11.034
  Learning Rate: 1.93e-04
  GPU Memory Used: 11870MB

[05:38:16] Step 14700/125000:
  Training Loss: 2.341
  Perplexity: 10.392
  Learning Rate: 1.93e-04
  GPU Memory Used: 11890MB

[05:39:25] Step 14750/125000:
  Training Loss: 2.481
  Perplexity: 11.951
  Learning Rate: 1.93e-04
  GPU Memory Used: 12147MB

[05:40:35] Step 14800/125000:
  Training Loss: 2.231
  Perplexity: 9.312
  Learning Rate: 1.93e-04
  GPU Memory Used: 12085MB

[05:41:42] Step 14850/125000:
  Training Loss: 2.390
  Perplexity: 10.914
  Learning Rate: 1.93e-04
  GPU Memory Used: 12194MB

[05:42:52] Step 14900/125000:
  Training Loss: 2.295
  Perplexity: 9.925
  Learning Rate: 1.93e-04
  GPU Memory Used: 11816MB

[05:44:00] Step 14950/125000:
  Training Loss: 2.433
  Perplexity: 11.391
  Learning Rate: 1.93e-04
  GPU Memory Used: 12017MB

[05:45:09] Step 15000/125000:
  Training Loss: 2.271
  Perplexity: 9.692
  Learning Rate: 1.93e-04
  GPU Memory Used: 12012MB


Evaluation Results:
  Validation Loss: 2.437
  Validation Perplexity: 11.444

Saving model checkpoint: proposal_model/checkpoint-15000


Epoch 31/250
==================================================
[05:46:18] Step 15050/125000:
  Training Loss: 2.341
  Perplexity: 10.393
  Learning Rate: 1.93e-04
  GPU Memory Used: 12169MB

[05:47:26] Step 15100/125000:
  Training Loss: 2.182
  Perplexity: 8.866
  Learning Rate: 1.93e-04
  GPU Memory Used: 12184MB

[05:48:35] Step 15150/125000:
  Training Loss: 2.320
  Perplexity: 10.175
  Learning Rate: 1.93e-04
  GPU Memory Used: 12181MB

[05:49:44] Step 15200/125000:
  Training Loss: 2.371
  Perplexity: 10.707
  Learning Rate: 1.93e-04
  GPU Memory Used: 12020MB

[05:50:51] Step 15250/125000:
  Training Loss: 2.427
  Perplexity: 11.322
  Learning Rate: 1.93e-04
  GPU Memory Used: 12010MB

[05:52:00] Step 15300/125000:
  Training Loss: 2.297
  Perplexity: 9.944
  Learning Rate: 1.93e-04
  GPU Memory Used: 11999MB

[05:53:10] Step 15350/125000:
  Training Loss: 2.253
  Perplexity: 9.516
  Learning Rate: 1.93e-04
  GPU Memory Used: 11970MB

[05:54:19] Step 15400/125000:
  Training Loss: 2.327
  Perplexity: 10.243
  Learning Rate: 1.93e-04
  GPU Memory Used: 12048MB

[05:55:27] Step 15450/125000:
  Training Loss: 2.315
  Perplexity: 10.127
  Learning Rate: 1.93e-04
  GPU Memory Used: 12056MB

[05:56:38] Step 15500/125000:
  Training Loss: 2.354
  Perplexity: 10.530
  Learning Rate: 1.93e-04
  GPU Memory Used: 11800MB


Evaluation Results:
  Validation Loss: 2.482
  Validation Perplexity: 11.969

Saving model checkpoint: proposal_model/checkpoint-15500


Epoch 32/250
==================================================
[05:57:50] Step 15550/125000:
  Training Loss: 2.315
  Perplexity: 10.120
  Learning Rate: 1.92e-04
  GPU Memory Used: 11811MB

[05:58:58] Step 15600/125000:
  Training Loss: 2.445
  Perplexity: 11.535
  Learning Rate: 1.92e-04
  GPU Memory Used: 11972MB

[06:00:05] Step 15650/125000:
  Training Loss: 2.505
  Perplexity: 12.241
  Learning Rate: 1.92e-04
  GPU Memory Used: 11883MB

[06:01:14] Step 15700/125000:
  Training Loss: 2.337
  Perplexity: 10.353
  Learning Rate: 1.92e-04
  GPU Memory Used: 11969MB

[06:02:26] Step 15750/125000:
  Training Loss: 2.332
  Perplexity: 10.300
  Learning Rate: 1.92e-04
  GPU Memory Used: 11973MB

[06:03:35] Step 15800/125000:
  Training Loss: 2.417
  Perplexity: 11.207
  Learning Rate: 1.92e-04
  GPU Memory Used: 11965MB

[06:04:45] Step 15850/125000:
  Training Loss: 2.434
  Perplexity: 11.406
  Learning Rate: 1.92e-04
  GPU Memory Used: 11965MB

[06:05:55] Step 15900/125000:
  Training Loss: 2.454
  Perplexity: 11.639
  Learning Rate: 1.92e-04
  GPU Memory Used: 11955MB

[06:07:03] Step 15950/125000:
  Training Loss: 2.286
  Perplexity: 9.831
  Learning Rate: 1.92e-04
  GPU Memory Used: 11929MB

[06:08:14] Step 16000/125000:
  Training Loss: 2.354
  Perplexity: 10.524
  Learning Rate: 1.92e-04
  GPU Memory Used: 12056MB


Evaluation Results:
  Validation Loss: 2.377
  Validation Perplexity: 10.770

Saving model checkpoint: proposal_model/checkpoint-16000


Epoch 33/250
==================================================
[06:09:24] Step 16050/125000:
  Training Loss: 2.412
  Perplexity: 11.155
  Learning Rate: 1.92e-04
  GPU Memory Used: 12186MB

[06:10:33] Step 16100/125000:
  Training Loss: 2.356
  Perplexity: 10.553
  Learning Rate: 1.92e-04
  GPU Memory Used: 12155MB

[06:11:42] Step 16150/125000:
  Training Loss: 2.338
  Perplexity: 10.356
  Learning Rate: 1.92e-04
  GPU Memory Used: 12086MB

[06:12:51] Step 16200/125000:
  Training Loss: 2.341
  Perplexity: 10.394
  Learning Rate: 1.92e-04
  GPU Memory Used: 11904MB

[06:14:01] Step 16250/125000:
  Training Loss: 2.422
  Perplexity: 11.266
  Learning Rate: 1.92e-04
  GPU Memory Used: 12105MB

[06:15:12] Step 16300/125000:
  Training Loss: 2.398
  Perplexity: 11.004
  Learning Rate: 1.92e-04
  GPU Memory Used: 11916MB

[06:16:21] Step 16350/125000:
  Training Loss: 2.254
  Perplexity: 9.526
  Learning Rate: 1.92e-04
  GPU Memory Used: 12195MB

[06:17:31] Step 16400/125000:
  Training Loss: 2.292
  Perplexity: 9.898
  Learning Rate: 1.92e-04
  GPU Memory Used: 12178MB

[06:18:41] Step 16450/125000:
  Training Loss: 2.328
  Perplexity: 10.256
  Learning Rate: 1.92e-04
  GPU Memory Used: 11964MB

[06:19:52] Step 16500/125000:
  Training Loss: 2.278
  Perplexity: 9.752
  Learning Rate: 1.92e-04
  GPU Memory Used: 11942MB


Evaluation Results:
  Validation Loss: 2.438
  Validation Perplexity: 11.455

Saving model checkpoint: proposal_model/checkpoint-16500


Epoch 34/250
==================================================
[06:21:00] Step 16550/125000:
  Training Loss: 2.366
  Perplexity: 10.656
  Learning Rate: 1.91e-04
  GPU Memory Used: 12083MB

[06:22:10] Step 16600/125000:
  Training Loss: 2.317
  Perplexity: 10.145
  Learning Rate: 1.91e-04
  GPU Memory Used: 11900MB

[06:23:17] Step 16650/125000:
  Training Loss: 2.307
  Perplexity: 10.047
  Learning Rate: 1.91e-04
  GPU Memory Used: 11927MB

[06:24:24] Step 16700/125000:
  Training Loss: 2.321
  Perplexity: 10.189
  Learning Rate: 1.91e-04
  GPU Memory Used: 11963MB

[06:25:32] Step 16750/125000:
  Training Loss: 2.166
  Perplexity: 8.720
  Learning Rate: 1.91e-04
  GPU Memory Used: 12083MB

[06:26:39] Step 16800/125000:
  Training Loss: 2.335
  Perplexity: 10.327
  Learning Rate: 1.91e-04
  GPU Memory Used: 11816MB

[06:27:48] Step 16850/125000:
  Training Loss: 2.287
  Perplexity: 9.849
  Learning Rate: 1.91e-04
  GPU Memory Used: 11802MB

[06:28:55] Step 16900/125000:
  Training Loss: 2.400
  Perplexity: 11.021
  Learning Rate: 1.91e-04
  GPU Memory Used: 11873MB

[06:30:03] Step 16950/125000:
  Training Loss: 2.316
  Perplexity: 10.130
  Learning Rate: 1.91e-04
  GPU Memory Used: 11862MB

[06:31:12] Step 17000/125000:
  Training Loss: 2.211
  Perplexity: 9.126
  Learning Rate: 1.91e-04
  GPU Memory Used: 12178MB


Evaluation Results:
  Validation Loss: 2.329
  Validation Perplexity: 10.268

Saving model checkpoint: proposal_model/checkpoint-17000


Epoch 35/250
==================================================
[06:32:22] Step 17050/125000:
  Training Loss: 2.344
  Perplexity: 10.424
  Learning Rate: 1.91e-04
  GPU Memory Used: 12137MB

[06:33:31] Step 17100/125000:
  Training Loss: 2.429
  Perplexity: 11.347
  Learning Rate: 1.91e-04
  GPU Memory Used: 11853MB

[06:34:40] Step 17150/125000:
  Training Loss: 2.329
  Perplexity: 10.264
  Learning Rate: 1.91e-04
  GPU Memory Used: 11809MB

[06:35:51] Step 17200/125000:
  Training Loss: 2.283
  Perplexity: 9.803
  Learning Rate: 1.91e-04
  GPU Memory Used: 11842MB

[06:36:58] Step 17250/125000:
  Training Loss: 2.246
  Perplexity: 9.447
  Learning Rate: 1.91e-04
  GPU Memory Used: 11817MB

[06:38:06] Step 17300/125000:
  Training Loss: 2.335
  Perplexity: 10.330
  Learning Rate: 1.91e-04
  GPU Memory Used: 12172MB

[06:39:16] Step 17350/125000:
  Training Loss: 2.332
  Perplexity: 10.301
  Learning Rate: 1.91e-04
  GPU Memory Used: 12092MB

[06:40:27] Step 17400/125000:
  Training Loss: 2.290
  Perplexity: 9.873
  Learning Rate: 1.91e-04
  GPU Memory Used: 12071MB

[06:41:34] Step 17450/125000:
  Training Loss: 2.151
  Perplexity: 8.589
  Learning Rate: 1.91e-04
  GPU Memory Used: 12126MB

[06:42:44] Step 17500/125000:
  Training Loss: 2.370
  Perplexity: 10.693
  Learning Rate: 1.90e-04
  GPU Memory Used: 11927MB


Evaluation Results:
  Validation Loss: 2.266
  Validation Perplexity: 9.641

Saving model checkpoint: proposal_model/checkpoint-17500


Epoch 36/250
==================================================
[06:43:53] Step 17550/125000:
  Training Loss: 2.242
  Perplexity: 9.411
  Learning Rate: 1.90e-04
  GPU Memory Used: 11933MB

[06:45:01] Step 17600/125000:
  Training Loss: 2.177
  Perplexity: 8.824
  Learning Rate: 1.90e-04
  GPU Memory Used: 12055MB

[06:46:10] Step 17650/125000:
  Training Loss: 2.243
  Perplexity: 9.418
  Learning Rate: 1.90e-04
  GPU Memory Used: 11975MB

[06:47:21] Step 17700/125000:
  Training Loss: 2.326
  Perplexity: 10.238
  Learning Rate: 1.90e-04
  GPU Memory Used: 11922MB

[06:48:28] Step 17750/125000:
  Training Loss: 2.283
  Perplexity: 9.811
  Learning Rate: 1.90e-04
  GPU Memory Used: 12080MB

[06:49:37] Step 17800/125000:
  Training Loss: 2.195
  Perplexity: 8.977
  Learning Rate: 1.90e-04
  GPU Memory Used: 11861MB

[06:50:45] Step 17850/125000:
  Training Loss: 2.210
  Perplexity: 9.115
  Learning Rate: 1.90e-04
  GPU Memory Used: 12000MB

[06:51:54] Step 17900/125000:
  Training Loss: 2.229
  Perplexity: 9.287
  Learning Rate: 1.90e-04
  GPU Memory Used: 11926MB

[06:53:03] Step 17950/125000:
  Training Loss: 2.221
  Perplexity: 9.218
  Learning Rate: 1.90e-04
  GPU Memory Used: 11832MB

[06:54:11] Step 18000/125000:
  Training Loss: 2.474
  Perplexity: 11.867
  Learning Rate: 1.90e-04
  GPU Memory Used: 11973MB


Evaluation Results:
  Validation Loss: 2.460
  Validation Perplexity: 11.707

Saving model checkpoint: proposal_model/checkpoint-18000


Epoch 37/250
==================================================
[06:55:20] Step 18050/125000:
  Training Loss: 2.189
  Perplexity: 8.930
  Learning Rate: 1.90e-04
  GPU Memory Used: 12192MB

[06:56:31] Step 18100/125000:
  Training Loss: 2.286
  Perplexity: 9.840
  Learning Rate: 1.90e-04
  GPU Memory Used: 12055MB

[06:57:40] Step 18150/125000:
  Training Loss: 2.422
  Perplexity: 11.266
  Learning Rate: 1.90e-04
  GPU Memory Used: 11809MB

[06:58:50] Step 18200/125000:
  Training Loss: 2.279
  Perplexity: 9.764
  Learning Rate: 1.90e-04
  GPU Memory Used: 12088MB

[06:59:57] Step 18250/125000:
  Training Loss: 2.359
  Perplexity: 10.578
  Learning Rate: 1.90e-04
  GPU Memory Used: 11825MB

[07:01:06] Step 18300/125000:
  Training Loss: 2.301
  Perplexity: 9.983
  Learning Rate: 1.90e-04
  GPU Memory Used: 12166MB

[07:02:17] Step 18350/125000:
  Training Loss: 2.306
  Perplexity: 10.036
  Learning Rate: 1.90e-04
  GPU Memory Used: 12098MB

[07:03:25] Step 18400/125000:
  Training Loss: 2.470
  Perplexity: 11.817
  Learning Rate: 1.89e-04
  GPU Memory Used: 11866MB

[07:04:33] Step 18450/125000:
  Training Loss: 2.312
  Perplexity: 10.097
  Learning Rate: 1.89e-04
  GPU Memory Used: 11986MB

[07:05:43] Step 18500/125000:
  Training Loss: 2.337
  Perplexity: 10.355
  Learning Rate: 1.89e-04
  GPU Memory Used: 11827MB


Evaluation Results:
  Validation Loss: 2.529
  Validation Perplexity: 12.537

Saving model checkpoint: proposal_model/checkpoint-18500


Epoch 38/250
==================================================
[07:06:54] Step 18550/125000:
  Training Loss: 2.113
  Perplexity: 8.272
  Learning Rate: 1.89e-04
  GPU Memory Used: 12167MB

[07:08:02] Step 18600/125000:
  Training Loss: 2.288
  Perplexity: 9.857
  Learning Rate: 1.89e-04
  GPU Memory Used: 12124MB

[07:09:13] Step 18650/125000:
  Training Loss: 2.274
  Perplexity: 9.718
  Learning Rate: 1.89e-04
  GPU Memory Used: 12006MB

[07:10:21] Step 18700/125000:
  Training Loss: 2.075
  Perplexity: 7.968
  Learning Rate: 1.89e-04
  GPU Memory Used: 12013MB

[07:11:29] Step 18750/125000:
  Training Loss: 2.347
  Perplexity: 10.451
  Learning Rate: 1.89e-04
  GPU Memory Used: 12101MB

[07:12:39] Step 18800/125000:
  Training Loss: 2.177
  Perplexity: 8.818
  Learning Rate: 1.89e-04
  GPU Memory Used: 12142MB

[07:13:50] Step 18850/125000:
  Training Loss: 2.268
  Perplexity: 9.664
  Learning Rate: 1.89e-04
  GPU Memory Used: 11969MB

[07:14:57] Step 18900/125000:
  Training Loss: 2.160
  Perplexity: 8.670
  Learning Rate: 1.89e-04
  GPU Memory Used: 11829MB

[07:16:06] Step 18950/125000:
  Training Loss: 2.405
  Perplexity: 11.083
  Learning Rate: 1.89e-04
  GPU Memory Used: 12015MB

[07:17:16] Step 19000/125000:
  Training Loss: 2.379
  Perplexity: 10.798
  Learning Rate: 1.89e-04
  GPU Memory Used: 11843MB


Evaluation Results:
  Validation Loss: 2.331
  Validation Perplexity: 10.290

Saving model checkpoint: proposal_model/checkpoint-19000


Epoch 39/250
==================================================
[07:18:24] Step 19050/125000:
  Training Loss: 2.214
  Perplexity: 9.149
  Learning Rate: 1.89e-04
  GPU Memory Used: 12084MB

[07:19:35] Step 19100/125000:
  Training Loss: 2.250
  Perplexity: 9.489
  Learning Rate: 1.89e-04
  GPU Memory Used: 11971MB

[07:20:44] Step 19150/125000:
  Training Loss: 2.144
  Perplexity: 8.532
  Learning Rate: 1.89e-04
  GPU Memory Used: 11862MB

[07:21:51] Step 19200/125000:
  Training Loss: 2.238
  Perplexity: 9.374
  Learning Rate: 1.89e-04
  GPU Memory Used: 12004MB

[07:22:58] Step 19250/125000:
  Training Loss: 2.056
  Perplexity: 7.814
  Learning Rate: 1.89e-04
  GPU Memory Used: 11821MB

[07:24:09] Step 19300/125000:
  Training Loss: 2.280
  Perplexity: 9.776
  Learning Rate: 1.88e-04
  GPU Memory Used: 12150MB

[07:25:17] Step 19350/125000:
  Training Loss: 2.223
  Perplexity: 9.239
  Learning Rate: 1.88e-04
  GPU Memory Used: 12039MB

[07:26:27] Step 19400/125000:
  Training Loss: 2.309
  Perplexity: 10.068
  Learning Rate: 1.88e-04
  GPU Memory Used: 12198MB

[07:27:38] Step 19450/125000:
  Training Loss: 2.160
  Perplexity: 8.675
  Learning Rate: 1.88e-04
  GPU Memory Used: 12050MB

[07:28:47] Step 19500/125000:
  Training Loss: 2.186
  Perplexity: 8.895
  Learning Rate: 1.88e-04
  GPU Memory Used: 11820MB


Evaluation Results:
  Validation Loss: 2.257
  Validation Perplexity: 9.555

Saving model checkpoint: proposal_model/checkpoint-19500


Epoch 40/250
==================================================
[07:29:55] Step 19550/125000:
  Training Loss: 2.253
  Perplexity: 9.517
  Learning Rate: 1.88e-04
  GPU Memory Used: 11840MB

[07:31:05] Step 19600/125000:
  Training Loss: 2.271
  Perplexity: 9.688
  Learning Rate: 1.88e-04
  GPU Memory Used: 12133MB

[07:32:14] Step 19650/125000:
  Training Loss: 2.313
  Perplexity: 10.109
  Learning Rate: 1.88e-04
  GPU Memory Used: 12048MB

[07:33:22] Step 19700/125000:
  Training Loss: 2.283
  Perplexity: 9.809
  Learning Rate: 1.88e-04
  GPU Memory Used: 12081MB

[07:34:32] Step 19750/125000:
  Training Loss: 2.149
  Perplexity: 8.573
  Learning Rate: 1.88e-04
  GPU Memory Used: 12023MB

[07:35:40] Step 19800/125000:
  Training Loss: 2.155
  Perplexity: 8.631
  Learning Rate: 1.88e-04
  GPU Memory Used: 12079MB

[07:36:51] Step 19850/125000:
  Training Loss: 2.214
  Perplexity: 9.148
  Learning Rate: 1.88e-04
  GPU Memory Used: 11881MB

[07:38:00] Step 19900/125000:
  Training Loss: 2.277
  Perplexity: 9.745
  Learning Rate: 1.88e-04
  GPU Memory Used: 11933MB

[07:39:09] Step 19950/125000:
  Training Loss: 2.320
  Perplexity: 10.177
  Learning Rate: 1.88e-04
  GPU Memory Used: 12181MB

[07:40:17] Step 20000/125000:
  Training Loss: 2.218
  Perplexity: 9.185
  Learning Rate: 1.88e-04
  GPU Memory Used: 12025MB


Evaluation Results:
  Validation Loss: 2.329
  Validation Perplexity: 10.272

Saving model checkpoint: proposal_model/checkpoint-20000


Epoch 41/250
==================================================
[07:41:27] Step 20050/125000:
  Training Loss: 2.287
  Perplexity: 9.848
  Learning Rate: 1.88e-04
  GPU Memory Used: 12165MB

[07:42:36] Step 20100/125000:
  Training Loss: 2.500
  Perplexity: 12.185
  Learning Rate: 1.88e-04
  GPU Memory Used: 11813MB

[07:43:43] Step 20150/125000:
  Training Loss: 2.123
  Perplexity: 8.355
  Learning Rate: 1.87e-04
  GPU Memory Used: 12137MB

[07:44:50] Step 20200/125000:
  Training Loss: 2.212
  Perplexity: 9.131
  Learning Rate: 1.87e-04
  GPU Memory Used: 11812MB

[07:45:57] Step 20250/125000:
  Training Loss: 2.321
  Perplexity: 10.187
  Learning Rate: 1.87e-04
  GPU Memory Used: 12024MB

[07:47:08] Step 20300/125000:
  Training Loss: 2.195
  Perplexity: 8.979
  Learning Rate: 1.87e-04
  GPU Memory Used: 11957MB

[07:48:18] Step 20350/125000:
  Training Loss: 2.253
  Perplexity: 9.516
  Learning Rate: 1.87e-04
  GPU Memory Used: 12033MB

[07:49:28] Step 20400/125000:
  Training Loss: 2.098
  Perplexity: 8.147
  Learning Rate: 1.87e-04
  GPU Memory Used: 12034MB

[07:50:35] Step 20450/125000:
  Training Loss: 2.217
  Perplexity: 9.183
  Learning Rate: 1.87e-04
  GPU Memory Used: 12189MB

[07:51:47] Step 20500/125000:
  Training Loss: 2.120
  Perplexity: 8.330
  Learning Rate: 1.87e-04
  GPU Memory Used: 11879MB


Evaluation Results:
  Validation Loss: 2.314
  Validation Perplexity: 10.117

Saving model checkpoint: proposal_model/checkpoint-20500


Epoch 42/250
==================================================
[07:52:54] Step 20550/125000:
  Training Loss: 2.213
  Perplexity: 9.145
  Learning Rate: 1.87e-04
  GPU Memory Used: 12037MB

[07:54:03] Step 20600/125000:
  Training Loss: 2.280
  Perplexity: 9.774
  Learning Rate: 1.87e-04
  GPU Memory Used: 12167MB

[07:55:12] Step 20650/125000:
  Training Loss: 2.345
  Perplexity: 10.434
  Learning Rate: 1.87e-04
  GPU Memory Used: 11949MB

[07:56:24] Step 20700/125000:
  Training Loss: 2.288
  Perplexity: 9.854
  Learning Rate: 1.87e-04
  GPU Memory Used: 12171MB

[07:57:33] Step 20750/125000:
  Training Loss: 2.270
  Perplexity: 9.681
  Learning Rate: 1.87e-04
  GPU Memory Used: 11978MB

[07:58:42] Step 20800/125000:
  Training Loss: 2.213
  Perplexity: 9.139
  Learning Rate: 1.87e-04
  GPU Memory Used: 12112MB

[07:59:51] Step 20850/125000:
  Training Loss: 2.280
  Perplexity: 9.779
  Learning Rate: 1.87e-04
  GPU Memory Used: 12056MB

[08:00:57] Step 20900/125000:
  Training Loss: 2.300
  Perplexity: 9.975
  Learning Rate: 1.87e-04
  GPU Memory Used: 12104MB

[08:02:07] Step 20950/125000:
  Training Loss: 2.222
  Perplexity: 9.227
  Learning Rate: 1.86e-04
  GPU Memory Used: 11995MB

[08:03:15] Step 21000/125000:
  Training Loss: 2.190
  Perplexity: 8.937
  Learning Rate: 1.86e-04
  GPU Memory Used: 11822MB


Evaluation Results:
  Validation Loss: 2.269
  Validation Perplexity: 9.671

Saving model checkpoint: proposal_model/checkpoint-21000


Epoch 43/250
==================================================
[08:04:24] Step 21050/125000:
  Training Loss: 2.199
  Perplexity: 9.017
  Learning Rate: 1.86e-04
  GPU Memory Used: 12189MB

[08:05:30] Step 21100/125000:
  Training Loss: 2.048
  Perplexity: 7.751
  Learning Rate: 1.86e-04
  GPU Memory Used: 11808MB

[08:06:38] Step 21150/125000:
  Training Loss: 2.301
  Perplexity: 9.986
  Learning Rate: 1.86e-04
  GPU Memory Used: 11959MB

[08:07:47] Step 21200/125000:
  Training Loss: 2.122
  Perplexity: 8.347
  Learning Rate: 1.86e-04
  GPU Memory Used: 12175MB

[08:08:56] Step 21250/125000:
  Training Loss: 2.174
  Perplexity: 8.795
  Learning Rate: 1.86e-04
  GPU Memory Used: 12058MB

[08:10:07] Step 21300/125000:
  Training Loss: 2.111
  Perplexity: 8.260
  Learning Rate: 1.86e-04
  GPU Memory Used: 11842MB

[08:11:15] Step 21350/125000:
  Training Loss: 2.201
  Perplexity: 9.032
  Learning Rate: 1.86e-04
  GPU Memory Used: 12024MB

[08:12:25] Step 21400/125000:
  Training Loss: 2.177
  Perplexity: 8.820
  Learning Rate: 1.86e-04
  GPU Memory Used: 12035MB

[08:13:35] Step 21450/125000:
  Training Loss: 2.142
  Perplexity: 8.514
  Learning Rate: 1.86e-04
  GPU Memory Used: 12006MB

[08:14:44] Step 21500/125000:
  Training Loss: 2.139
  Perplexity: 8.492
  Learning Rate: 1.86e-04
  GPU Memory Used: 12106MB


Evaluation Results:
  Validation Loss: 2.353
  Validation Perplexity: 10.519

Saving model checkpoint: proposal_model/checkpoint-21500


Epoch 44/250
==================================================
[08:15:53] Step 21550/125000:
  Training Loss: 2.136
  Perplexity: 8.467
  Learning Rate: 1.86e-04
  GPU Memory Used: 11879MB

[08:17:03] Step 21600/125000:
  Training Loss: 2.295
  Perplexity: 9.923
  Learning Rate: 1.86e-04
  GPU Memory Used: 12007MB

[08:18:15] Step 21650/125000:
  Training Loss: 2.232
  Perplexity: 9.316
  Learning Rate: 1.86e-04
  GPU Memory Used: 12054MB

[08:19:24] Step 21700/125000:
  Training Loss: 2.262
  Perplexity: 9.606
  Learning Rate: 1.85e-04
  GPU Memory Used: 11993MB

[08:20:32] Step 21750/125000:
  Training Loss: 2.287
  Perplexity: 9.848
  Learning Rate: 1.85e-04
  GPU Memory Used: 12031MB

[08:21:41] Step 21800/125000:
  Training Loss: 2.359
  Perplexity: 10.585
  Learning Rate: 1.85e-04
  GPU Memory Used: 12030MB

[08:22:52] Step 21850/125000:
  Training Loss: 2.318
  Perplexity: 10.160
  Learning Rate: 1.85e-04
  GPU Memory Used: 11873MB

[08:24:02] Step 21900/125000:
  Training Loss: 2.187
  Perplexity: 8.904
  Learning Rate: 1.85e-04
  GPU Memory Used: 11933MB

[08:25:13] Step 21950/125000:
  Training Loss: 2.301
  Perplexity: 9.980
  Learning Rate: 1.85e-04
  GPU Memory Used: 11838MB

[08:26:22] Step 22000/125000:
  Training Loss: 2.137
  Perplexity: 8.477
  Learning Rate: 1.85e-04
  GPU Memory Used: 11803MB


Evaluation Results:
  Validation Loss: 2.323
  Validation Perplexity: 10.202

Saving model checkpoint: proposal_model/checkpoint-22000


Epoch 45/250
==================================================
[08:27:32] Step 22050/125000:
  Training Loss: 2.368
  Perplexity: 10.672
  Learning Rate: 1.85e-04
  GPU Memory Used: 11896MB

[08:28:41] Step 22100/125000:
  Training Loss: 2.189
  Perplexity: 8.928
  Learning Rate: 1.85e-04
  GPU Memory Used: 11824MB

[08:29:50] Step 22150/125000:
  Training Loss: 2.261
  Perplexity: 9.592
  Learning Rate: 1.85e-04
  GPU Memory Used: 11900MB

[08:31:01] Step 22200/125000:
  Training Loss: 2.126
  Perplexity: 8.379
  Learning Rate: 1.85e-04
  GPU Memory Used: 12049MB

[08:32:11] Step 22250/125000:
  Training Loss: 2.183
  Perplexity: 8.869
  Learning Rate: 1.85e-04
  GPU Memory Used: 11961MB

[08:33:21] Step 22300/125000:
  Training Loss: 2.333
  Perplexity: 10.310
  Learning Rate: 1.85e-04
  GPU Memory Used: 11914MB

[08:34:29] Step 22350/125000:
  Training Loss: 2.121
  Perplexity: 8.336
  Learning Rate: 1.85e-04
  GPU Memory Used: 12151MB

[08:35:37] Step 22400/125000:
  Training Loss: 2.210
  Perplexity: 9.112
  Learning Rate: 1.85e-04
  GPU Memory Used: 11906MB

[08:36:46] Step 22450/125000:
  Training Loss: 2.175
  Perplexity: 8.805
  Learning Rate: 1.85e-04
  GPU Memory Used: 11820MB

[08:37:53] Step 22500/125000:
  Training Loss: 2.134
  Perplexity: 8.449
  Learning Rate: 1.84e-04
  GPU Memory Used: 11875MB


Evaluation Results:
  Validation Loss: 2.163
  Validation Perplexity: 8.701

Saving model checkpoint: proposal_model/checkpoint-22500


Epoch 46/250
==================================================
[08:39:03] Step 22550/125000:
  Training Loss: 2.205
  Perplexity: 9.070
  Learning Rate: 1.84e-04
  GPU Memory Used: 11940MB

[08:40:11] Step 22600/125000:
  Training Loss: 2.069
  Perplexity: 7.918
  Learning Rate: 1.84e-04
  GPU Memory Used: 12078MB

[08:41:20] Step 22650/125000:
  Training Loss: 2.146
  Perplexity: 8.549
  Learning Rate: 1.84e-04
  GPU Memory Used: 11933MB

[08:42:28] Step 22700/125000:
  Training Loss: 2.226
  Perplexity: 9.262
  Learning Rate: 1.84e-04
  GPU Memory Used: 11888MB

[08:43:38] Step 22750/125000:
  Training Loss: 2.058
  Perplexity: 7.832
  Learning Rate: 1.84e-04
  GPU Memory Used: 11827MB

[08:44:46] Step 22800/125000:
  Training Loss: 2.117
  Perplexity: 8.303
  Learning Rate: 1.84e-04
  GPU Memory Used: 11966MB

[08:45:57] Step 22850/125000:
  Training Loss: 2.154
  Perplexity: 8.622
  Learning Rate: 1.84e-04
  GPU Memory Used: 11808MB

[08:47:06] Step 22900/125000:
  Training Loss: 2.153
  Perplexity: 8.610
  Learning Rate: 1.84e-04
  GPU Memory Used: 11870MB

[08:48:15] Step 22950/125000:
  Training Loss: 2.024
  Perplexity: 7.571
  Learning Rate: 1.84e-04
  GPU Memory Used: 11908MB

[08:49:25] Step 23000/125000:
  Training Loss: 2.087
  Perplexity: 8.060
  Learning Rate: 1.84e-04
  GPU Memory Used: 11918MB


Evaluation Results:
  Validation Loss: 2.336
  Validation Perplexity: 10.336

Saving model checkpoint: proposal_model/checkpoint-23000


Epoch 47/250
==================================================
[08:50:34] Step 23050/125000:
  Training Loss: 2.324
  Perplexity: 10.215
  Learning Rate: 1.84e-04
  GPU Memory Used: 12101MB

[08:51:43] Step 23100/125000:
  Training Loss: 2.162
  Perplexity: 8.686
  Learning Rate: 1.84e-04
  GPU Memory Used: 11841MB

[08:52:54] Step 23150/125000:
  Training Loss: 2.080
  Perplexity: 8.003
  Learning Rate: 1.84e-04
  GPU Memory Used: 12012MB

[08:54:02] Step 23200/125000:
  Training Loss: 2.151
  Perplexity: 8.589
  Learning Rate: 1.83e-04
  GPU Memory Used: 11972MB

[08:55:11] Step 23250/125000:
  Training Loss: 2.125
  Perplexity: 8.374
  Learning Rate: 1.83e-04
  GPU Memory Used: 11873MB

[08:56:20] Step 23300/125000:
  Training Loss: 2.039
  Perplexity: 7.683
  Learning Rate: 1.83e-04
  GPU Memory Used: 11966MB

[08:57:31] Step 23350/125000:
  Training Loss: 2.094
  Perplexity: 8.116
  Learning Rate: 1.83e-04
  GPU Memory Used: 12058MB

[08:58:40] Step 23400/125000:
  Training Loss: 2.103
  Perplexity: 8.189
  Learning Rate: 1.83e-04
  GPU Memory Used: 12061MB

[08:59:47] Step 23450/125000:
  Training Loss: 2.068
  Perplexity: 7.908
  Learning Rate: 1.83e-04
  GPU Memory Used: 12194MB

[09:00:56] Step 23500/125000:
  Training Loss: 2.250
  Perplexity: 9.492
  Learning Rate: 1.83e-04
  GPU Memory Used: 12015MB


Evaluation Results:
  Validation Loss: 2.210
  Validation Perplexity: 9.117

Saving model checkpoint: proposal_model/checkpoint-23500


Epoch 48/250
==================================================
[09:02:04] Step 23550/125000:
  Training Loss: 2.185
  Perplexity: 8.886
  Learning Rate: 1.83e-04
  GPU Memory Used: 12019MB

[09:03:12] Step 23600/125000:
  Training Loss: 2.102
  Perplexity: 8.182
  Learning Rate: 1.83e-04
  GPU Memory Used: 11882MB

[09:04:20] Step 23650/125000:
  Training Loss: 2.166
  Perplexity: 8.724
  Learning Rate: 1.83e-04
  GPU Memory Used: 12014MB

[09:05:29] Step 23700/125000:
  Training Loss: 1.988
  Perplexity: 7.297
  Learning Rate: 1.83e-04
  GPU Memory Used: 11883MB

[09:06:41] Step 23750/125000:
  Training Loss: 2.217
  Perplexity: 9.184
  Learning Rate: 1.83e-04
  GPU Memory Used: 12054MB

[09:07:49] Step 23800/125000:
  Training Loss: 2.188
  Perplexity: 8.920
  Learning Rate: 1.83e-04
  GPU Memory Used: 11950MB

[09:08:57] Step 23850/125000:
  Training Loss: 2.067
  Perplexity: 7.898
  Learning Rate: 1.83e-04
  GPU Memory Used: 11949MB

[09:10:05] Step 23900/125000:
  Training Loss: 2.081
  Perplexity: 8.012
  Learning Rate: 1.82e-04
  GPU Memory Used: 11832MB

[09:11:14] Step 23950/125000:
  Training Loss: 2.233
  Perplexity: 9.327
  Learning Rate: 1.82e-04
  GPU Memory Used: 11983MB

[09:12:23] Step 24000/125000:
  Training Loss: 2.132
  Perplexity: 8.433
  Learning Rate: 1.82e-04
  GPU Memory Used: 12089MB


Evaluation Results:
  Validation Loss: 2.344
  Validation Perplexity: 10.426

Saving model checkpoint: proposal_model/checkpoint-24000


Epoch 49/250
==================================================
[09:13:32] Step 24050/125000:
  Training Loss: 2.189
  Perplexity: 8.923
  Learning Rate: 1.82e-04
  GPU Memory Used: 12078MB

[09:14:43] Step 24100/125000:
  Training Loss: 2.345
  Perplexity: 10.430
  Learning Rate: 1.82e-04
  GPU Memory Used: 12043MB

[09:15:51] Step 24150/125000:
  Training Loss: 2.104
  Perplexity: 8.197
  Learning Rate: 1.82e-04
  GPU Memory Used: 12160MB

[09:17:01] Step 24200/125000:
  Training Loss: 2.099
  Perplexity: 8.158
  Learning Rate: 1.82e-04
  GPU Memory Used: 12175MB

[09:18:12] Step 24250/125000:
  Training Loss: 2.135
  Perplexity: 8.460
  Learning Rate: 1.82e-04
  GPU Memory Used: 12190MB

[09:19:20] Step 24300/125000:
  Training Loss: 2.156
  Perplexity: 8.636
  Learning Rate: 1.82e-04
  GPU Memory Used: 12186MB

[09:20:30] Step 24350/125000:
  Training Loss: 2.288
  Perplexity: 9.857
  Learning Rate: 1.82e-04
  GPU Memory Used: 12172MB

[09:21:38] Step 24400/125000:
  Training Loss: 2.128
  Perplexity: 8.396
  Learning Rate: 1.82e-04
  GPU Memory Used: 11967MB

[09:22:45] Step 24450/125000:
  Training Loss: 2.099
  Perplexity: 8.161
  Learning Rate: 1.82e-04
  GPU Memory Used: 12046MB

[09:23:52] Step 24500/125000:
  Training Loss: 2.136
  Perplexity: 8.465
  Learning Rate: 1.82e-04
  GPU Memory Used: 11928MB


Evaluation Results:
  Validation Loss: 2.243
  Validation Perplexity: 9.421

Saving model checkpoint: proposal_model/checkpoint-24500


Epoch 50/250
==================================================
[09:25:01] Step 24550/125000:
  Training Loss: 2.163
  Perplexity: 8.696
  Learning Rate: 1.82e-04
  GPU Memory Used: 11856MB

[09:26:09] Step 24600/125000:
  Training Loss: 2.119
  Perplexity: 8.321
  Learning Rate: 1.81e-04
  GPU Memory Used: 11841MB

[09:27:17] Step 24650/125000:
  Training Loss: 2.133
  Perplexity: 8.444
  Learning Rate: 1.81e-04
  GPU Memory Used: 11838MB

[09:28:26] Step 24700/125000:
  Training Loss: 2.074
  Perplexity: 7.955
  Learning Rate: 1.81e-04
  GPU Memory Used: 12185MB

[09:29:35] Step 24750/125000:
  Training Loss: 2.231
  Perplexity: 9.313
  Learning Rate: 1.81e-04
  GPU Memory Used: 11872MB

[09:30:43] Step 24800/125000:
  Training Loss: 2.030
  Perplexity: 7.616
  Learning Rate: 1.81e-04
  GPU Memory Used: 12009MB

[09:31:51] Step 24850/125000:
  Training Loss: 2.233
  Perplexity: 9.324
  Learning Rate: 1.81e-04
  GPU Memory Used: 11827MB

[09:33:02] Step 24900/125000:
  Training Loss: 2.138
  Perplexity: 8.485
  Learning Rate: 1.81e-04
  GPU Memory Used: 11949MB

[09:34:12] Step 24950/125000:
  Training Loss: 2.071
  Perplexity: 7.936
  Learning Rate: 1.81e-04
  GPU Memory Used: 11940MB

[09:35:23] Step 25000/125000:
  Training Loss: 2.228
  Perplexity: 9.280
  Learning Rate: 1.81e-04
  GPU Memory Used: 11968MB


Evaluation Results:
  Validation Loss: 2.053
  Validation Perplexity: 7.789

Saving model checkpoint: proposal_model/checkpoint-25000


Epoch 51/250
==================================================
[09:36:31] Step 25050/125000:
  Training Loss: 2.075
  Perplexity: 7.968
  Learning Rate: 1.81e-04
  GPU Memory Used: 11878MB

[09:37:41] Step 25100/125000:
  Training Loss: 2.044
  Perplexity: 7.718
  Learning Rate: 1.81e-04
  GPU Memory Used: 11997MB

[09:38:51] Step 25150/125000:
  Training Loss: 2.088
  Perplexity: 8.070
  Learning Rate: 1.81e-04
  GPU Memory Used: 12104MB

[09:40:00] Step 25200/125000:
  Training Loss: 2.130
  Perplexity: 8.413
  Learning Rate: 1.81e-04
  GPU Memory Used: 12021MB

[09:41:08] Step 25250/125000:
  Training Loss: 1.981
  Perplexity: 7.249
  Learning Rate: 1.81e-04
  GPU Memory Used: 11982MB

[09:42:17] Step 25300/125000:
  Training Loss: 2.101
  Perplexity: 8.175
  Learning Rate: 1.80e-04
  GPU Memory Used: 12071MB

[09:43:26] Step 25350/125000:
  Training Loss: 2.176
  Perplexity: 8.814
  Learning Rate: 1.80e-04
  GPU Memory Used: 11927MB

[09:44:35] Step 25400/125000:
  Training Loss: 2.175
  Perplexity: 8.802
  Learning Rate: 1.80e-04
  GPU Memory Used: 12163MB

[09:45:46] Step 25450/125000:
  Training Loss: 2.340
  Perplexity: 10.385
  Learning Rate: 1.80e-04
  GPU Memory Used: 11837MB

[09:46:53] Step 25500/125000:
  Training Loss: 2.123
  Perplexity: 8.356
  Learning Rate: 1.80e-04
  GPU Memory Used: 12011MB


Evaluation Results:
  Validation Loss: 2.276
  Validation Perplexity: 9.736

Saving model checkpoint: proposal_model/checkpoint-25500


Epoch 52/250
==================================================
[09:48:01] Step 25550/125000:
  Training Loss: 2.086
  Perplexity: 8.052
  Learning Rate: 1.80e-04
  GPU Memory Used: 12087MB

[09:49:11] Step 25600/125000:
  Training Loss: 2.002
  Perplexity: 7.401
  Learning Rate: 1.80e-04
  GPU Memory Used: 12157MB

[09:50:21] Step 25650/125000:
  Training Loss: 2.013
  Perplexity: 7.485
  Learning Rate: 1.80e-04
  GPU Memory Used: 12099MB

[09:51:27] Step 25700/125000:
  Training Loss: 2.148
  Perplexity: 8.566
  Learning Rate: 1.80e-04
  GPU Memory Used: 12015MB

[09:52:36] Step 25750/125000:
  Training Loss: 2.166
  Perplexity: 8.720
  Learning Rate: 1.80e-04
  GPU Memory Used: 11824MB

[09:53:45] Step 25800/125000:
  Training Loss: 2.162
  Perplexity: 8.687
  Learning Rate: 1.80e-04
  GPU Memory Used: 11965MB

[09:54:56] Step 25850/125000:
  Training Loss: 2.065
  Perplexity: 7.883
  Learning Rate: 1.80e-04
  GPU Memory Used: 11896MB

[09:56:04] Step 25900/125000:
  Training Loss: 2.307
  Perplexity: 10.046
  Learning Rate: 1.80e-04
  GPU Memory Used: 11987MB

[09:57:16] Step 25950/125000:
  Training Loss: 2.201
  Perplexity: 9.032
  Learning Rate: 1.79e-04
  GPU Memory Used: 12126MB

[09:58:24] Step 26000/125000:
  Training Loss: 2.199
  Perplexity: 9.016
  Learning Rate: 1.79e-04
  GPU Memory Used: 12035MB


Evaluation Results:
  Validation Loss: 2.228
  Validation Perplexity: 9.278

Saving model checkpoint: proposal_model/checkpoint-26000


Epoch 53/250
==================================================
[09:59:33] Step 26050/125000:
  Training Loss: 2.187
  Perplexity: 8.906
  Learning Rate: 1.79e-04
  GPU Memory Used: 12030MB

[10:00:41] Step 26100/125000:
  Training Loss: 2.137
  Perplexity: 8.475
  Learning Rate: 1.79e-04
  GPU Memory Used: 11995MB

[10:01:52] Step 26150/125000:
  Training Loss: 1.922
  Perplexity: 6.833
  Learning Rate: 1.79e-04
  GPU Memory Used: 12148MB

[10:03:00] Step 26200/125000:
  Training Loss: 2.208
  Perplexity: 9.099
  Learning Rate: 1.79e-04
  GPU Memory Used: 11887MB

[10:04:09] Step 26250/125000:
  Training Loss: 2.262
  Perplexity: 9.604
  Learning Rate: 1.79e-04
  GPU Memory Used: 11826MB

[10:05:17] Step 26300/125000:
  Training Loss: 2.001
  Perplexity: 7.399
  Learning Rate: 1.79e-04
  GPU Memory Used: 12157MB

[10:06:26] Step 26350/125000:
  Training Loss: 2.117
  Perplexity: 8.303
  Learning Rate: 1.79e-04
  GPU Memory Used: 12067MB

[10:07:36] Step 26400/125000:
  Training Loss: 2.173
  Perplexity: 8.785
  Learning Rate: 1.79e-04
  GPU Memory Used: 12150MB

[10:08:46] Step 26450/125000:
  Training Loss: 2.107
  Perplexity: 8.222
  Learning Rate: 1.79e-04
  GPU Memory Used: 11865MB

[10:09:54] Step 26500/125000:
  Training Loss: 2.168
  Perplexity: 8.742
  Learning Rate: 1.79e-04
  GPU Memory Used: 11884MB


Evaluation Results:
  Validation Loss: 2.296
  Validation Perplexity: 9.934

Saving model checkpoint: proposal_model/checkpoint-26500


Epoch 54/250
==================================================
[10:11:02] Step 26550/125000:
  Training Loss: 2.169
  Perplexity: 8.746
  Learning Rate: 1.79e-04
  GPU Memory Used: 11918MB

[10:12:13] Step 26600/125000:
  Training Loss: 2.059
  Perplexity: 7.838
  Learning Rate: 1.78e-04
  GPU Memory Used: 11922MB

[10:13:22] Step 26650/125000:
  Training Loss: 2.047
  Perplexity: 7.747
  Learning Rate: 1.78e-04
  GPU Memory Used: 12075MB

[10:14:31] Step 26700/125000:
  Training Loss: 2.167
  Perplexity: 8.732
  Learning Rate: 1.78e-04
  GPU Memory Used: 12120MB

[10:15:41] Step 26750/125000:
  Training Loss: 2.059
  Perplexity: 7.840
  Learning Rate: 1.78e-04
  GPU Memory Used: 11947MB

[10:16:50] Step 26800/125000:
  Training Loss: 2.055
  Perplexity: 7.803
  Learning Rate: 1.78e-04
  GPU Memory Used: 11835MB

[10:17:58] Step 26850/125000:
  Training Loss: 2.017
  Perplexity: 7.517
  Learning Rate: 1.78e-04
  GPU Memory Used: 11969MB

[10:19:06] Step 26900/125000:
  Training Loss: 2.184
  Perplexity: 8.885
  Learning Rate: 1.78e-04
  GPU Memory Used: 12048MB

[10:20:15] Step 26950/125000:
  Training Loss: 2.099
  Perplexity: 8.160
  Learning Rate: 1.78e-04
  GPU Memory Used: 12131MB

[10:21:22] Step 27000/125000:
  Training Loss: 2.038
  Perplexity: 7.673
  Learning Rate: 1.78e-04
  GPU Memory Used: 11835MB


Evaluation Results:
  Validation Loss: 2.228
  Validation Perplexity: 9.284

Saving model checkpoint: proposal_model/checkpoint-27000


Epoch 55/250
==================================================
[10:22:32] Step 27050/125000:
  Training Loss: 2.157
  Perplexity: 8.641
  Learning Rate: 1.78e-04
  GPU Memory Used: 12111MB

[10:23:43] Step 27100/125000:
  Training Loss: 2.017
  Perplexity: 7.517
  Learning Rate: 1.78e-04
  GPU Memory Used: 11907MB

[10:24:53] Step 27150/125000:
  Training Loss: 2.146
  Perplexity: 8.553
  Learning Rate: 1.78e-04
  GPU Memory Used: 11966MB

[10:26:02] Step 27200/125000:
  Training Loss: 2.101
  Perplexity: 8.177
  Learning Rate: 1.78e-04
  GPU Memory Used: 11941MB

[10:27:12] Step 27250/125000:
  Training Loss: 1.876
  Perplexity: 6.530
  Learning Rate: 1.77e-04
  GPU Memory Used: 11937MB

[10:28:22] Step 27300/125000:
  Training Loss: 2.054
  Perplexity: 7.803
  Learning Rate: 1.77e-04
  GPU Memory Used: 12027MB

[10:29:32] Step 27350/125000:
  Training Loss: 2.144
  Perplexity: 8.531
  Learning Rate: 1.77e-04
  GPU Memory Used: 12164MB

[10:30:40] Step 27400/125000:
  Training Loss: 2.100
  Perplexity: 8.168
  Learning Rate: 1.77e-04
  GPU Memory Used: 12181MB

[10:31:46] Step 27450/125000:
  Training Loss: 1.892
  Perplexity: 6.634
  Learning Rate: 1.77e-04
  GPU Memory Used: 11972MB

[10:32:57] Step 27500/125000:
  Training Loss: 2.204
  Perplexity: 9.060
  Learning Rate: 1.77e-04
  GPU Memory Used: 12015MB


Evaluation Results:
  Validation Loss: 2.048
  Validation Perplexity: 7.754

Saving model checkpoint: proposal_model/checkpoint-27500


Epoch 56/250
==================================================
[10:34:05] Step 27550/125000:
  Training Loss: 2.141
  Perplexity: 8.511
  Learning Rate: 1.77e-04
  GPU Memory Used: 11828MB

[10:35:16] Step 27600/125000:
  Training Loss: 1.959
  Perplexity: 7.089
  Learning Rate: 1.77e-04
  GPU Memory Used: 11878MB

[10:36:23] Step 27650/125000:
  Training Loss: 2.163
  Perplexity: 8.695
  Learning Rate: 1.77e-04
  GPU Memory Used: 11805MB

[10:37:33] Step 27700/125000:
  Training Loss: 2.043
  Perplexity: 7.718
  Learning Rate: 1.77e-04
  GPU Memory Used: 11977MB

[10:38:43] Step 27750/125000:
  Training Loss: 2.064
  Perplexity: 7.880
  Learning Rate: 1.77e-04
  GPU Memory Used: 12142MB

[10:39:52] Step 27800/125000:
  Training Loss: 2.210
  Perplexity: 9.118
  Learning Rate: 1.77e-04
  GPU Memory Used: 11847MB

[10:41:01] Step 27850/125000:
  Training Loss: 2.218
  Perplexity: 9.193
  Learning Rate: 1.76e-04
  GPU Memory Used: 12161MB

[10:42:09] Step 27900/125000:
  Training Loss: 2.099
  Perplexity: 8.159
  Learning Rate: 1.76e-04
  GPU Memory Used: 11871MB

[10:43:18] Step 27950/125000:
  Training Loss: 2.157
  Perplexity: 8.646
  Learning Rate: 1.76e-04
  GPU Memory Used: 12025MB

[10:44:27] Step 28000/125000:
  Training Loss: 2.088
  Perplexity: 8.065
  Learning Rate: 1.76e-04
  GPU Memory Used: 12062MB


Evaluation Results:
  Validation Loss: 2.165
  Validation Perplexity: 8.717

Saving model checkpoint: proposal_model/checkpoint-28000


Epoch 57/250
==================================================
[10:45:35] Step 28050/125000:
  Training Loss: 2.065
  Perplexity: 7.886
  Learning Rate: 1.76e-04
  GPU Memory Used: 11820MB

[10:46:46] Step 28100/125000:
  Training Loss: 1.947
  Perplexity: 7.010
  Learning Rate: 1.76e-04
  GPU Memory Used: 12021MB

[10:47:56] Step 28150/125000:
  Training Loss: 2.006
  Perplexity: 7.431
  Learning Rate: 1.76e-04
  GPU Memory Used: 11879MB

[10:49:06] Step 28200/125000:
  Training Loss: 2.049
  Perplexity: 7.762
  Learning Rate: 1.76e-04
  GPU Memory Used: 11817MB

[10:50:14] Step 28250/125000:
  Training Loss: 2.167
  Perplexity: 8.730
  Learning Rate: 1.76e-04
  GPU Memory Used: 12182MB

[10:51:25] Step 28300/125000:
  Training Loss: 2.101
  Perplexity: 8.172
  Learning Rate: 1.76e-04
  GPU Memory Used: 11879MB

[10:52:35] Step 28350/125000:
  Training Loss: 2.188
  Perplexity: 8.919
  Learning Rate: 1.76e-04
  GPU Memory Used: 12002MB

[10:53:44] Step 28400/125000:
  Training Loss: 2.219
  Perplexity: 9.201
  Learning Rate: 1.76e-04
  GPU Memory Used: 11838MB

[10:54:54] Step 28450/125000:
  Training Loss: 2.119
  Perplexity: 8.320
  Learning Rate: 1.76e-04
  GPU Memory Used: 12163MB

[10:56:03] Step 28500/125000:
  Training Loss: 2.162
  Perplexity: 8.689
  Learning Rate: 1.75e-04
  GPU Memory Used: 11939MB


Evaluation Results:
  Validation Loss: 2.253
  Validation Perplexity: 9.515

Saving model checkpoint: proposal_model/checkpoint-28500


Epoch 58/250
==================================================
[10:57:12] Step 28550/125000:
  Training Loss: 2.020
  Perplexity: 7.535
  Learning Rate: 1.75e-04
  GPU Memory Used: 12129MB

[10:58:20] Step 28600/125000:
  Training Loss: 2.071
  Perplexity: 7.936
  Learning Rate: 1.75e-04
  GPU Memory Used: 11999MB

[10:59:29] Step 28650/125000:
  Training Loss: 2.114
  Perplexity: 8.281
  Learning Rate: 1.75e-04
  GPU Memory Used: 12129MB

[11:00:37] Step 28700/125000:
  Training Loss: 2.081
  Perplexity: 8.013
  Learning Rate: 1.75e-04
  GPU Memory Used: 12066MB

[11:01:46] Step 28750/125000:
  Training Loss: 2.007
  Perplexity: 7.439
  Learning Rate: 1.75e-04
  GPU Memory Used: 12166MB

[11:02:56] Step 28800/125000:
  Training Loss: 2.077
  Perplexity: 7.982
  Learning Rate: 1.75e-04
  GPU Memory Used: 12045MB

[11:04:05] Step 28850/125000:
  Training Loss: 2.257
  Perplexity: 9.558
  Learning Rate: 1.75e-04
  GPU Memory Used: 11843MB

[11:05:15] Step 28900/125000:
  Training Loss: 1.986
  Perplexity: 7.283
  Learning Rate: 1.75e-04
  GPU Memory Used: 11846MB

[11:06:20] Step 28950/125000:
  Training Loss: 2.287
  Perplexity: 9.843
  Learning Rate: 1.75e-04
  GPU Memory Used: 12072MB

[11:07:28] Step 29000/125000:
  Training Loss: 2.097
  Perplexity: 8.142
  Learning Rate: 1.75e-04
  GPU Memory Used: 12013MB


Evaluation Results:
  Validation Loss: 2.170
  Validation Perplexity: 8.756

Saving model checkpoint: proposal_model/checkpoint-29000


Epoch 59/250
==================================================
[11:08:36] Step 29050/125000:
  Training Loss: 2.000
  Perplexity: 7.391
  Learning Rate: 1.75e-04
  GPU Memory Used: 12012MB

[11:09:45] Step 29100/125000:
  Training Loss: 2.068
  Perplexity: 7.906
  Learning Rate: 1.74e-04
  GPU Memory Used: 11986MB

[11:10:54] Step 29150/125000:
  Training Loss: 2.118
  Perplexity: 8.318
  Learning Rate: 1.74e-04
  GPU Memory Used: 11928MB

[11:12:01] Step 29200/125000:
  Training Loss: 2.045
  Perplexity: 7.726
  Learning Rate: 1.74e-04
  GPU Memory Used: 12177MB

[11:13:11] Step 29250/125000:
  Training Loss: 2.076
  Perplexity: 7.974
  Learning Rate: 1.74e-04
  GPU Memory Used: 12015MB

[11:14:21] Step 29300/125000:
  Training Loss: 1.980
  Perplexity: 7.240
  Learning Rate: 1.74e-04
  GPU Memory Used: 12033MB

[11:15:30] Step 29350/125000:
  Training Loss: 2.064
  Perplexity: 7.877
  Learning Rate: 1.74e-04
  GPU Memory Used: 12054MB

[11:16:37] Step 29400/125000:
  Training Loss: 2.015
  Perplexity: 7.500
  Learning Rate: 1.74e-04
  GPU Memory Used: 12022MB

[11:17:47] Step 29450/125000:
  Training Loss: 2.128
  Perplexity: 8.399
  Learning Rate: 1.74e-04
  GPU Memory Used: 11885MB

[11:18:57] Step 29500/125000:
  Training Loss: 1.967
  Perplexity: 7.146
  Learning Rate: 1.74e-04
  GPU Memory Used: 11852MB


Evaluation Results:
  Validation Loss: 2.142
  Validation Perplexity: 8.518

Saving model checkpoint: proposal_model/checkpoint-29500


Epoch 60/250
==================================================
[11:20:06] Step 29550/125000:
  Training Loss: 1.828
  Perplexity: 6.221
  Learning Rate: 1.74e-04
  GPU Memory Used: 12188MB

[11:21:16] Step 29600/125000:
  Training Loss: 2.042
  Perplexity: 7.710
  Learning Rate: 1.74e-04
  GPU Memory Used: 11890MB

[11:22:25] Step 29650/125000:
  Training Loss: 2.098
  Perplexity: 8.153
  Learning Rate: 1.73e-04
  GPU Memory Used: 12064MB

[11:23:33] Step 29700/125000:
  Training Loss: 2.081
  Perplexity: 8.012
  Learning Rate: 1.73e-04
  GPU Memory Used: 11989MB

[11:24:44] Step 29750/125000:
  Training Loss: 1.976
  Perplexity: 7.211
  Learning Rate: 1.73e-04
  GPU Memory Used: 11930MB

[11:25:53] Step 29800/125000:
  Training Loss: 2.002
  Perplexity: 7.405
  Learning Rate: 1.73e-04
  GPU Memory Used: 11930MB

[11:26:59] Step 29850/125000:
  Training Loss: 2.085
  Perplexity: 8.042
  Learning Rate: 1.73e-04
  GPU Memory Used: 11879MB

[11:28:08] Step 29900/125000:
  Training Loss: 2.153
  Perplexity: 8.608
  Learning Rate: 1.73e-04
  GPU Memory Used: 11973MB

[11:29:15] Step 29950/125000:
  Training Loss: 2.096
  Perplexity: 8.136
  Learning Rate: 1.73e-04
  GPU Memory Used: 11819MB

[11:30:23] Step 30000/125000:
  Training Loss: 2.066
  Perplexity: 7.897
  Learning Rate: 1.73e-04
  GPU Memory Used: 11893MB


Evaluation Results:
  Validation Loss: 2.258
  Validation Perplexity: 9.563

Saving model checkpoint: proposal_model/checkpoint-30000


Epoch 61/250
==================================================
[11:31:33] Step 30050/125000:
  Training Loss: 2.073
  Perplexity: 7.950
  Learning Rate: 1.73e-04
  GPU Memory Used: 11814MB

[11:32:44] Step 30100/125000:
  Training Loss: 2.091
  Perplexity: 8.091
  Learning Rate: 1.73e-04
  GPU Memory Used: 11938MB

[11:33:52] Step 30150/125000:
  Training Loss: 1.999
  Perplexity: 7.382
  Learning Rate: 1.73e-04
  GPU Memory Used: 11993MB

[11:35:03] Step 30200/125000:
  Training Loss: 2.099
  Perplexity: 8.159
  Learning Rate: 1.73e-04
  GPU Memory Used: 12188MB

[11:36:13] Step 30250/125000:
  Training Loss: 2.185
  Perplexity: 8.886
  Learning Rate: 1.72e-04
  GPU Memory Used: 12060MB

[11:37:22] Step 30300/125000:
  Training Loss: 2.031
  Perplexity: 7.622
  Learning Rate: 1.72e-04
  GPU Memory Used: 11908MB

[11:38:32] Step 30350/125000:
  Training Loss: 2.131
  Perplexity: 8.425
  Learning Rate: 1.72e-04
  GPU Memory Used: 12081MB

[11:39:41] Step 30400/125000:
  Training Loss: 2.032
  Perplexity: 7.630
  Learning Rate: 1.72e-04
  GPU Memory Used: 12134MB

[11:40:51] Step 30450/125000:
  Training Loss: 1.955
  Perplexity: 7.061
  Learning Rate: 1.72e-04
  GPU Memory Used: 12180MB

[11:41:59] Step 30500/125000:
  Training Loss: 2.008
  Perplexity: 7.445
  Learning Rate: 1.72e-04
  GPU Memory Used: 12028MB


Evaluation Results:
  Validation Loss: 2.166
  Validation Perplexity: 8.723

Saving model checkpoint: proposal_model/checkpoint-30500


Epoch 62/250
==================================================
[11:43:08] Step 30550/125000:
  Training Loss: 1.923
  Perplexity: 6.844
  Learning Rate: 1.72e-04
  GPU Memory Used: 12178MB

[11:44:17] Step 30600/125000:
  Training Loss: 2.036
  Perplexity: 7.659
  Learning Rate: 1.72e-04
  GPU Memory Used: 11851MB

[11:45:25] Step 30650/125000:
  Training Loss: 1.978
  Perplexity: 7.231
  Learning Rate: 1.72e-04
  GPU Memory Used: 12088MB

[11:46:33] Step 30700/125000:
  Training Loss: 2.043
  Perplexity: 7.717
  Learning Rate: 1.72e-04
  GPU Memory Used: 12182MB

[11:47:42] Step 30750/125000:
  Training Loss: 2.024
  Perplexity: 7.566
  Learning Rate: 1.72e-04
  GPU Memory Used: 12013MB

[11:48:52] Step 30800/125000:
  Training Loss: 2.099
  Perplexity: 8.156
  Learning Rate: 1.72e-04
  GPU Memory Used: 11885MB

[11:50:01] Step 30850/125000:
  Training Loss: 1.994
  Perplexity: 7.344
  Learning Rate: 1.71e-04
  GPU Memory Used: 12019MB

[11:51:10] Step 30900/125000:
  Training Loss: 2.003
  Perplexity: 7.411
  Learning Rate: 1.71e-04
  GPU Memory Used: 11841MB

[11:52:17] Step 30950/125000:
  Training Loss: 2.084
  Perplexity: 8.039
  Learning Rate: 1.71e-04
  GPU Memory Used: 11812MB

[11:53:25] Step 31000/125000:
  Training Loss: 1.977
  Perplexity: 7.218
  Learning Rate: 1.71e-04
  GPU Memory Used: 12072MB


Evaluation Results:
  Validation Loss: 2.172
  Validation Perplexity: 8.774

Saving model checkpoint: proposal_model/checkpoint-31000


Epoch 63/250
==================================================
[11:54:33] Step 31050/125000:
  Training Loss: 1.951
  Perplexity: 7.037
  Learning Rate: 1.71e-04
  GPU Memory Used: 12180MB

[11:55:43] Step 31100/125000:
  Training Loss: 1.910
  Perplexity: 6.750
  Learning Rate: 1.71e-04
  GPU Memory Used: 12084MB

[11:56:52] Step 31150/125000:
  Training Loss: 1.847
  Perplexity: 6.344
  Learning Rate: 1.71e-04
  GPU Memory Used: 11826MB

[11:57:59] Step 31200/125000:
  Training Loss: 1.964
  Perplexity: 7.131
  Learning Rate: 1.71e-04
  GPU Memory Used: 12003MB

[11:59:08] Step 31250/125000:
  Training Loss: 1.974
  Perplexity: 7.201
  Learning Rate: 1.71e-04
  GPU Memory Used: 11834MB

[12:00:19] Step 31300/125000:
  Training Loss: 2.039
  Perplexity: 7.681
  Learning Rate: 1.71e-04
  GPU Memory Used: 12007MB

[12:01:28] Step 31350/125000:
  Training Loss: 1.921
  Perplexity: 6.830
  Learning Rate: 1.71e-04
  GPU Memory Used: 12156MB

[12:02:36] Step 31400/125000:
  Training Loss: 2.017
  Perplexity: 7.517
  Learning Rate: 1.70e-04
  GPU Memory Used: 11963MB

[12:03:44] Step 31450/125000:
  Training Loss: 1.965
  Perplexity: 7.134
  Learning Rate: 1.70e-04
  GPU Memory Used: 12156MB

[12:04:54] Step 31500/125000:
  Training Loss: 1.928
  Perplexity: 6.874
  Learning Rate: 1.70e-04
  GPU Memory Used: 11934MB


Evaluation Results:
  Validation Loss: 2.121
  Validation Perplexity: 8.336

Saving model checkpoint: proposal_model/checkpoint-31500


Epoch 64/250
==================================================
[12:06:03] Step 31550/125000:
  Training Loss: 1.874
  Perplexity: 6.517
  Learning Rate: 1.70e-04
  GPU Memory Used: 12185MB

[12:07:12] Step 31600/125000:
  Training Loss: 1.888
  Perplexity: 6.607
  Learning Rate: 1.70e-04
  GPU Memory Used: 12082MB

[12:08:22] Step 31650/125000:
  Training Loss: 1.903
  Perplexity: 6.708
  Learning Rate: 1.70e-04
  GPU Memory Used: 12129MB

[12:09:30] Step 31700/125000:
  Training Loss: 1.958
  Perplexity: 7.083
  Learning Rate: 1.70e-04
  GPU Memory Used: 12142MB

[12:10:40] Step 31750/125000:
  Training Loss: 2.051
  Perplexity: 7.774
  Learning Rate: 1.70e-04
  GPU Memory Used: 12106MB

[12:11:50] Step 31800/125000:
  Training Loss: 1.948
  Perplexity: 7.015
  Learning Rate: 1.70e-04
  GPU Memory Used: 11984MB

[12:13:00] Step 31850/125000:
  Training Loss: 2.156
  Perplexity: 8.633
  Learning Rate: 1.70e-04
  GPU Memory Used: 11968MB

[12:14:08] Step 31900/125000:
  Training Loss: 1.991
  Perplexity: 7.322
  Learning Rate: 1.70e-04
  GPU Memory Used: 11839MB

[12:15:16] Step 31950/125000:
  Training Loss: 1.960
  Perplexity: 7.102
  Learning Rate: 1.69e-04
  GPU Memory Used: 12011MB

[12:16:25] Step 32000/125000:
  Training Loss: 2.091
  Perplexity: 8.090
  Learning Rate: 1.69e-04
  GPU Memory Used: 12169MB


Evaluation Results:
  Validation Loss: 2.214
  Validation Perplexity: 9.156

Saving model checkpoint: proposal_model/checkpoint-32000


Epoch 65/250
==================================================
[12:17:35] Step 32050/125000:
  Training Loss: 1.912
  Perplexity: 6.765
  Learning Rate: 1.69e-04
  GPU Memory Used: 12162MB

[12:18:45] Step 32100/125000:
  Training Loss: 1.994
  Perplexity: 7.348
  Learning Rate: 1.69e-04
  GPU Memory Used: 12125MB

[12:19:53] Step 32150/125000:
  Training Loss: 1.956
  Perplexity: 7.071
  Learning Rate: 1.69e-04
  GPU Memory Used: 11835MB

[12:21:03] Step 32200/125000:
  Training Loss: 2.050
  Perplexity: 7.764
  Learning Rate: 1.69e-04
  GPU Memory Used: 11831MB

[12:22:10] Step 32250/125000:
  Training Loss: 1.946
  Perplexity: 7.000
  Learning Rate: 1.69e-04
  GPU Memory Used: 12069MB

[12:23:19] Step 32300/125000:
  Training Loss: 2.051
  Perplexity: 7.772
  Learning Rate: 1.69e-04
  GPU Memory Used: 12190MB

[12:24:31] Step 32350/125000:
  Training Loss: 2.125
  Perplexity: 8.377
  Learning Rate: 1.69e-04
  GPU Memory Used: 11861MB

[12:25:42] Step 32400/125000:
  Training Loss: 1.934
  Perplexity: 6.917
  Learning Rate: 1.69e-04
  GPU Memory Used: 12183MB

[12:26:50] Step 32450/125000:
  Training Loss: 2.014
  Perplexity: 7.495
  Learning Rate: 1.69e-04
  GPU Memory Used: 11826MB

[12:27:58] Step 32500/125000:
  Training Loss: 1.964
  Perplexity: 7.131
  Learning Rate: 1.68e-04
  GPU Memory Used: 12176MB


Evaluation Results:
  Validation Loss: 2.164
  Validation Perplexity: 8.710

Saving model checkpoint: proposal_model/checkpoint-32500


Epoch 66/250
==================================================
[12:29:07] Step 32550/125000:
  Training Loss: 2.006
  Perplexity: 7.433
  Learning Rate: 1.68e-04
  GPU Memory Used: 12198MB

[12:30:15] Step 32600/125000:
  Training Loss: 2.048
  Perplexity: 7.749
  Learning Rate: 1.68e-04
  GPU Memory Used: 12088MB

[12:31:25] Step 32650/125000:
  Training Loss: 1.838
  Perplexity: 6.281
  Learning Rate: 1.68e-04
  GPU Memory Used: 12175MB

[12:32:34] Step 32700/125000:
  Training Loss: 1.966
  Perplexity: 7.142
  Learning Rate: 1.68e-04
  GPU Memory Used: 12197MB

[12:33:43] Step 32750/125000:
  Training Loss: 1.969
  Perplexity: 7.166
  Learning Rate: 1.68e-04
  GPU Memory Used: 11939MB

[12:34:52] Step 32800/125000:
  Training Loss: 2.039
  Perplexity: 7.685
  Learning Rate: 1.68e-04
  GPU Memory Used: 11847MB

[12:36:02] Step 32850/125000:
  Training Loss: 1.955
  Perplexity: 7.064
  Learning Rate: 1.68e-04
  GPU Memory Used: 12101MB

[12:37:09] Step 32900/125000:
  Training Loss: 1.988
  Perplexity: 7.298
  Learning Rate: 1.68e-04
  GPU Memory Used: 12082MB

[12:38:17] Step 32950/125000:
  Training Loss: 1.847
  Perplexity: 6.339
  Learning Rate: 1.68e-04
  GPU Memory Used: 11847MB

[12:39:26] Step 33000/125000:
  Training Loss: 1.977
  Perplexity: 7.218
  Learning Rate: 1.68e-04
  GPU Memory Used: 12169MB


Evaluation Results:
  Validation Loss: 2.116
  Validation Perplexity: 8.301

Saving model checkpoint: proposal_model/checkpoint-33000


Epoch 67/250
==================================================
[12:40:35] Step 33050/125000:
  Training Loss: 2.096
  Perplexity: 8.130
  Learning Rate: 1.67e-04
  GPU Memory Used: 11852MB

[12:41:44] Step 33100/125000:
  Training Loss: 2.006
  Perplexity: 7.437
  Learning Rate: 1.67e-04
  GPU Memory Used: 11871MB

[12:42:52] Step 33150/125000:
  Training Loss: 2.087
  Perplexity: 8.060
  Learning Rate: 1.67e-04
  GPU Memory Used: 12015MB

[12:44:01] Step 33200/125000:
  Training Loss: 1.816
  Perplexity: 6.149
  Learning Rate: 1.67e-04
  GPU Memory Used: 12164MB

[12:45:09] Step 33250/125000:
  Training Loss: 1.874
  Perplexity: 6.515
  Learning Rate: 1.67e-04
  GPU Memory Used: 11914MB

[12:46:20] Step 33300/125000:
  Training Loss: 1.909
  Perplexity: 6.745
  Learning Rate: 1.67e-04
  GPU Memory Used: 12098MB

[12:47:30] Step 33350/125000:
  Training Loss: 1.848
  Perplexity: 6.346
  Learning Rate: 1.67e-04
  GPU Memory Used: 12021MB

[12:48:40] Step 33400/125000:
  Training Loss: 1.903
  Perplexity: 6.708
  Learning Rate: 1.67e-04
  GPU Memory Used: 11990MB

[12:49:47] Step 33450/125000:
  Training Loss: 1.928
  Perplexity: 6.877
  Learning Rate: 1.67e-04
  GPU Memory Used: 11930MB

[12:50:56] Step 33500/125000:
  Training Loss: 1.997
  Perplexity: 7.370
  Learning Rate: 1.67e-04
  GPU Memory Used: 11887MB


Evaluation Results:
  Validation Loss: 2.062
  Validation Perplexity: 7.863

Saving model checkpoint: proposal_model/checkpoint-33500


Epoch 68/250
==================================================
[12:52:04] Step 33550/125000:
  Training Loss: 1.896
  Perplexity: 6.663
  Learning Rate: 1.67e-04
  GPU Memory Used: 11996MB

[12:53:14] Step 33600/125000:
  Training Loss: 1.972
  Perplexity: 7.187
  Learning Rate: 1.66e-04
  GPU Memory Used: 12115MB

[12:54:23] Step 33650/125000:
  Training Loss: 1.951
  Perplexity: 7.032
  Learning Rate: 1.66e-04
  GPU Memory Used: 11869MB

[12:55:33] Step 33700/125000:
  Training Loss: 2.030
  Perplexity: 7.617
  Learning Rate: 1.66e-04
  GPU Memory Used: 12107MB

[12:56:41] Step 33750/125000:
  Training Loss: 1.925
  Perplexity: 6.858
  Learning Rate: 1.66e-04
  GPU Memory Used: 12089MB

[12:57:49] Step 33800/125000:
  Training Loss: 1.984
  Perplexity: 7.275
  Learning Rate: 1.66e-04
  GPU Memory Used: 11957MB

[12:58:57] Step 33850/125000:
  Training Loss: 2.005
  Perplexity: 7.428
  Learning Rate: 1.66e-04
  GPU Memory Used: 12098MB

[13:00:07] Step 33900/125000:
  Training Loss: 1.921
  Perplexity: 6.831
  Learning Rate: 1.66e-04
  GPU Memory Used: 11834MB

[13:01:16] Step 33950/125000:
  Training Loss: 2.088
  Perplexity: 8.071
  Learning Rate: 1.66e-04
  GPU Memory Used: 11816MB

[13:02:27] Step 34000/125000:
  Training Loss: 1.939
  Perplexity: 6.954
  Learning Rate: 1.66e-04
  GPU Memory Used: 12090MB


Evaluation Results:
  Validation Loss: 2.141
  Validation Perplexity: 8.505

Saving model checkpoint: proposal_model/checkpoint-34000


Epoch 69/250
==================================================
[13:03:36] Step 34050/125000:
  Training Loss: 2.011
  Perplexity: 7.472
  Learning Rate: 1.66e-04
  GPU Memory Used: 12114MB

[13:04:47] Step 34100/125000:
  Training Loss: 1.960
  Perplexity: 7.096
  Learning Rate: 1.65e-04
  GPU Memory Used: 12106MB

[13:05:58] Step 34150/125000:
  Training Loss: 2.019
  Perplexity: 7.533
  Learning Rate: 1.65e-04
  GPU Memory Used: 11844MB

[13:07:07] Step 34200/125000:
  Training Loss: 1.908
  Perplexity: 6.740
  Learning Rate: 1.65e-04
  GPU Memory Used: 12041MB

[13:08:17] Step 34250/125000:
  Training Loss: 1.995
  Perplexity: 7.349
  Learning Rate: 1.65e-04
  GPU Memory Used: 12055MB

[13:09:27] Step 34300/125000:
  Training Loss: 2.047
  Perplexity: 7.743
  Learning Rate: 1.65e-04
  GPU Memory Used: 11864MB

[13:10:38] Step 34350/125000:
  Training Loss: 2.088
  Perplexity: 8.070
  Learning Rate: 1.65e-04
  GPU Memory Used: 11926MB

[13:11:47] Step 34400/125000:
  Training Loss: 1.903
  Perplexity: 6.704
  Learning Rate: 1.65e-04
  GPU Memory Used: 12068MB

[13:12:56] Step 34450/125000:
  Training Loss: 1.886
  Perplexity: 6.592
  Learning Rate: 1.65e-04
  GPU Memory Used: 12078MB

[13:14:04] Step 34500/125000:
  Training Loss: 1.828
  Perplexity: 6.219
  Learning Rate: 1.65e-04
  GPU Memory Used: 12115MB


Evaluation Results:
  Validation Loss: 2.045
  Validation Perplexity: 7.728

Saving model checkpoint: proposal_model/checkpoint-34500


Epoch 70/250
==================================================
[13:15:13] Step 34550/125000:
  Training Loss: 1.804
  Perplexity: 6.074
  Learning Rate: 1.65e-04
  GPU Memory Used: 11822MB

[13:16:22] Step 34600/125000:
  Training Loss: 1.825
  Perplexity: 6.201
  Learning Rate: 1.65e-04
  GPU Memory Used: 12153MB

[13:17:32] Step 34650/125000:
  Training Loss: 1.881
  Perplexity: 6.560
  Learning Rate: 1.64e-04
  GPU Memory Used: 11890MB

[13:18:41] Step 34700/125000:
  Training Loss: 1.955
  Perplexity: 7.067
  Learning Rate: 1.64e-04
  GPU Memory Used: 12046MB

[13:19:51] Step 34750/125000:
  Training Loss: 1.880
  Perplexity: 6.555
  Learning Rate: 1.64e-04
  GPU Memory Used: 11922MB

[13:21:00] Step 34800/125000:
  Training Loss: 1.819
  Perplexity: 6.168
  Learning Rate: 1.64e-04
  GPU Memory Used: 11961MB

[13:22:09] Step 34850/125000:
  Training Loss: 1.931
  Perplexity: 6.893
  Learning Rate: 1.64e-04
  GPU Memory Used: 11936MB

[13:23:17] Step 34900/125000:
  Training Loss: 1.994
  Perplexity: 7.343
  Learning Rate: 1.64e-04
  GPU Memory Used: 12175MB

[13:24:27] Step 34950/125000:
  Training Loss: 1.968
  Perplexity: 7.156
  Learning Rate: 1.64e-04
  GPU Memory Used: 11982MB

[13:25:38] Step 35000/125000:
  Training Loss: 1.859
  Perplexity: 6.416
  Learning Rate: 1.64e-04
  GPU Memory Used: 12122MB


Evaluation Results:
  Validation Loss: 1.924
  Validation Perplexity: 6.848

Saving model checkpoint: proposal_model/checkpoint-35000


Epoch 71/250
==================================================
[13:26:47] Step 35050/125000:
  Training Loss: 2.115
  Perplexity: 8.288
  Learning Rate: 1.64e-04
  GPU Memory Used: 12058MB

[13:27:54] Step 35100/125000:
  Training Loss: 1.902
  Perplexity: 6.698
  Learning Rate: 1.64e-04
  GPU Memory Used: 12124MB

[13:29:03] Step 35150/125000:
  Training Loss: 1.984
  Perplexity: 7.274
  Learning Rate: 1.63e-04
  GPU Memory Used: 12124MB

[13:30:14] Step 35200/125000:
  Training Loss: 1.971
  Perplexity: 7.175
  Learning Rate: 1.63e-04
  GPU Memory Used: 11827MB

[13:31:24] Step 35250/125000:
  Training Loss: 1.919
  Perplexity: 6.811
  Learning Rate: 1.63e-04
  GPU Memory Used: 11849MB

[13:32:32] Step 35300/125000:
  Training Loss: 2.033
  Perplexity: 7.641
  Learning Rate: 1.63e-04
  GPU Memory Used: 11878MB

[13:33:39] Step 35350/125000:
  Training Loss: 1.984
  Perplexity: 7.272
  Learning Rate: 1.63e-04
  GPU Memory Used: 12132MB

[13:34:47] Step 35400/125000:
  Training Loss: 1.930
  Perplexity: 6.889
  Learning Rate: 1.63e-04
  GPU Memory Used: 11839MB

[13:35:56] Step 35450/125000:
  Training Loss: 1.987
  Perplexity: 7.291
  Learning Rate: 1.63e-04
  GPU Memory Used: 11860MB

[13:37:06] Step 35500/125000:
  Training Loss: 1.960
  Perplexity: 7.098
  Learning Rate: 1.63e-04
  GPU Memory Used: 12010MB


Evaluation Results:
  Validation Loss: 2.164
  Validation Perplexity: 8.707

Saving model checkpoint: proposal_model/checkpoint-35500


Epoch 72/250
==================================================
[13:38:15] Step 35550/125000:
  Training Loss: 1.876
  Perplexity: 6.527
  Learning Rate: 1.63e-04
  GPU Memory Used: 11810MB

[13:39:23] Step 35600/125000:
  Training Loss: 1.836
  Perplexity: 6.270
  Learning Rate: 1.63e-04
  GPU Memory Used: 12113MB

[13:40:32] Step 35650/125000:
  Training Loss: 1.843
  Perplexity: 6.312
  Learning Rate: 1.62e-04
  GPU Memory Used: 12006MB

[13:41:41] Step 35700/125000:
  Training Loss: 1.936
  Perplexity: 6.930
  Learning Rate: 1.62e-04
  GPU Memory Used: 11876MB

[13:42:50] Step 35750/125000:
  Training Loss: 1.895
  Perplexity: 6.655
  Learning Rate: 1.62e-04
  GPU Memory Used: 11924MB

[13:43:58] Step 35800/125000:
  Training Loss: 1.947
  Perplexity: 7.004
  Learning Rate: 1.62e-04
  GPU Memory Used: 11942MB

[13:45:06] Step 35850/125000:
  Training Loss: 1.914
  Perplexity: 6.779
  Learning Rate: 1.62e-04
  GPU Memory Used: 11819MB

[13:46:15] Step 35900/125000:
  Training Loss: 1.912
  Perplexity: 6.767
  Learning Rate: 1.62e-04
  GPU Memory Used: 11990MB

[13:47:25] Step 35950/125000:
  Training Loss: 1.935
  Perplexity: 6.926
  Learning Rate: 1.62e-04
  GPU Memory Used: 12004MB

[13:48:35] Step 36000/125000:
  Training Loss: 1.857
  Perplexity: 6.403
  Learning Rate: 1.62e-04
  GPU Memory Used: 11862MB


Evaluation Results:
  Validation Loss: 1.937
  Validation Perplexity: 6.937

Saving model checkpoint: proposal_model/checkpoint-36000


Epoch 73/250
==================================================
[13:49:44] Step 36050/125000:
  Training Loss: 1.987
  Perplexity: 7.294
  Learning Rate: 1.62e-04
  GPU Memory Used: 12091MB

[13:50:54] Step 36100/125000:
  Training Loss: 1.940
  Perplexity: 6.956
  Learning Rate: 1.62e-04
  GPU Memory Used: 11895MB

[13:52:03] Step 36150/125000:
  Training Loss: 1.972
  Perplexity: 7.182
  Learning Rate: 1.61e-04
  GPU Memory Used: 11855MB

[13:53:11] Step 36200/125000:
  Training Loss: 1.873
  Perplexity: 6.509
  Learning Rate: 1.61e-04
  GPU Memory Used: 11956MB

[13:54:22] Step 36250/125000:
  Training Loss: 1.888
  Perplexity: 6.608
  Learning Rate: 1.61e-04
  GPU Memory Used: 11829MB

[13:55:31] Step 36300/125000:
  Training Loss: 1.902
  Perplexity: 6.696
  Learning Rate: 1.61e-04
  GPU Memory Used: 12127MB

[13:56:41] Step 36350/125000:
  Training Loss: 1.771
  Perplexity: 5.877
  Learning Rate: 1.61e-04
  GPU Memory Used: 11927MB

[13:57:50] Step 36400/125000:
  Training Loss: 1.912
  Perplexity: 6.763
  Learning Rate: 1.61e-04
  GPU Memory Used: 11812MB

[13:58:59] Step 36450/125000:
  Training Loss: 1.893
  Perplexity: 6.636
  Learning Rate: 1.61e-04
  GPU Memory Used: 11915MB

[14:00:07] Step 36500/125000:
  Training Loss: 1.915
  Perplexity: 6.790
  Learning Rate: 1.61e-04
  GPU Memory Used: 11967MB


Evaluation Results:
  Validation Loss: 1.935
  Validation Perplexity: 6.926

Saving model checkpoint: proposal_model/checkpoint-36500


Epoch 74/250
==================================================
[14:01:15] Step 36550/125000:
  Training Loss: 1.969
  Perplexity: 7.163
  Learning Rate: 1.61e-04
  GPU Memory Used: 11828MB

[14:02:24] Step 36600/125000:
  Training Loss: 2.009
  Perplexity: 7.455
  Learning Rate: 1.61e-04
  GPU Memory Used: 12172MB

[14:03:32] Step 36650/125000:
  Training Loss: 1.981
  Perplexity: 7.251
  Learning Rate: 1.60e-04
  GPU Memory Used: 11930MB

[14:04:42] Step 36700/125000:
  Training Loss: 1.848
  Perplexity: 6.346
  Learning Rate: 1.60e-04
  GPU Memory Used: 12036MB

[14:05:53] Step 36750/125000:
  Training Loss: 1.977
  Perplexity: 7.218
  Learning Rate: 1.60e-04
  GPU Memory Used: 11862MB

[14:07:02] Step 36800/125000:
  Training Loss: 1.876
  Perplexity: 6.529
  Learning Rate: 1.60e-04
  GPU Memory Used: 11899MB

[14:08:11] Step 36850/125000:
  Training Loss: 1.916
  Perplexity: 6.792
  Learning Rate: 1.60e-04
  GPU Memory Used: 11967MB

[14:09:19] Step 36900/125000:
  Training Loss: 1.904
  Perplexity: 6.712
  Learning Rate: 1.60e-04
  GPU Memory Used: 11811MB

[14:10:28] Step 36950/125000:
  Training Loss: 1.920
  Perplexity: 6.824
  Learning Rate: 1.60e-04
  GPU Memory Used: 11982MB

[14:11:38] Step 37000/125000:
  Training Loss: 1.924
  Perplexity: 6.845
  Learning Rate: 1.60e-04
  GPU Memory Used: 11853MB


Evaluation Results:
  Validation Loss: 1.945
  Validation Perplexity: 6.991

Saving model checkpoint: proposal_model/checkpoint-37000


Epoch 75/250
==================================================
[14:12:48] Step 37050/125000:
  Training Loss: 1.712
  Perplexity: 5.539
  Learning Rate: 1.60e-04
  GPU Memory Used: 12081MB

[14:13:59] Step 37100/125000:
  Training Loss: 1.821
  Perplexity: 6.176
  Learning Rate: 1.60e-04
  GPU Memory Used: 12096MB

[14:15:09] Step 37150/125000:
  Training Loss: 1.922
  Perplexity: 6.832
  Learning Rate: 1.59e-04
  GPU Memory Used: 11805MB

[14:16:17] Step 37200/125000:
  Training Loss: 1.942
  Perplexity: 6.974
  Learning Rate: 1.59e-04
  GPU Memory Used: 11929MB

[14:17:25] Step 37250/125000:
  Training Loss: 1.818
  Perplexity: 6.158
  Learning Rate: 1.59e-04
  GPU Memory Used: 12068MB

[14:18:36] Step 37300/125000:
  Training Loss: 1.821
  Perplexity: 6.178
  Learning Rate: 1.59e-04
  GPU Memory Used: 12050MB

[14:19:46] Step 37350/125000:
  Training Loss: 2.080
  Perplexity: 8.002
  Learning Rate: 1.59e-04
  GPU Memory Used: 11959MB

[14:20:55] Step 37400/125000:
  Training Loss: 1.780
  Perplexity: 5.931
  Learning Rate: 1.59e-04
  GPU Memory Used: 12131MB

[14:22:05] Step 37450/125000:
  Training Loss: 1.891
  Perplexity: 6.625
  Learning Rate: 1.59e-04
  GPU Memory Used: 12068MB

[14:23:15] Step 37500/125000:
  Training Loss: 1.877
  Perplexity: 6.535
  Learning Rate: 1.59e-04
  GPU Memory Used: 12043MB


Evaluation Results:
  Validation Loss: 1.975
  Validation Perplexity: 7.205

Saving model checkpoint: proposal_model/checkpoint-37500


Epoch 76/250
==================================================
[14:24:24] Step 37550/125000:
  Training Loss: 1.888
  Perplexity: 6.606
  Learning Rate: 1.59e-04
  GPU Memory Used: 11853MB

[14:25:37] Step 37600/125000:
  Training Loss: 1.770
  Perplexity: 5.870
  Learning Rate: 1.59e-04
  GPU Memory Used: 12105MB

[14:26:47] Step 37650/125000:
  Training Loss: 1.772
  Perplexity: 5.885
  Learning Rate: 1.58e-04
  GPU Memory Used: 11835MB

[14:27:56] Step 37700/125000:
  Training Loss: 1.837
  Perplexity: 6.277
  Learning Rate: 1.58e-04
  GPU Memory Used: 12181MB

[14:29:07] Step 37750/125000:
  Training Loss: 1.816
  Perplexity: 6.145
  Learning Rate: 1.58e-04
  GPU Memory Used: 11831MB

[14:30:15] Step 37800/125000:
  Training Loss: 1.980
  Perplexity: 7.240
  Learning Rate: 1.58e-04
  GPU Memory Used: 11891MB

[14:31:26] Step 37850/125000:
  Training Loss: 1.938
  Perplexity: 6.947
  Learning Rate: 1.58e-04
  GPU Memory Used: 11925MB

[14:32:35] Step 37900/125000:
  Training Loss: 1.921
  Perplexity: 6.824
  Learning Rate: 1.58e-04
  GPU Memory Used: 11945MB

[14:33:44] Step 37950/125000:
  Training Loss: 1.947
  Perplexity: 7.010
  Learning Rate: 1.58e-04
  GPU Memory Used: 12009MB

[14:34:53] Step 38000/125000:
  Training Loss: 1.943
  Perplexity: 6.983
  Learning Rate: 1.58e-04
  GPU Memory Used: 11974MB


Evaluation Results:
  Validation Loss: 1.994
  Validation Perplexity: 7.348

Saving model checkpoint: proposal_model/checkpoint-38000


Epoch 77/250
==================================================
[14:36:03] Step 38050/125000:
  Training Loss: 1.897
  Perplexity: 6.667
  Learning Rate: 1.58e-04
  GPU Memory Used: 12017MB

[14:37:12] Step 38100/125000:
  Training Loss: 1.882
  Perplexity: 6.567
  Learning Rate: 1.58e-04
  GPU Memory Used: 11979MB

[14:38:21] Step 38150/125000:
  Training Loss: 1.832
  Perplexity: 6.245
  Learning Rate: 1.57e-04
  GPU Memory Used: 12160MB

[14:39:31] Step 38200/125000:
  Training Loss: 1.818
  Perplexity: 6.161
  Learning Rate: 1.57e-04
  GPU Memory Used: 12062MB

[14:40:41] Step 38250/125000:
  Training Loss: 2.022
  Perplexity: 7.551
  Learning Rate: 1.57e-04
  GPU Memory Used: 11822MB

[14:41:51] Step 38300/125000:
  Training Loss: 1.793
  Perplexity: 6.005
  Learning Rate: 1.57e-04
  GPU Memory Used: 12180MB

[14:42:59] Step 38350/125000:
  Training Loss: 1.972
  Perplexity: 7.185
  Learning Rate: 1.57e-04
  GPU Memory Used: 12146MB

[14:44:08] Step 38400/125000:
  Training Loss: 1.864
  Perplexity: 6.453
  Learning Rate: 1.57e-04
  GPU Memory Used: 12173MB

[14:45:14] Step 38450/125000:
  Training Loss: 1.834
  Perplexity: 6.258
  Learning Rate: 1.57e-04
  GPU Memory Used: 12127MB

[14:46:24] Step 38500/125000:
  Training Loss: 1.917
  Perplexity: 6.800
  Learning Rate: 1.57e-04
  GPU Memory Used: 11999MB


Evaluation Results:
  Validation Loss: 2.083
  Validation Perplexity: 8.031

Saving model checkpoint: proposal_model/checkpoint-38500


Epoch 78/250
==================================================
[14:47:37] Step 38550/125000:
  Training Loss: 1.755
  Perplexity: 5.784
  Learning Rate: 1.57e-04
  GPU Memory Used: 11814MB

[14:48:45] Step 38600/125000:
  Training Loss: 1.827
  Perplexity: 6.213
  Learning Rate: 1.57e-04
  GPU Memory Used: 12198MB

[14:49:55] Step 38650/125000:
  Training Loss: 1.922
  Perplexity: 6.837
  Learning Rate: 1.56e-04
  GPU Memory Used: 12086MB

[14:51:05] Step 38700/125000:
  Training Loss: 1.810
  Perplexity: 6.108
  Learning Rate: 1.56e-04
  GPU Memory Used: 12057MB

[14:52:15] Step 38750/125000:
  Training Loss: 1.875
  Perplexity: 6.518
  Learning Rate: 1.56e-04
  GPU Memory Used: 11936MB

[14:53:25] Step 38800/125000:
  Training Loss: 1.874
  Perplexity: 6.515
  Learning Rate: 1.56e-04
  GPU Memory Used: 12187MB

[14:54:36] Step 38850/125000:
  Training Loss: 1.913
  Perplexity: 6.774
  Learning Rate: 1.56e-04
  GPU Memory Used: 12062MB

[14:55:45] Step 38900/125000:
  Training Loss: 1.837
  Perplexity: 6.276
  Learning Rate: 1.56e-04
  GPU Memory Used: 11824MB

[14:56:53] Step 38950/125000:
  Training Loss: 1.828
  Perplexity: 6.221
  Learning Rate: 1.56e-04
  GPU Memory Used: 12151MB

[14:58:05] Step 39000/125000:
  Training Loss: 1.915
  Perplexity: 6.789
  Learning Rate: 1.56e-04
  GPU Memory Used: 12055MB


Evaluation Results:
  Validation Loss: 1.858
  Validation Perplexity: 6.411

Saving model checkpoint: proposal_model/checkpoint-39000


Epoch 79/250
==================================================
[14:59:14] Step 39050/125000:
  Training Loss: 1.860
  Perplexity: 6.422
  Learning Rate: 1.56e-04
  GPU Memory Used: 11883MB

[15:00:23] Step 39100/125000:
  Training Loss: 1.950
  Perplexity: 7.030
  Learning Rate: 1.55e-04
  GPU Memory Used: 12012MB

[15:01:31] Step 39150/125000:
  Training Loss: 1.909
  Perplexity: 6.747
  Learning Rate: 1.55e-04
  GPU Memory Used: 11803MB

[15:02:40] Step 39200/125000:
  Training Loss: 1.848
  Perplexity: 6.347
  Learning Rate: 1.55e-04
  GPU Memory Used: 12087MB

[15:03:50] Step 39250/125000:
  Training Loss: 1.881
  Perplexity: 6.560
  Learning Rate: 1.55e-04
  GPU Memory Used: 12031MB

[15:04:59] Step 39300/125000:
  Training Loss: 1.734
  Perplexity: 5.663
  Learning Rate: 1.55e-04
  GPU Memory Used: 11951MB

[15:06:09] Step 39350/125000:
  Training Loss: 1.583
  Perplexity: 4.871
  Learning Rate: 1.55e-04
  GPU Memory Used: 12051MB

[15:07:20] Step 39400/125000:
  Training Loss: 1.873
  Perplexity: 6.506
  Learning Rate: 1.55e-04
  GPU Memory Used: 12162MB

[15:08:29] Step 39450/125000:
  Training Loss: 1.813
  Perplexity: 6.129
  Learning Rate: 1.55e-04
  GPU Memory Used: 11802MB

[15:09:37] Step 39500/125000:
  Training Loss: 1.947
  Perplexity: 7.006
  Learning Rate: 1.55e-04
  GPU Memory Used: 12151MB


Evaluation Results:
  Validation Loss: 2.071
  Validation Perplexity: 7.929

Saving model checkpoint: proposal_model/checkpoint-39500


Epoch 80/250
==================================================
[15:10:46] Step 39550/125000:
  Training Loss: 1.916
  Perplexity: 6.791
  Learning Rate: 1.55e-04
  GPU Memory Used: 12198MB

[15:11:57] Step 39600/125000:
  Training Loss: 1.827
  Perplexity: 6.217
  Learning Rate: 1.54e-04
  GPU Memory Used: 11814MB

[15:13:08] Step 39650/125000:
  Training Loss: 1.814
  Perplexity: 6.136
  Learning Rate: 1.54e-04
  GPU Memory Used: 12021MB

[15:14:17] Step 39700/125000:
  Training Loss: 1.815
  Perplexity: 6.139
  Learning Rate: 1.54e-04
  GPU Memory Used: 11946MB

[15:15:25] Step 39750/125000:
  Training Loss: 1.923
  Perplexity: 6.841
  Learning Rate: 1.54e-04
  GPU Memory Used: 11835MB

[15:16:33] Step 39800/125000:
  Training Loss: 1.948
  Perplexity: 7.016
  Learning Rate: 1.54e-04
  GPU Memory Used: 11809MB

[15:17:42] Step 39850/125000:
  Training Loss: 1.869
  Perplexity: 6.479
  Learning Rate: 1.54e-04
  GPU Memory Used: 12133MB

[15:18:52] Step 39900/125000:
  Training Loss: 1.825
  Perplexity: 6.200
  Learning Rate: 1.54e-04
  GPU Memory Used: 11965MB

[15:19:58] Step 39950/125000:
  Training Loss: 1.974
  Perplexity: 7.196
  Learning Rate: 1.54e-04
  GPU Memory Used: 12139MB

[15:21:08] Step 40000/125000:
  Training Loss: 1.845
  Perplexity: 6.329
  Learning Rate: 1.54e-04
  GPU Memory Used: 11845MB


Evaluation Results:
  Validation Loss: 1.926
  Validation Perplexity: 6.859

Saving model checkpoint: proposal_model/checkpoint-40000


Epoch 81/250
==================================================
[15:22:18] Step 40050/125000:
  Training Loss: 1.923
  Perplexity: 6.840
  Learning Rate: 1.53e-04
  GPU Memory Used: 11896MB

[15:23:28] Step 40100/125000:
  Training Loss: 1.795
  Perplexity: 6.021
  Learning Rate: 1.53e-04
  GPU Memory Used: 11998MB

[15:24:38] Step 40150/125000:
  Training Loss: 1.859
  Perplexity: 6.419
  Learning Rate: 1.53e-04
  GPU Memory Used: 12134MB

[15:25:48] Step 40200/125000:
  Training Loss: 1.853
  Perplexity: 6.376
  Learning Rate: 1.53e-04
  GPU Memory Used: 11898MB

[15:26:56] Step 40250/125000:
  Training Loss: 1.770
  Perplexity: 5.870
  Learning Rate: 1.53e-04
  GPU Memory Used: 11815MB

[15:28:06] Step 40300/125000:
  Training Loss: 1.873
  Perplexity: 6.510
  Learning Rate: 1.53e-04
  GPU Memory Used: 11935MB

[15:29:14] Step 40350/125000:
  Training Loss: 1.827
  Perplexity: 6.218
  Learning Rate: 1.53e-04
  GPU Memory Used: 12056MB

[15:30:24] Step 40400/125000:
  Training Loss: 1.830
  Perplexity: 6.231
  Learning Rate: 1.53e-04
  GPU Memory Used: 12033MB

[15:31:34] Step 40450/125000:
  Training Loss: 1.801
  Perplexity: 6.056
  Learning Rate: 1.53e-04
  GPU Memory Used: 11884MB

[15:32:41] Step 40500/125000:
  Training Loss: 1.789
  Perplexity: 5.983
  Learning Rate: 1.53e-04
  GPU Memory Used: 11826MB


Evaluation Results:
  Validation Loss: 1.976
  Validation Perplexity: 7.215

Saving model checkpoint: proposal_model/checkpoint-40500


Epoch 82/250
==================================================
[15:33:50] Step 40550/125000:
  Training Loss: 1.822
  Perplexity: 6.187
  Learning Rate: 1.52e-04
  GPU Memory Used: 11875MB

[15:35:00] Step 40600/125000:
  Training Loss: 1.837
  Perplexity: 6.275
  Learning Rate: 1.52e-04
  GPU Memory Used: 12023MB

[15:36:09] Step 40650/125000:
  Training Loss: 1.753
  Perplexity: 5.774
  Learning Rate: 1.52e-04
  GPU Memory Used: 12076MB

[15:37:17] Step 40700/125000:
  Training Loss: 1.855
  Perplexity: 6.391
  Learning Rate: 1.52e-04
  GPU Memory Used: 11814MB

[15:38:26] Step 40750/125000:
  Training Loss: 1.832
  Perplexity: 6.244
  Learning Rate: 1.52e-04
  GPU Memory Used: 11895MB

[15:39:36] Step 40800/125000:
  Training Loss: 1.763
  Perplexity: 5.832
  Learning Rate: 1.52e-04
  GPU Memory Used: 11910MB

[15:40:47] Step 40850/125000:
  Training Loss: 1.970
  Perplexity: 7.173
  Learning Rate: 1.52e-04
  GPU Memory Used: 11961MB

[15:41:57] Step 40900/125000:
  Training Loss: 1.828
  Perplexity: 6.221
  Learning Rate: 1.52e-04
  GPU Memory Used: 12129MB

[15:43:07] Step 40950/125000:
  Training Loss: 1.932
  Perplexity: 6.903
  Learning Rate: 1.52e-04
  GPU Memory Used: 12145MB

[15:44:16] Step 41000/125000:
  Training Loss: 1.935
  Perplexity: 6.922
  Learning Rate: 1.51e-04
  GPU Memory Used: 11872MB


Evaluation Results:
  Validation Loss: 2.021
  Validation Perplexity: 7.545

Saving model checkpoint: proposal_model/checkpoint-41000


Epoch 83/250
==================================================
[15:45:24] Step 41050/125000:
  Training Loss: 1.837
  Perplexity: 6.279
  Learning Rate: 1.51e-04
  GPU Memory Used: 11840MB

[15:46:33] Step 41100/125000:
  Training Loss: 1.810
  Perplexity: 6.112
  Learning Rate: 1.51e-04
  GPU Memory Used: 11953MB

[15:47:43] Step 41150/125000:
  Training Loss: 1.779
  Perplexity: 5.926
  Learning Rate: 1.51e-04
  GPU Memory Used: 11917MB

[15:48:50] Step 41200/125000:
  Training Loss: 1.851
  Perplexity: 6.365
  Learning Rate: 1.51e-04
  GPU Memory Used: 11851MB

[15:50:00] Step 41250/125000:
  Training Loss: 1.720
  Perplexity: 5.584
  Learning Rate: 1.51e-04
  GPU Memory Used: 11805MB

[15:51:09] Step 41300/125000:
  Training Loss: 1.818
  Perplexity: 6.158
  Learning Rate: 1.51e-04
  GPU Memory Used: 11845MB

[15:52:18] Step 41350/125000:
  Training Loss: 1.802
  Perplexity: 6.063
  Learning Rate: 1.51e-04
  GPU Memory Used: 11961MB

[15:53:28] Step 41400/125000:
  Training Loss: 1.789
  Perplexity: 5.982
  Learning Rate: 1.51e-04
  GPU Memory Used: 12147MB

[15:54:36] Step 41450/125000:
  Training Loss: 1.931
  Perplexity: 6.899
  Learning Rate: 1.50e-04
  GPU Memory Used: 11824MB

[15:55:44] Step 41500/125000:
  Training Loss: 1.888
  Perplexity: 6.608
  Learning Rate: 1.50e-04
  GPU Memory Used: 12160MB


Evaluation Results:
  Validation Loss: 1.924
  Validation Perplexity: 6.850

Saving model checkpoint: proposal_model/checkpoint-41500


Epoch 84/250
==================================================
[15:56:54] Step 41550/125000:
  Training Loss: 1.850
  Perplexity: 6.360
  Learning Rate: 1.50e-04
  GPU Memory Used: 11922MB

[15:58:04] Step 41600/125000:
  Training Loss: 1.794
  Perplexity: 6.013
  Learning Rate: 1.50e-04
  GPU Memory Used: 11885MB

[15:59:15] Step 41650/125000:
  Training Loss: 1.723
  Perplexity: 5.603
  Learning Rate: 1.50e-04
  GPU Memory Used: 11877MB

[16:00:25] Step 41700/125000:
  Training Loss: 1.823
  Perplexity: 6.191
  Learning Rate: 1.50e-04
  GPU Memory Used: 11923MB

[16:01:36] Step 41750/125000:
  Training Loss: 1.844
  Perplexity: 6.322
  Learning Rate: 1.50e-04
  GPU Memory Used: 11843MB

[16:02:46] Step 41800/125000:
  Training Loss: 1.833
  Perplexity: 6.252
  Learning Rate: 1.50e-04
  GPU Memory Used: 11933MB

[16:03:54] Step 41850/125000:
  Training Loss: 1.822
  Perplexity: 6.187
  Learning Rate: 1.50e-04
  GPU Memory Used: 12073MB

[16:05:01] Step 41900/125000:
  Training Loss: 1.913
  Perplexity: 6.776
  Learning Rate: 1.49e-04
  GPU Memory Used: 11949MB

[16:06:10] Step 41950/125000:
  Training Loss: 1.873
  Perplexity: 6.506
  Learning Rate: 1.49e-04
  GPU Memory Used: 11864MB

[16:07:20] Step 42000/125000:
  Training Loss: 1.890
  Perplexity: 6.620
  Learning Rate: 1.49e-04
  GPU Memory Used: 12176MB


Evaluation Results:
  Validation Loss: 1.990
  Validation Perplexity: 7.316

Saving model checkpoint: proposal_model/checkpoint-42000


Epoch 85/250
==================================================
[16:08:28] Step 42050/125000:
  Training Loss: 1.903
  Perplexity: 6.704
  Learning Rate: 1.49e-04
  GPU Memory Used: 12126MB

[16:09:35] Step 42100/125000:
  Training Loss: 1.882
  Perplexity: 6.567
  Learning Rate: 1.49e-04
  GPU Memory Used: 12122MB

[16:10:44] Step 42150/125000:
  Training Loss: 1.753
  Perplexity: 5.771
  Learning Rate: 1.49e-04
  GPU Memory Used: 11984MB

[16:11:53] Step 42200/125000:
  Training Loss: 1.800
  Perplexity: 6.049
  Learning Rate: 1.49e-04
  GPU Memory Used: 11909MB

[16:13:03] Step 42250/125000:
  Training Loss: 1.773
  Perplexity: 5.888
  Learning Rate: 1.49e-04
  GPU Memory Used: 11885MB

[16:14:13] Step 42300/125000:
  Training Loss: 1.854
  Perplexity: 6.384
  Learning Rate: 1.49e-04
  GPU Memory Used: 12138MB

[16:15:24] Step 42350/125000:
  Training Loss: 1.859
  Perplexity: 6.418
  Learning Rate: 1.49e-04
  GPU Memory Used: 12040MB

[16:16:33] Step 42400/125000:
  Training Loss: 1.818
  Perplexity: 6.158
  Learning Rate: 1.48e-04
  GPU Memory Used: 11958MB

[16:17:41] Step 42450/125000:
  Training Loss: 1.898
  Perplexity: 6.674
  Learning Rate: 1.48e-04
  GPU Memory Used: 11876MB

[16:18:51] Step 42500/125000:
  Training Loss: 1.881
  Perplexity: 6.562
  Learning Rate: 1.48e-04
  GPU Memory Used: 12051MB


Evaluation Results:
  Validation Loss: 2.018
  Validation Perplexity: 7.526

Saving model checkpoint: proposal_model/checkpoint-42500


Epoch 86/250
==================================================
[16:20:02] Step 42550/125000:
  Training Loss: 1.849
  Perplexity: 6.351
  Learning Rate: 1.48e-04
  GPU Memory Used: 11840MB

[16:21:09] Step 42600/125000:
  Training Loss: 1.785
  Perplexity: 5.960
  Learning Rate: 1.48e-04
  GPU Memory Used: 11860MB

[16:22:17] Step 42650/125000:
  Training Loss: 1.717
  Perplexity: 5.565
  Learning Rate: 1.48e-04
  GPU Memory Used: 11934MB

[16:23:25] Step 42700/125000:
  Training Loss: 1.805
  Perplexity: 6.079
  Learning Rate: 1.48e-04
  GPU Memory Used: 11879MB

[16:24:34] Step 42750/125000:
  Training Loss: 1.702
  Perplexity: 5.487
  Learning Rate: 1.48e-04
  GPU Memory Used: 12042MB

[16:25:43] Step 42800/125000:
  Training Loss: 1.810
  Perplexity: 6.108
  Learning Rate: 1.48e-04
  GPU Memory Used: 12189MB

[16:26:52] Step 42850/125000:
  Training Loss: 1.831
  Perplexity: 6.242
  Learning Rate: 1.47e-04
  GPU Memory Used: 11965MB

[16:28:02] Step 42900/125000:
  Training Loss: 1.759
  Perplexity: 5.805
  Learning Rate: 1.47e-04
  GPU Memory Used: 12062MB

[16:29:12] Step 42950/125000:
  Training Loss: 1.843
  Perplexity: 6.317
  Learning Rate: 1.47e-04
  GPU Memory Used: 11982MB

[16:30:21] Step 43000/125000:
  Training Loss: 1.892
  Perplexity: 6.634
  Learning Rate: 1.47e-04
  GPU Memory Used: 11933MB


Evaluation Results:
  Validation Loss: 1.942
  Validation Perplexity: 6.974

Saving model checkpoint: proposal_model/checkpoint-43000


Epoch 87/250
==================================================
[16:31:30] Step 43050/125000:
  Training Loss: 1.852
  Perplexity: 6.372
  Learning Rate: 1.47e-04
  GPU Memory Used: 11801MB

[16:32:39] Step 43100/125000:
  Training Loss: 1.729
  Perplexity: 5.633
  Learning Rate: 1.47e-04
  GPU Memory Used: 12179MB

[16:33:48] Step 43150/125000:
  Training Loss: 1.840
  Perplexity: 6.295
  Learning Rate: 1.47e-04
  GPU Memory Used: 11949MB

[16:34:56] Step 43200/125000:
  Training Loss: 1.818
  Perplexity: 6.159
  Learning Rate: 1.47e-04
  GPU Memory Used: 12168MB

[16:36:06] Step 43250/125000:
  Training Loss: 1.849
  Perplexity: 6.354
  Learning Rate: 1.47e-04
  GPU Memory Used: 12044MB

[16:37:14] Step 43300/125000:
  Training Loss: 1.790
  Perplexity: 5.992
  Learning Rate: 1.46e-04
  GPU Memory Used: 12052MB

[16:38:24] Step 43350/125000:
  Training Loss: 1.804
  Perplexity: 6.072
  Learning Rate: 1.46e-04
  GPU Memory Used: 11972MB

[16:39:35] Step 43400/125000:
  Training Loss: 1.836
  Perplexity: 6.271
  Learning Rate: 1.46e-04
  GPU Memory Used: 11920MB

[16:40:44] Step 43450/125000:
  Training Loss: 1.805
  Perplexity: 6.078
  Learning Rate: 1.46e-04
  GPU Memory Used: 12051MB

[16:41:53] Step 43500/125000:
  Training Loss: 1.791
  Perplexity: 5.996
  Learning Rate: 1.46e-04
  GPU Memory Used: 11901MB


Evaluation Results:
  Validation Loss: 2.036
  Validation Perplexity: 7.660

Saving model checkpoint: proposal_model/checkpoint-43500


Epoch 88/250
==================================================
[16:43:01] Step 43550/125000:
  Training Loss: 1.859
  Perplexity: 6.419
  Learning Rate: 1.46e-04
  GPU Memory Used: 12003MB

[16:44:10] Step 43600/125000:
  Training Loss: 1.761
  Perplexity: 5.817
  Learning Rate: 1.46e-04
  GPU Memory Used: 12107MB

[16:45:17] Step 43650/125000:
  Training Loss: 1.792
  Perplexity: 6.004
  Learning Rate: 1.46e-04
  GPU Memory Used: 11910MB

[16:46:30] Step 43700/125000:
  Training Loss: 1.897
  Perplexity: 6.666
  Learning Rate: 1.46e-04
  GPU Memory Used: 11953MB

[16:47:41] Step 43750/125000:
  Training Loss: 1.910
  Perplexity: 6.755
  Learning Rate: 1.45e-04
  GPU Memory Used: 12138MB

[16:48:48] Step 43800/125000:
  Training Loss: 1.751
  Perplexity: 5.760
  Learning Rate: 1.45e-04
  GPU Memory Used: 12107MB

[16:49:58] Step 43850/125000:
  Training Loss: 1.604
  Perplexity: 4.975
  Learning Rate: 1.45e-04
  GPU Memory Used: 12062MB

[16:51:08] Step 43900/125000:
  Training Loss: 1.842
  Perplexity: 6.306
  Learning Rate: 1.45e-04
  GPU Memory Used: 12070MB

[16:52:13] Step 43950/125000:
  Training Loss: 1.950
  Perplexity: 7.031
  Learning Rate: 1.45e-04
  GPU Memory Used: 11833MB

[16:53:22] Step 44000/125000:
  Training Loss: 1.737
  Perplexity: 5.679
  Learning Rate: 1.45e-04
  GPU Memory Used: 12126MB


Evaluation Results:
  Validation Loss: 1.897
  Validation Perplexity: 6.669

Saving model checkpoint: proposal_model/checkpoint-44000


Epoch 89/250
==================================================
[16:54:31] Step 44050/125000:
  Training Loss: 1.952
  Perplexity: 7.045
  Learning Rate: 1.45e-04
  GPU Memory Used: 12179MB

[16:55:40] Step 44100/125000:
  Training Loss: 1.879
  Perplexity: 6.548
  Learning Rate: 1.45e-04
  GPU Memory Used: 12007MB

[16:56:49] Step 44150/125000:
  Training Loss: 1.707
  Perplexity: 5.515
  Learning Rate: 1.45e-04
  GPU Memory Used: 12009MB

[16:57:58] Step 44200/125000:
  Training Loss: 1.745
  Perplexity: 5.728
  Learning Rate: 1.44e-04
  GPU Memory Used: 12181MB

[16:59:07] Step 44250/125000:
  Training Loss: 1.796
  Perplexity: 6.027
  Learning Rate: 1.44e-04
  GPU Memory Used: 11902MB

[17:00:17] Step 44300/125000:
  Training Loss: 1.761
  Perplexity: 5.821
  Learning Rate: 1.44e-04
  GPU Memory Used: 12043MB

[17:01:26] Step 44350/125000:
  Training Loss: 1.714
  Perplexity: 5.550
  Learning Rate: 1.44e-04
  GPU Memory Used: 11981MB

[17:02:34] Step 44400/125000:
  Training Loss: 1.709
  Perplexity: 5.522
  Learning Rate: 1.44e-04
  GPU Memory Used: 11940MB

[17:03:44] Step 44450/125000:
  Training Loss: 1.744
  Perplexity: 5.720
  Learning Rate: 1.44e-04
  GPU Memory Used: 11837MB

[17:04:53] Step 44500/125000:
  Training Loss: 1.734
  Perplexity: 5.660
  Learning Rate: 1.44e-04
  GPU Memory Used: 12006MB


Evaluation Results:
  Validation Loss: 1.883
  Validation Perplexity: 6.575

Saving model checkpoint: proposal_model/checkpoint-44500


Epoch 90/250
==================================================
[17:06:02] Step 44550/125000:
  Training Loss: 1.683
  Perplexity: 5.380
  Learning Rate: 1.44e-04
  GPU Memory Used: 12190MB

[17:07:11] Step 44600/125000:
  Training Loss: 1.729
  Perplexity: 5.638
  Learning Rate: 1.43e-04
  GPU Memory Used: 11936MB

[17:08:18] Step 44650/125000:
  Training Loss: 1.884
  Perplexity: 6.578
  Learning Rate: 1.43e-04
  GPU Memory Used: 12039MB

[17:09:27] Step 44700/125000:
  Training Loss: 1.764
  Perplexity: 5.837
  Learning Rate: 1.43e-04
  GPU Memory Used: 12106MB

[17:10:35] Step 44750/125000:
  Training Loss: 1.751
  Perplexity: 5.762
  Learning Rate: 1.43e-04
  GPU Memory Used: 12025MB

[17:11:45] Step 44800/125000:
  Training Loss: 1.615
  Perplexity: 5.026
  Learning Rate: 1.43e-04
  GPU Memory Used: 12093MB

[17:12:56] Step 44850/125000:
  Training Loss: 1.762
  Perplexity: 5.822
  Learning Rate: 1.43e-04
  GPU Memory Used: 12188MB

[17:14:04] Step 44900/125000:
  Training Loss: 1.795
  Perplexity: 6.017
  Learning Rate: 1.43e-04
  GPU Memory Used: 12054MB

[17:15:13] Step 44950/125000:
  Training Loss: 1.864
  Perplexity: 6.452
  Learning Rate: 1.43e-04
  GPU Memory Used: 11872MB

[17:16:23] Step 45000/125000:
  Training Loss: 1.740
  Perplexity: 5.699
  Learning Rate: 1.43e-04
  GPU Memory Used: 11850MB


Evaluation Results:
  Validation Loss: 1.936
  Validation Perplexity: 6.933

Saving model checkpoint: proposal_model/checkpoint-45000


Epoch 91/250
==================================================
[17:17:31] Step 45050/125000:
  Training Loss: 1.768
  Perplexity: 5.861
  Learning Rate: 1.42e-04
  GPU Memory Used: 11910MB

[17:18:39] Step 45100/125000:
  Training Loss: 1.811
  Perplexity: 6.119
  Learning Rate: 1.42e-04
  GPU Memory Used: 11920MB

[17:19:48] Step 45150/125000:
  Training Loss: 1.776
  Perplexity: 5.908
  Learning Rate: 1.42e-04
  GPU Memory Used: 12019MB

[17:20:57] Step 45200/125000:
  Training Loss: 1.723
  Perplexity: 5.602
  Learning Rate: 1.42e-04
  GPU Memory Used: 11860MB

[17:22:05] Step 45250/125000:
  Training Loss: 1.879
  Perplexity: 6.549
  Learning Rate: 1.42e-04
  GPU Memory Used: 12141MB

[17:23:12] Step 45300/125000:
  Training Loss: 1.809
  Perplexity: 6.105
  Learning Rate: 1.42e-04
  GPU Memory Used: 11875MB

[17:24:20] Step 45350/125000:
  Training Loss: 1.908
  Perplexity: 6.738
  Learning Rate: 1.42e-04
  GPU Memory Used: 11880MB

[17:25:32] Step 45400/125000:
  Training Loss: 1.812
  Perplexity: 6.120
  Learning Rate: 1.42e-04
  GPU Memory Used: 12102MB

[17:26:41] Step 45450/125000:
  Training Loss: 1.863
  Perplexity: 6.441
  Learning Rate: 1.42e-04
  GPU Memory Used: 12051MB

[17:27:50] Step 45500/125000:
  Training Loss: 1.850
  Perplexity: 6.362
  Learning Rate: 1.41e-04
  GPU Memory Used: 11998MB


Evaluation Results:
  Validation Loss: 1.732
  Validation Perplexity: 5.654

Saving model checkpoint: proposal_model/checkpoint-45500


Epoch 92/250
==================================================
[17:28:58] Step 45550/125000:
  Training Loss: 1.628
  Perplexity: 5.096
  Learning Rate: 1.41e-04
  GPU Memory Used: 12092MB

[17:30:06] Step 45600/125000:
  Training Loss: 1.799
  Perplexity: 6.046
  Learning Rate: 1.41e-04
  GPU Memory Used: 11938MB

[17:31:16] Step 45650/125000:
  Training Loss: 1.742
  Perplexity: 5.707
  Learning Rate: 1.41e-04
  GPU Memory Used: 11852MB

[17:32:26] Step 45700/125000:
  Training Loss: 1.747
  Perplexity: 5.736
  Learning Rate: 1.41e-04
  GPU Memory Used: 11886MB

[17:33:36] Step 45750/125000:
  Training Loss: 1.873
  Perplexity: 6.505
  Learning Rate: 1.41e-04
  GPU Memory Used: 12129MB

[17:34:46] Step 45800/125000:
  Training Loss: 1.777
  Perplexity: 5.914
  Learning Rate: 1.41e-04
  GPU Memory Used: 12151MB

[17:35:55] Step 45850/125000:
  Training Loss: 1.647
  Perplexity: 5.191
  Learning Rate: 1.41e-04
  GPU Memory Used: 11895MB

[17:37:03] Step 45900/125000:
  Training Loss: 1.780
  Perplexity: 5.929
  Learning Rate: 1.41e-04
  GPU Memory Used: 11969MB

[17:38:11] Step 45950/125000:
  Training Loss: 1.751
  Perplexity: 5.760
  Learning Rate: 1.40e-04
  GPU Memory Used: 12153MB

[17:39:20] Step 46000/125000:
  Training Loss: 1.833
  Perplexity: 6.253
  Learning Rate: 1.40e-04
  GPU Memory Used: 12196MB


Evaluation Results:
  Validation Loss: 1.848
  Validation Perplexity: 6.347

Saving model checkpoint: proposal_model/checkpoint-46000


Epoch 93/250
==================================================
[17:40:29] Step 46050/125000:
  Training Loss: 1.699
  Perplexity: 5.468
  Learning Rate: 1.40e-04
  GPU Memory Used: 11957MB

[17:41:39] Step 46100/125000:
  Training Loss: 1.797
  Perplexity: 6.033
  Learning Rate: 1.40e-04
  GPU Memory Used: 11809MB

[17:42:50] Step 46150/125000:
  Training Loss: 1.755
  Perplexity: 5.786
  Learning Rate: 1.40e-04
  GPU Memory Used: 11922MB

[17:43:56] Step 46200/125000:
  Training Loss: 1.799
  Perplexity: 6.041
  Learning Rate: 1.40e-04
  GPU Memory Used: 11978MB

[17:45:05] Step 46250/125000:
  Training Loss: 1.741
  Perplexity: 5.705
  Learning Rate: 1.40e-04
  GPU Memory Used: 11913MB

[17:46:14] Step 46300/125000:
  Training Loss: 1.666
  Perplexity: 5.292
  Learning Rate: 1.40e-04
  GPU Memory Used: 12152MB

[17:47:23] Step 46350/125000:
  Training Loss: 1.718
  Perplexity: 5.575
  Learning Rate: 1.39e-04
  GPU Memory Used: 12061MB

[17:48:30] Step 46400/125000:
  Training Loss: 1.842
  Perplexity: 6.307
  Learning Rate: 1.39e-04
  GPU Memory Used: 11814MB

[17:49:39] Step 46450/125000:
  Training Loss: 1.703
  Perplexity: 5.488
  Learning Rate: 1.39e-04
  GPU Memory Used: 11883MB

[17:50:49] Step 46500/125000:
  Training Loss: 1.874
  Perplexity: 6.513
  Learning Rate: 1.39e-04
  GPU Memory Used: 11933MB


Evaluation Results:
  Validation Loss: 1.888
  Validation Perplexity: 6.608

Saving model checkpoint: proposal_model/checkpoint-46500


Epoch 94/250
==================================================
[17:51:57] Step 46550/125000:
  Training Loss: 1.844
  Perplexity: 6.319
  Learning Rate: 1.39e-04
  GPU Memory Used: 11811MB

[17:53:06] Step 46600/125000:
  Training Loss: 1.684
  Perplexity: 5.386
  Learning Rate: 1.39e-04
  GPU Memory Used: 11962MB

[17:54:18] Step 46650/125000:
  Training Loss: 1.791
  Perplexity: 5.995
  Learning Rate: 1.39e-04
  GPU Memory Used: 11910MB

[17:55:26] Step 46700/125000:
  Training Loss: 1.792
  Perplexity: 6.002
  Learning Rate: 1.39e-04
  GPU Memory Used: 11958MB

[17:56:35] Step 46750/125000:
  Training Loss: 1.766
  Perplexity: 5.849
  Learning Rate: 1.39e-04
  GPU Memory Used: 12179MB

[17:57:45] Step 46800/125000:
  Training Loss: 1.712
  Perplexity: 5.539
  Learning Rate: 1.38e-04
  GPU Memory Used: 11822MB

[17:58:54] Step 46850/125000:
  Training Loss: 1.578
  Perplexity: 4.844
  Learning Rate: 1.38e-04
  GPU Memory Used: 11840MB

[17:59:59] Step 46900/125000:
  Training Loss: 1.704
  Perplexity: 5.497
  Learning Rate: 1.38e-04
  GPU Memory Used: 11843MB

[18:01:09] Step 46950/125000:
  Training Loss: 1.870
  Perplexity: 6.485
  Learning Rate: 1.38e-04
  GPU Memory Used: 12146MB

[18:02:17] Step 47000/125000:
  Training Loss: 1.807
  Perplexity: 6.090
  Learning Rate: 1.38e-04
  GPU Memory Used: 12156MB


Evaluation Results:
  Validation Loss: 1.883
  Validation Perplexity: 6.576

Saving model checkpoint: proposal_model/checkpoint-47000


Epoch 95/250
==================================================
[18:03:28] Step 47050/125000:
  Training Loss: 1.839
  Perplexity: 6.290
  Learning Rate: 1.38e-04
  GPU Memory Used: 12157MB

[18:04:37] Step 47100/125000:
  Training Loss: 1.721
  Perplexity: 5.588
  Learning Rate: 1.38e-04
  GPU Memory Used: 12093MB

[18:05:46] Step 47150/125000:
  Training Loss: 1.818
  Perplexity: 6.161
  Learning Rate: 1.38e-04
  GPU Memory Used: 12059MB

[18:06:53] Step 47200/125000:
  Training Loss: 1.776
  Perplexity: 5.904
  Learning Rate: 1.38e-04
  GPU Memory Used: 11827MB

[18:08:00] Step 47250/125000:
  Training Loss: 1.688
  Perplexity: 5.410
  Learning Rate: 1.37e-04
  GPU Memory Used: 11990MB

[18:09:09] Step 47300/125000:
  Training Loss: 1.701
  Perplexity: 5.481
  Learning Rate: 1.37e-04
  GPU Memory Used: 12164MB

[18:10:19] Step 47350/125000:
  Training Loss: 1.775
  Perplexity: 5.900
  Learning Rate: 1.37e-04
  GPU Memory Used: 11956MB

[18:11:29] Step 47400/125000:
  Training Loss: 1.691
  Perplexity: 5.427
  Learning Rate: 1.37e-04
  GPU Memory Used: 11856MB

[18:12:38] Step 47450/125000:
  Training Loss: 1.703
  Perplexity: 5.493
  Learning Rate: 1.37e-04
  GPU Memory Used: 12164MB

[18:13:48] Step 47500/125000:
  Training Loss: 1.732
  Perplexity: 5.654
  Learning Rate: 1.37e-04
  GPU Memory Used: 11801MB


Evaluation Results:
  Validation Loss: 1.780
  Validation Perplexity: 5.933

Saving model checkpoint: proposal_model/checkpoint-47500


Epoch 96/250
==================================================
[18:14:57] Step 47550/125000:
  Training Loss: 1.735
  Perplexity: 5.668
  Learning Rate: 1.37e-04
  GPU Memory Used: 11821MB

[18:16:06] Step 47600/125000:
  Training Loss: 1.677
  Perplexity: 5.348
  Learning Rate: 1.37e-04
  GPU Memory Used: 11906MB

[18:17:17] Step 47650/125000:
  Training Loss: 1.797
  Perplexity: 6.030
  Learning Rate: 1.36e-04
  GPU Memory Used: 12003MB

[18:18:24] Step 47700/125000:
  Training Loss: 1.870
  Perplexity: 6.486
  Learning Rate: 1.36e-04
  GPU Memory Used: 11991MB

[18:19:33] Step 47750/125000:
  Training Loss: 1.645
  Perplexity: 5.180
  Learning Rate: 1.36e-04
  GPU Memory Used: 12129MB

[18:20:42] Step 47800/125000:
  Training Loss: 1.845
  Perplexity: 6.329
  Learning Rate: 1.36e-04
  GPU Memory Used: 12195MB

[18:21:51] Step 47850/125000:
  Training Loss: 1.808
  Perplexity: 6.101
  Learning Rate: 1.36e-04
  GPU Memory Used: 11860MB

[18:23:00] Step 47900/125000:
  Training Loss: 1.702
  Perplexity: 5.486
  Learning Rate: 1.36e-04
  GPU Memory Used: 11997MB

[18:24:08] Step 47950/125000:
  Training Loss: 1.832
  Perplexity: 6.248
  Learning Rate: 1.36e-04
  GPU Memory Used: 11926MB

[18:25:16] Step 48000/125000:
  Training Loss: 1.750
  Perplexity: 5.754
  Learning Rate: 1.36e-04
  GPU Memory Used: 12004MB


Evaluation Results:
  Validation Loss: 1.836
  Validation Perplexity: 6.271

Saving model checkpoint: proposal_model/checkpoint-48000


Epoch 97/250
==================================================
[18:26:26] Step 48050/125000:
  Training Loss: 1.710
  Perplexity: 5.527
  Learning Rate: 1.36e-04
  GPU Memory Used: 12193MB

[18:27:35] Step 48100/125000:
  Training Loss: 1.875
  Perplexity: 6.518
  Learning Rate: 1.35e-04
  GPU Memory Used: 11994MB

[18:28:44] Step 48150/125000:
  Training Loss: 1.681
  Perplexity: 5.373
  Learning Rate: 1.35e-04
  GPU Memory Used: 12086MB

[18:29:52] Step 48200/125000:
  Training Loss: 1.829
  Perplexity: 6.228
  Learning Rate: 1.35e-04
  GPU Memory Used: 12015MB

[18:31:01] Step 48250/125000:
  Training Loss: 1.800
  Perplexity: 6.052
  Learning Rate: 1.35e-04
  GPU Memory Used: 11949MB

[18:32:11] Step 48300/125000:
  Training Loss: 1.663
  Perplexity: 5.275
  Learning Rate: 1.35e-04
  GPU Memory Used: 12064MB

[18:33:21] Step 48350/125000:
  Training Loss: 1.911
  Perplexity: 6.759
  Learning Rate: 1.35e-04
  GPU Memory Used: 11961MB

[18:34:30] Step 48400/125000:
  Training Loss: 1.752
  Perplexity: 5.764
  Learning Rate: 1.35e-04
  GPU Memory Used: 12164MB

[18:35:40] Step 48450/125000:
  Training Loss: 1.738
  Perplexity: 5.684
  Learning Rate: 1.35e-04
  GPU Memory Used: 11856MB

[18:36:50] Step 48500/125000:
  Training Loss: 1.784
  Perplexity: 5.953
  Learning Rate: 1.34e-04
  GPU Memory Used: 11897MB


Evaluation Results:
  Validation Loss: 1.823
  Validation Perplexity: 6.190

Saving model checkpoint: proposal_model/checkpoint-48500


Epoch 98/250
==================================================
[18:37:59] Step 48550/125000:
  Training Loss: 1.924
  Perplexity: 6.850
  Learning Rate: 1.34e-04
  GPU Memory Used: 11962MB

[18:39:10] Step 48600/125000:
  Training Loss: 1.665
  Perplexity: 5.288
  Learning Rate: 1.34e-04
  GPU Memory Used: 12188MB

[18:40:19] Step 48650/125000:
  Training Loss: 1.850
  Perplexity: 6.360
  Learning Rate: 1.34e-04
  GPU Memory Used: 12179MB

[18:41:27] Step 48700/125000:
  Training Loss: 1.693
  Perplexity: 5.434
  Learning Rate: 1.34e-04
  GPU Memory Used: 11899MB

[18:42:35] Step 48750/125000:
  Training Loss: 1.764
  Perplexity: 5.835
  Learning Rate: 1.34e-04
  GPU Memory Used: 11902MB

[18:43:42] Step 48800/125000:
  Training Loss: 1.757
  Perplexity: 5.794
  Learning Rate: 1.34e-04
  GPU Memory Used: 11818MB

[18:44:50] Step 48850/125000:
  Training Loss: 1.806
  Perplexity: 6.083
  Learning Rate: 1.34e-04
  GPU Memory Used: 11988MB

[18:46:00] Step 48900/125000:
  Training Loss: 1.800
  Perplexity: 6.050
  Learning Rate: 1.34e-04
  GPU Memory Used: 12104MB

[18:47:07] Step 48950/125000:
  Training Loss: 1.664
  Perplexity: 5.282
  Learning Rate: 1.33e-04
  GPU Memory Used: 11856MB

[18:48:15] Step 49000/125000:
  Training Loss: 1.671
  Perplexity: 5.316
  Learning Rate: 1.33e-04
  GPU Memory Used: 11967MB


Evaluation Results:
  Validation Loss: 1.854
  Validation Perplexity: 6.386

Saving model checkpoint: proposal_model/checkpoint-49000


Epoch 99/250
==================================================
[18:49:22] Step 49050/125000:
  Training Loss: 1.783
  Perplexity: 5.949
  Learning Rate: 1.33e-04
  GPU Memory Used: 11975MB

[18:50:31] Step 49100/125000:
  Training Loss: 1.818
  Perplexity: 6.157
  Learning Rate: 1.33e-04
  GPU Memory Used: 11973MB

[18:51:42] Step 49150/125000:
  Training Loss: 1.806
  Perplexity: 6.085
  Learning Rate: 1.33e-04
  GPU Memory Used: 12049MB

[18:52:52] Step 49200/125000:
  Training Loss: 1.682
  Perplexity: 5.377
  Learning Rate: 1.33e-04
  GPU Memory Used: 12105MB

[18:53:59] Step 49250/125000:
  Training Loss: 1.690
  Perplexity: 5.417
  Learning Rate: 1.33e-04
  GPU Memory Used: 12015MB

[18:55:09] Step 49300/125000:
  Training Loss: 1.686
  Perplexity: 5.400
  Learning Rate: 1.33e-04
  GPU Memory Used: 11984MB

[18:56:18] Step 49350/125000:
  Training Loss: 1.778
  Perplexity: 5.919
  Learning Rate: 1.32e-04
  GPU Memory Used: 12115MB

[18:57:26] Step 49400/125000:
  Training Loss: 1.762
  Perplexity: 5.825
  Learning Rate: 1.32e-04
  GPU Memory Used: 11802MB

[18:58:34] Step 49450/125000:
  Training Loss: 1.734
  Perplexity: 5.665
  Learning Rate: 1.32e-04
  GPU Memory Used: 11843MB

[18:59:44] Step 49500/125000:
  Training Loss: 1.901
  Perplexity: 6.691
  Learning Rate: 1.32e-04
  GPU Memory Used: 12133MB


Evaluation Results:
  Validation Loss: 1.937
  Validation Perplexity: 6.941

Saving model checkpoint: proposal_model/checkpoint-49500


Epoch 100/250
==================================================
[19:00:52] Step 49550/125000:
  Training Loss: 1.690
  Perplexity: 5.418
  Learning Rate: 1.32e-04
  GPU Memory Used: 12001MB

[19:02:01] Step 49600/125000:
  Training Loss: 1.781
  Perplexity: 5.938
  Learning Rate: 1.32e-04
  GPU Memory Used: 11936MB

[19:03:09] Step 49650/125000:
  Training Loss: 1.759
  Perplexity: 5.806
  Learning Rate: 1.32e-04
  GPU Memory Used: 11894MB

[19:04:20] Step 49700/125000:
  Training Loss: 1.655
  Perplexity: 5.233
  Learning Rate: 1.32e-04
  GPU Memory Used: 11960MB

[19:05:29] Step 49750/125000:
  Training Loss: 1.844
  Perplexity: 6.321
  Learning Rate: 1.31e-04
  GPU Memory Used: 12034MB

[19:06:40] Step 49800/125000:
  Training Loss: 1.631
  Perplexity: 5.109
  Learning Rate: 1.31e-04
  GPU Memory Used: 11895MB

[19:07:48] Step 49850/125000:
  Training Loss: 1.755
  Perplexity: 5.786
  Learning Rate: 1.31e-04
  GPU Memory Used: 12058MB

[19:08:57] Step 49900/125000:
  Training Loss: 1.685
  Perplexity: 5.391
  Learning Rate: 1.31e-04
  GPU Memory Used: 11843MB

[19:10:07] Step 49950/125000:
  Training Loss: 1.728
  Perplexity: 5.630
  Learning Rate: 1.31e-04
  GPU Memory Used: 11919MB

[19:11:15] Step 50000/125000:
  Training Loss: 1.697
  Perplexity: 5.459
  Learning Rate: 1.31e-04
  GPU Memory Used: 12061MB


Evaluation Results:
  Validation Loss: 1.835
  Validation Perplexity: 6.268

Saving model checkpoint: proposal_model/checkpoint-50000


Epoch 101/250
==================================================
[19:12:24] Step 50050/125000:
  Training Loss: 1.626
  Perplexity: 5.082
  Learning Rate: 1.31e-04
  GPU Memory Used: 11959MB

[19:13:32] Step 50100/125000:
  Training Loss: 1.629
  Perplexity: 5.097
  Learning Rate: 1.31e-04
  GPU Memory Used: 11801MB

[19:14:39] Step 50150/125000:
  Training Loss: 1.740
  Perplexity: 5.699
  Learning Rate: 1.31e-04
  GPU Memory Used: 11956MB

[19:15:49] Step 50200/125000:
  Training Loss: 1.605
  Perplexity: 4.979
  Learning Rate: 1.30e-04
  GPU Memory Used: 12104MB

[19:16:58] Step 50250/125000:
  Training Loss: 1.679
  Perplexity: 5.360
  Learning Rate: 1.30e-04
  GPU Memory Used: 12114MB

[19:18:07] Step 50300/125000:
  Training Loss: 1.600
  Perplexity: 4.952
  Learning Rate: 1.30e-04
  GPU Memory Used: 12104MB

[19:19:17] Step 50350/125000:
  Training Loss: 1.736
  Perplexity: 5.673
  Learning Rate: 1.30e-04
  GPU Memory Used: 11862MB

[19:20:26] Step 50400/125000:
  Training Loss: 1.686
  Perplexity: 5.400
  Learning Rate: 1.30e-04
  GPU Memory Used: 12170MB

[19:21:36] Step 50450/125000:
  Training Loss: 1.662
  Perplexity: 5.269
  Learning Rate: 1.30e-04
  GPU Memory Used: 12129MB

[19:22:45] Step 50500/125000:
  Training Loss: 1.727
  Perplexity: 5.625
  Learning Rate: 1.30e-04
  GPU Memory Used: 12059MB


Evaluation Results:
  Validation Loss: 1.905
  Validation Perplexity: 6.716

Saving model checkpoint: proposal_model/checkpoint-50500


Epoch 102/250
==================================================
[19:23:54] Step 50550/125000:
  Training Loss: 1.710
  Perplexity: 5.527
  Learning Rate: 1.30e-04
  GPU Memory Used: 11838MB

[19:25:02] Step 50600/125000:
  Training Loss: 1.779
  Perplexity: 5.926
  Learning Rate: 1.29e-04
  GPU Memory Used: 12133MB

[19:26:10] Step 50650/125000:
  Training Loss: 1.685
  Perplexity: 5.394
  Learning Rate: 1.29e-04
  GPU Memory Used: 11920MB

[19:27:19] Step 50700/125000:
  Training Loss: 1.738
  Perplexity: 5.683
  Learning Rate: 1.29e-04
  GPU Memory Used: 12183MB

[19:28:30] Step 50750/125000:
  Training Loss: 1.786
  Perplexity: 5.968
  Learning Rate: 1.29e-04
  GPU Memory Used: 12116MB

[19:29:41] Step 50800/125000:
  Training Loss: 1.664
  Perplexity: 5.281
  Learning Rate: 1.29e-04
  GPU Memory Used: 12015MB

[19:30:50] Step 50850/125000:
  Training Loss: 1.802
  Perplexity: 6.064
  Learning Rate: 1.29e-04
  GPU Memory Used: 11892MB

[19:31:58] Step 50900/125000:
  Training Loss: 1.719
  Perplexity: 5.580
  Learning Rate: 1.29e-04
  GPU Memory Used: 12189MB

[19:33:08] Step 50950/125000:
  Training Loss: 1.777
  Perplexity: 5.910
  Learning Rate: 1.29e-04
  GPU Memory Used: 11995MB

[19:34:17] Step 51000/125000:
  Training Loss: 1.881
  Perplexity: 6.559
  Learning Rate: 1.29e-04
  GPU Memory Used: 11848MB


Evaluation Results:
  Validation Loss: 1.795
  Validation Perplexity: 6.017

Saving model checkpoint: proposal_model/checkpoint-51000


Epoch 103/250
==================================================
[19:35:27] Step 51050/125000:
  Training Loss: 1.713
  Perplexity: 5.547
  Learning Rate: 1.28e-04
  GPU Memory Used: 11904MB

[19:36:36] Step 51100/125000:
  Training Loss: 1.599
  Perplexity: 4.949
  Learning Rate: 1.28e-04
  GPU Memory Used: 12043MB

[19:37:46] Step 51150/125000:
  Training Loss: 1.871
  Perplexity: 6.492
  Learning Rate: 1.28e-04
  GPU Memory Used: 12092MB

[19:38:56] Step 51200/125000:
  Training Loss: 1.692
  Perplexity: 5.429
  Learning Rate: 1.28e-04
  GPU Memory Used: 12149MB

[19:40:04] Step 51250/125000:
  Training Loss: 1.733
  Perplexity: 5.658
  Learning Rate: 1.28e-04
  GPU Memory Used: 12014MB

[19:41:14] Step 51300/125000:
  Training Loss: 1.807
  Perplexity: 6.091
  Learning Rate: 1.28e-04
  GPU Memory Used: 11837MB

[19:42:24] Step 51350/125000:
  Training Loss: 1.700
  Perplexity: 5.472
  Learning Rate: 1.28e-04
  GPU Memory Used: 11844MB

[19:43:32] Step 51400/125000:
  Training Loss: 1.716
  Perplexity: 5.561
  Learning Rate: 1.28e-04
  GPU Memory Used: 12086MB

[19:44:42] Step 51450/125000:
  Training Loss: 1.764
  Perplexity: 5.839
  Learning Rate: 1.27e-04
  GPU Memory Used: 11884MB

[19:45:52] Step 51500/125000:
  Training Loss: 1.839
  Perplexity: 6.288
  Learning Rate: 1.27e-04
  GPU Memory Used: 12185MB


Evaluation Results:
  Validation Loss: 1.883
  Validation Perplexity: 6.573

Saving model checkpoint: proposal_model/checkpoint-51500


Epoch 104/250
==================================================
[19:47:01] Step 51550/125000:
  Training Loss: 1.690
  Perplexity: 5.419
  Learning Rate: 1.27e-04
  GPU Memory Used: 12115MB

[19:48:11] Step 51600/125000:
  Training Loss: 1.791
  Perplexity: 5.997
  Learning Rate: 1.27e-04
  GPU Memory Used: 11809MB

[19:49:22] Step 51650/125000:
  Training Loss: 1.686
  Perplexity: 5.399
  Learning Rate: 1.27e-04
  GPU Memory Used: 11940MB

[19:50:31] Step 51700/125000:
  Training Loss: 1.637
  Perplexity: 5.140
  Learning Rate: 1.27e-04
  GPU Memory Used: 11924MB

[19:51:41] Step 51750/125000:
  Training Loss: 1.693
  Perplexity: 5.437
  Learning Rate: 1.27e-04
  GPU Memory Used: 11891MB

[19:52:49] Step 51800/125000:
  Training Loss: 1.843
  Perplexity: 6.316
  Learning Rate: 1.27e-04
  GPU Memory Used: 12037MB

[19:54:01] Step 51850/125000:
  Training Loss: 1.845
  Perplexity: 6.330
  Learning Rate: 1.26e-04
  GPU Memory Used: 12188MB

[19:55:11] Step 51900/125000:
  Training Loss: 1.718
  Perplexity: 5.576
  Learning Rate: 1.26e-04
  GPU Memory Used: 11876MB

[19:56:21] Step 51950/125000:
  Training Loss: 1.915
  Perplexity: 6.788
  Learning Rate: 1.26e-04
  GPU Memory Used: 12005MB

[19:57:30] Step 52000/125000:
  Training Loss: 1.781
  Perplexity: 5.935
  Learning Rate: 1.26e-04
  GPU Memory Used: 11876MB


Evaluation Results:
  Validation Loss: 1.874
  Validation Perplexity: 6.514

Saving model checkpoint: proposal_model/checkpoint-52000


Epoch 105/250
==================================================
[19:58:40] Step 52050/125000:
  Training Loss: 1.621
  Perplexity: 5.059
  Learning Rate: 1.26e-04
  GPU Memory Used: 12142MB

[19:59:47] Step 52100/125000:
  Training Loss: 1.703
  Perplexity: 5.490
  Learning Rate: 1.26e-04
  GPU Memory Used: 12146MB

[20:00:57] Step 52150/125000:
  Training Loss: 1.664
  Perplexity: 5.281
  Learning Rate: 1.26e-04
  GPU Memory Used: 12176MB

[20:02:06] Step 52200/125000:
  Training Loss: 1.581
  Perplexity: 4.862
  Learning Rate: 1.26e-04
  GPU Memory Used: 11915MB

[20:03:15] Step 52250/125000:
  Training Loss: 1.774
  Perplexity: 5.893
  Learning Rate: 1.25e-04
  GPU Memory Used: 11913MB

[20:04:24] Step 52300/125000:
  Training Loss: 1.690
  Perplexity: 5.418
  Learning Rate: 1.25e-04
  GPU Memory Used: 12026MB

[20:05:32] Step 52350/125000:
  Training Loss: 1.798
  Perplexity: 6.040
  Learning Rate: 1.25e-04
  GPU Memory Used: 11813MB

[20:06:42] Step 52400/125000:
  Training Loss: 1.625
  Perplexity: 5.080
  Learning Rate: 1.25e-04
  GPU Memory Used: 12016MB

[20:07:50] Step 52450/125000:
  Training Loss: 1.788
  Perplexity: 5.977
  Learning Rate: 1.25e-04
  GPU Memory Used: 12069MB

[20:08:57] Step 52500/125000:
  Training Loss: 1.733
  Perplexity: 5.660
  Learning Rate: 1.25e-04
  GPU Memory Used: 11995MB


Evaluation Results:
  Validation Loss: 1.731
  Validation Perplexity: 5.647

Saving model checkpoint: proposal_model/checkpoint-52500


Epoch 106/250
==================================================
[20:10:07] Step 52550/125000:
  Training Loss: 1.700
  Perplexity: 5.476
  Learning Rate: 1.25e-04
  GPU Memory Used: 12043MB

[20:11:16] Step 52600/125000:
  Training Loss: 1.834
  Perplexity: 6.259
  Learning Rate: 1.25e-04
  GPU Memory Used: 12117MB

[20:12:25] Step 52650/125000:
  Training Loss: 1.794
  Perplexity: 6.012
  Learning Rate: 1.25e-04
  GPU Memory Used: 11972MB

[20:13:35] Step 52700/125000:
  Training Loss: 1.709
  Perplexity: 5.525
  Learning Rate: 1.24e-04
  GPU Memory Used: 12036MB

[20:14:44] Step 52750/125000:
  Training Loss: 1.522
  Perplexity: 4.583
  Learning Rate: 1.24e-04
  GPU Memory Used: 12107MB

[20:15:53] Step 52800/125000:
  Training Loss: 1.631
  Perplexity: 5.109
  Learning Rate: 1.24e-04
  GPU Memory Used: 12100MB

[20:17:02] Step 52850/125000:
  Training Loss: 1.704
  Perplexity: 5.496
  Learning Rate: 1.24e-04
  GPU Memory Used: 12146MB

[20:18:09] Step 52900/125000:
  Training Loss: 1.625
  Perplexity: 5.079
  Learning Rate: 1.24e-04
  GPU Memory Used: 12005MB

[20:19:18] Step 52950/125000:
  Training Loss: 1.642
  Perplexity: 5.166
  Learning Rate: 1.24e-04
  GPU Memory Used: 12194MB

[20:20:28] Step 53000/125000:
  Training Loss: 1.581
  Perplexity: 4.862
  Learning Rate: 1.24e-04
  GPU Memory Used: 11910MB


Evaluation Results:
  Validation Loss: 1.909
  Validation Perplexity: 6.746

Saving model checkpoint: proposal_model/checkpoint-53000


Epoch 107/250
==================================================
[20:21:37] Step 53050/125000:
  Training Loss: 1.860
  Perplexity: 6.426
  Learning Rate: 1.24e-04
  GPU Memory Used: 12131MB

[20:22:49] Step 53100/125000:
  Training Loss: 1.781
  Perplexity: 5.935
  Learning Rate: 1.23e-04
  GPU Memory Used: 12190MB

[20:23:57] Step 53150/125000:
  Training Loss: 1.676
  Perplexity: 5.342
  Learning Rate: 1.23e-04
  GPU Memory Used: 12123MB

[20:25:07] Step 53200/125000:
  Training Loss: 1.693
  Perplexity: 5.434
  Learning Rate: 1.23e-04
  GPU Memory Used: 11921MB

[20:26:16] Step 53250/125000:
  Training Loss: 1.703
  Perplexity: 5.488
  Learning Rate: 1.23e-04
  GPU Memory Used: 12106MB

[20:27:27] Step 53300/125000:
  Training Loss: 1.692
  Perplexity: 5.428
  Learning Rate: 1.23e-04
  GPU Memory Used: 12099MB

[20:28:37] Step 53350/125000:
  Training Loss: 1.694
  Perplexity: 5.440
  Learning Rate: 1.23e-04
  GPU Memory Used: 11812MB

[20:29:46] Step 53400/125000:
  Training Loss: 1.629
  Perplexity: 5.097
  Learning Rate: 1.23e-04
  GPU Memory Used: 12160MB

[20:30:56] Step 53450/125000:
  Training Loss: 1.669
  Perplexity: 5.308
  Learning Rate: 1.23e-04
  GPU Memory Used: 11969MB

[20:32:08] Step 53500/125000:
  Training Loss: 1.708
  Perplexity: 5.520
  Learning Rate: 1.22e-04
  GPU Memory Used: 12073MB


Evaluation Results:
  Validation Loss: 1.788
  Validation Perplexity: 5.980

Saving model checkpoint: proposal_model/checkpoint-53500


Epoch 108/250
==================================================
[20:33:17] Step 53550/125000:
  Training Loss: 1.691
  Perplexity: 5.426
  Learning Rate: 1.22e-04
  GPU Memory Used: 11876MB

[20:34:27] Step 53600/125000:
  Training Loss: 1.650
  Perplexity: 5.209
  Learning Rate: 1.22e-04
  GPU Memory Used: 11904MB

[20:35:35] Step 53650/125000:
  Training Loss: 1.727
  Perplexity: 5.623
  Learning Rate: 1.22e-04
  GPU Memory Used: 11961MB

[20:36:42] Step 53700/125000:
  Training Loss: 1.634
  Perplexity: 5.124
  Learning Rate: 1.22e-04
  GPU Memory Used: 11845MB

[20:37:51] Step 53750/125000:
  Training Loss: 1.683
  Perplexity: 5.384
  Learning Rate: 1.22e-04
  GPU Memory Used: 12099MB

[20:39:02] Step 53800/125000:
  Training Loss: 1.664
  Perplexity: 5.282
  Learning Rate: 1.22e-04
  GPU Memory Used: 12030MB

[20:40:12] Step 53850/125000:
  Training Loss: 1.707
  Perplexity: 5.513
  Learning Rate: 1.22e-04
  GPU Memory Used: 12130MB

[20:41:22] Step 53900/125000:
  Training Loss: 1.697
  Perplexity: 5.458
  Learning Rate: 1.21e-04
  GPU Memory Used: 11850MB

[20:42:31] Step 53950/125000:
  Training Loss: 1.701
  Perplexity: 5.481
  Learning Rate: 1.21e-04
  GPU Memory Used: 12065MB

[20:43:40] Step 54000/125000:
  Training Loss: 1.822
  Perplexity: 6.186
  Learning Rate: 1.21e-04
  GPU Memory Used: 11996MB


Evaluation Results:
  Validation Loss: 1.849
  Validation Perplexity: 6.354

Saving model checkpoint: proposal_model/checkpoint-54000


Epoch 109/250
==================================================
[20:44:47] Step 54050/125000:
  Training Loss: 1.611
  Perplexity: 5.007
  Learning Rate: 1.21e-04
  GPU Memory Used: 11867MB

[20:45:57] Step 54100/125000:
  Training Loss: 1.716
  Perplexity: 5.561
  Learning Rate: 1.21e-04
  GPU Memory Used: 11810MB

[20:47:05] Step 54150/125000:
  Training Loss: 1.677
  Perplexity: 5.351
  Learning Rate: 1.21e-04
  GPU Memory Used: 12042MB

[20:48:13] Step 54200/125000:
  Training Loss: 1.711
  Perplexity: 5.532
  Learning Rate: 1.21e-04
  GPU Memory Used: 11836MB

[20:49:21] Step 54250/125000:
  Training Loss: 1.746
  Perplexity: 5.731
  Learning Rate: 1.21e-04
  GPU Memory Used: 11989MB

[20:50:30] Step 54300/125000:
  Training Loss: 1.668
  Perplexity: 5.303
  Learning Rate: 1.20e-04
  GPU Memory Used: 11979MB

[20:51:37] Step 54350/125000:
  Training Loss: 1.624
  Perplexity: 5.073
  Learning Rate: 1.20e-04
  GPU Memory Used: 12022MB

[20:52:45] Step 54400/125000:
  Training Loss: 1.661
  Perplexity: 5.264
  Learning Rate: 1.20e-04
  GPU Memory Used: 12199MB

[20:53:56] Step 54450/125000:
  Training Loss: 1.767
  Perplexity: 5.855
  Learning Rate: 1.20e-04
  GPU Memory Used: 12167MB

[20:55:05] Step 54500/125000:
  Training Loss: 1.712
  Perplexity: 5.543
  Learning Rate: 1.20e-04
  GPU Memory Used: 11974MB


Evaluation Results:
  Validation Loss: 1.821
  Validation Perplexity: 6.178

Saving model checkpoint: proposal_model/checkpoint-54500


Epoch 110/250
==================================================
[20:56:15] Step 54550/125000:
  Training Loss: 1.632
  Perplexity: 5.112
  Learning Rate: 1.20e-04
  GPU Memory Used: 12088MB

[20:57:24] Step 54600/125000:
  Training Loss: 1.666
  Perplexity: 5.292
  Learning Rate: 1.20e-04
  GPU Memory Used: 11997MB

[20:58:33] Step 54650/125000:
  Training Loss: 1.609
  Perplexity: 4.995
  Learning Rate: 1.20e-04
  GPU Memory Used: 12076MB

[20:59:42] Step 54700/125000:
  Training Loss: 1.691
  Perplexity: 5.423
  Learning Rate: 1.19e-04
  GPU Memory Used: 11921MB

[21:00:53] Step 54750/125000:
  Training Loss: 1.676
  Perplexity: 5.346
  Learning Rate: 1.19e-04
  GPU Memory Used: 12094MB

[21:02:02] Step 54800/125000:
  Training Loss: 1.744
  Perplexity: 5.718
  Learning Rate: 1.19e-04
  GPU Memory Used: 12007MB

[21:03:12] Step 54850/125000:
  Training Loss: 1.727
  Perplexity: 5.622
  Learning Rate: 1.19e-04
  GPU Memory Used: 11839MB

[21:04:19] Step 54900/125000:
  Training Loss: 1.494
  Perplexity: 4.457
  Learning Rate: 1.19e-04
  GPU Memory Used: 12159MB

[21:05:28] Step 54950/125000:
  Training Loss: 1.679
  Perplexity: 5.360
  Learning Rate: 1.19e-04
  GPU Memory Used: 12191MB

[21:06:37] Step 55000/125000:
  Training Loss: 1.622
  Perplexity: 5.066
  Learning Rate: 1.19e-04
  GPU Memory Used: 11828MB


Evaluation Results:
  Validation Loss: 1.756
  Validation Perplexity: 5.790

Saving model checkpoint: proposal_model/checkpoint-55000


Epoch 111/250
==================================================
[21:07:45] Step 55050/125000:
  Training Loss: 1.608
  Perplexity: 4.991
  Learning Rate: 1.19e-04
  GPU Memory Used: 11986MB

[21:08:53] Step 55100/125000:
  Training Loss: 1.683
  Perplexity: 5.382
  Learning Rate: 1.18e-04
  GPU Memory Used: 12036MB

[21:10:03] Step 55150/125000:
  Training Loss: 1.626
  Perplexity: 5.086
  Learning Rate: 1.18e-04
  GPU Memory Used: 11805MB

[21:11:12] Step 55200/125000:
  Training Loss: 1.720
  Perplexity: 5.583
  Learning Rate: 1.18e-04
  GPU Memory Used: 11873MB

[21:12:23] Step 55250/125000:
  Training Loss: 1.753
  Perplexity: 5.773
  Learning Rate: 1.18e-04
  GPU Memory Used: 11880MB

[21:13:34] Step 55300/125000:
  Training Loss: 1.683
  Perplexity: 5.380
  Learning Rate: 1.18e-04
  GPU Memory Used: 12165MB

[21:14:46] Step 55350/125000:
  Training Loss: 1.509
  Perplexity: 4.520
  Learning Rate: 1.18e-04
  GPU Memory Used: 11817MB

[21:15:56] Step 55400/125000:
  Training Loss: 1.693
  Perplexity: 5.436
  Learning Rate: 1.18e-04
  GPU Memory Used: 12033MB

[21:17:05] Step 55450/125000:
  Training Loss: 1.726
  Perplexity: 5.616
  Learning Rate: 1.18e-04
  GPU Memory Used: 11921MB

[21:18:15] Step 55500/125000:
  Training Loss: 1.675
  Perplexity: 5.341
  Learning Rate: 1.18e-04
  GPU Memory Used: 11810MB


Evaluation Results:
  Validation Loss: 1.699
  Validation Perplexity: 5.471

Saving model checkpoint: proposal_model/checkpoint-55500


Epoch 112/250
==================================================
[21:19:24] Step 55550/125000:
  Training Loss: 1.537
  Perplexity: 4.651
  Learning Rate: 1.17e-04
  GPU Memory Used: 12004MB

[21:20:34] Step 55600/125000:
  Training Loss: 1.666
  Perplexity: 5.291
  Learning Rate: 1.17e-04
  GPU Memory Used: 11902MB

[21:21:44] Step 55650/125000:
  Training Loss: 1.700
  Perplexity: 5.476
  Learning Rate: 1.17e-04
  GPU Memory Used: 11918MB

[21:22:55] Step 55700/125000:
  Training Loss: 1.565
  Perplexity: 4.782
  Learning Rate: 1.17e-04
  GPU Memory Used: 11873MB

[21:24:04] Step 55750/125000:
  Training Loss: 1.711
  Perplexity: 5.532
  Learning Rate: 1.17e-04
  GPU Memory Used: 12002MB

[21:25:13] Step 55800/125000:
  Training Loss: 1.678
  Perplexity: 5.353
  Learning Rate: 1.17e-04
  GPU Memory Used: 12078MB

[21:26:23] Step 55850/125000:
  Training Loss: 1.712
  Perplexity: 5.538
  Learning Rate: 1.17e-04
  GPU Memory Used: 11901MB

[21:27:33] Step 55900/125000:
  Training Loss: 1.664
  Perplexity: 5.279
  Learning Rate: 1.17e-04
  GPU Memory Used: 11967MB

[21:28:42] Step 55950/125000:
  Training Loss: 1.667
  Perplexity: 5.294
  Learning Rate: 1.16e-04
  GPU Memory Used: 12147MB

[21:29:52] Step 56000/125000:
  Training Loss: 1.752
  Perplexity: 5.764
  Learning Rate: 1.16e-04
  GPU Memory Used: 12174MB


Evaluation Results:
  Validation Loss: 1.753
  Validation Perplexity: 5.770

Saving model checkpoint: proposal_model/checkpoint-56000


Epoch 113/250
==================================================
[21:31:02] Step 56050/125000:
  Training Loss: 1.692
  Perplexity: 5.433
  Learning Rate: 1.16e-04
  GPU Memory Used: 12110MB

[21:32:11] Step 56100/125000:
  Training Loss: 1.774
  Perplexity: 5.894
  Learning Rate: 1.16e-04
  GPU Memory Used: 11886MB

[21:33:20] Step 56150/125000:
  Training Loss: 1.688
  Perplexity: 5.410
  Learning Rate: 1.16e-04
  GPU Memory Used: 12166MB

[21:34:29] Step 56200/125000:
  Training Loss: 1.627
  Perplexity: 5.091
  Learning Rate: 1.16e-04
  GPU Memory Used: 12120MB

[21:35:37] Step 56250/125000:
  Training Loss: 1.603
  Perplexity: 4.968
  Learning Rate: 1.16e-04
  GPU Memory Used: 11818MB

[21:36:44] Step 56300/125000:
  Training Loss: 1.749
  Perplexity: 5.747
  Learning Rate: 1.16e-04
  GPU Memory Used: 11937MB

[21:37:53] Step 56350/125000:
  Training Loss: 1.657
  Perplexity: 5.241
  Learning Rate: 1.15e-04
  GPU Memory Used: 12142MB

[21:39:02] Step 56400/125000:
  Training Loss: 1.673
  Perplexity: 5.326
  Learning Rate: 1.15e-04
  GPU Memory Used: 12086MB

[21:40:10] Step 56450/125000:
  Training Loss: 1.667
  Perplexity: 5.298
  Learning Rate: 1.15e-04
  GPU Memory Used: 12178MB

[21:41:19] Step 56500/125000:
  Training Loss: 1.611
  Perplexity: 5.010
  Learning Rate: 1.15e-04
  GPU Memory Used: 11824MB


Evaluation Results:
  Validation Loss: 1.717
  Validation Perplexity: 5.568

Saving model checkpoint: proposal_model/checkpoint-56500


Epoch 114/250
==================================================
[21:42:29] Step 56550/125000:
  Training Loss: 1.597
  Perplexity: 4.937
  Learning Rate: 1.15e-04
  GPU Memory Used: 12145MB

[21:43:37] Step 56600/125000:
  Training Loss: 1.775
  Perplexity: 5.901
  Learning Rate: 1.15e-04
  GPU Memory Used: 12006MB

[21:44:47] Step 56650/125000:
  Training Loss: 1.619
  Perplexity: 5.048
  Learning Rate: 1.15e-04
  GPU Memory Used: 11831MB

[21:45:56] Step 56700/125000:
  Training Loss: 1.641
  Perplexity: 5.159
  Learning Rate: 1.15e-04
  GPU Memory Used: 11877MB

[21:47:07] Step 56750/125000:
  Training Loss: 1.729
  Perplexity: 5.632
  Learning Rate: 1.14e-04
  GPU Memory Used: 12179MB

[21:48:16] Step 56800/125000:
  Training Loss: 1.700
  Perplexity: 5.473
  Learning Rate: 1.14e-04
  GPU Memory Used: 12191MB

[21:49:27] Step 56850/125000:
  Training Loss: 1.541
  Perplexity: 4.670
  Learning Rate: 1.14e-04
  GPU Memory Used: 11955MB

[21:50:34] Step 56900/125000:
  Training Loss: 1.680
  Perplexity: 5.367
  Learning Rate: 1.14e-04
  GPU Memory Used: 12065MB

[21:51:44] Step 56950/125000:
  Training Loss: 1.699
  Perplexity: 5.470
  Learning Rate: 1.14e-04
  GPU Memory Used: 12021MB

[21:52:51] Step 57000/125000:
  Training Loss: 1.654
  Perplexity: 5.227
  Learning Rate: 1.14e-04
  GPU Memory Used: 11886MB


Evaluation Results:
  Validation Loss: 1.751
  Validation Perplexity: 5.758

Saving model checkpoint: proposal_model/checkpoint-57000


Epoch 115/250
==================================================
[21:54:00] Step 57050/125000:
  Training Loss: 1.575
  Perplexity: 4.832
  Learning Rate: 1.14e-04
  GPU Memory Used: 11876MB

[21:55:09] Step 57100/125000:
  Training Loss: 1.759
  Perplexity: 5.807
  Learning Rate: 1.14e-04
  GPU Memory Used: 11856MB

[21:56:18] Step 57150/125000:
  Training Loss: 1.697
  Perplexity: 5.457
  Learning Rate: 1.13e-04
  GPU Memory Used: 12169MB

[21:57:26] Step 57200/125000:
  Training Loss: 1.606
  Perplexity: 4.983
  Learning Rate: 1.13e-04
  GPU Memory Used: 12166MB

[21:58:36] Step 57250/125000:
  Training Loss: 1.638
  Perplexity: 5.145
  Learning Rate: 1.13e-04
  GPU Memory Used: 12110MB

[21:59:45] Step 57300/125000:
  Training Loss: 1.655
  Perplexity: 5.235
  Learning Rate: 1.13e-04
  GPU Memory Used: 12086MB

[22:00:53] Step 57350/125000:
  Training Loss: 1.639
  Perplexity: 5.149
  Learning Rate: 1.13e-04
  GPU Memory Used: 11972MB

[22:02:02] Step 57400/125000:
  Training Loss: 1.780
  Perplexity: 5.929
  Learning Rate: 1.13e-04
  GPU Memory Used: 12097MB

[22:03:11] Step 57450/125000:
  Training Loss: 1.738
  Perplexity: 5.684
  Learning Rate: 1.13e-04
  GPU Memory Used: 11832MB

[22:04:19] Step 57500/125000:
  Training Loss: 1.650
  Perplexity: 5.206
  Learning Rate: 1.13e-04
  GPU Memory Used: 11827MB


Evaluation Results:
  Validation Loss: 1.804
  Validation Perplexity: 6.073

Saving model checkpoint: proposal_model/checkpoint-57500


Epoch 116/250
==================================================
[22:05:28] Step 57550/125000:
  Training Loss: 1.612
  Perplexity: 5.015
  Learning Rate: 1.12e-04
  GPU Memory Used: 11898MB

[22:06:38] Step 57600/125000:
  Training Loss: 1.648
  Perplexity: 5.197
  Learning Rate: 1.12e-04
  GPU Memory Used: 11948MB

[22:07:47] Step 57650/125000:
  Training Loss: 1.529
  Perplexity: 4.614
  Learning Rate: 1.12e-04
  GPU Memory Used: 11981MB

[22:08:57] Step 57700/125000:
  Training Loss: 1.703
  Perplexity: 5.490
  Learning Rate: 1.12e-04
  GPU Memory Used: 12175MB

[22:10:05] Step 57750/125000:
  Training Loss: 1.616
  Perplexity: 5.032
  Learning Rate: 1.12e-04
  GPU Memory Used: 11952MB

[22:11:15] Step 57800/125000:
  Training Loss: 1.685
  Perplexity: 5.390
  Learning Rate: 1.12e-04
  GPU Memory Used: 11827MB

[22:12:27] Step 57850/125000:
  Training Loss: 1.638
  Perplexity: 5.146
  Learning Rate: 1.12e-04
  GPU Memory Used: 12020MB

[22:13:35] Step 57900/125000:
  Training Loss: 1.746
  Perplexity: 5.733
  Learning Rate: 1.12e-04
  GPU Memory Used: 12191MB

[22:14:46] Step 57950/125000:
  Training Loss: 1.644
  Perplexity: 5.174
  Learning Rate: 1.11e-04
  GPU Memory Used: 11973MB

[22:15:54] Step 58000/125000:
  Training Loss: 1.590
  Perplexity: 4.906
  Learning Rate: 1.11e-04
  GPU Memory Used: 12036MB


Evaluation Results:
  Validation Loss: 1.678
  Validation Perplexity: 5.353

Saving model checkpoint: proposal_model/checkpoint-58000


Epoch 117/250
==================================================
[22:17:04] Step 58050/125000:
  Training Loss: 1.581
  Perplexity: 4.861
  Learning Rate: 1.11e-04
  GPU Memory Used: 11863MB

[22:18:14] Step 58100/125000:
  Training Loss: 1.649
  Perplexity: 5.201
  Learning Rate: 1.11e-04
  GPU Memory Used: 12152MB

[22:19:26] Step 58150/125000:
  Training Loss: 1.621
  Perplexity: 5.056
  Learning Rate: 1.11e-04
  GPU Memory Used: 11954MB

[22:20:36] Step 58200/125000:
  Training Loss: 1.665
  Perplexity: 5.288
  Learning Rate: 1.11e-04
  GPU Memory Used: 11906MB

[22:21:46] Step 58250/125000:
  Training Loss: 1.515
  Perplexity: 4.551
  Learning Rate: 1.11e-04
  GPU Memory Used: 12061MB

[22:22:57] Step 58300/125000:
  Training Loss: 1.702
  Perplexity: 5.487
  Learning Rate: 1.11e-04
  GPU Memory Used: 11813MB

[22:24:07] Step 58350/125000:
  Training Loss: 1.602
  Perplexity: 4.965
  Learning Rate: 1.10e-04
  GPU Memory Used: 11947MB

[22:25:16] Step 58400/125000:
  Training Loss: 1.627
  Perplexity: 5.089
  Learning Rate: 1.10e-04
  GPU Memory Used: 11838MB

[22:26:26] Step 58450/125000:
  Training Loss: 1.626
  Perplexity: 5.081
  Learning Rate: 1.10e-04
  GPU Memory Used: 11985MB

[22:27:36] Step 58500/125000:
  Training Loss: 1.711
  Perplexity: 5.536
  Learning Rate: 1.10e-04
  GPU Memory Used: 11888MB


Evaluation Results:
  Validation Loss: 1.892
  Validation Perplexity: 6.636

Saving model checkpoint: proposal_model/checkpoint-58500


Epoch 118/250
==================================================
[22:28:44] Step 58550/125000:
  Training Loss: 1.517
  Perplexity: 4.560
  Learning Rate: 1.10e-04
  GPU Memory Used: 12174MB

[22:29:53] Step 58600/125000:
  Training Loss: 1.628
  Perplexity: 5.096
  Learning Rate: 1.10e-04
  GPU Memory Used: 11892MB

[22:31:02] Step 58650/125000:
  Training Loss: 1.616
  Perplexity: 5.034
  Learning Rate: 1.10e-04
  GPU Memory Used: 11854MB

[22:32:12] Step 58700/125000:
  Training Loss: 1.587
  Perplexity: 4.891
  Learning Rate: 1.10e-04
  GPU Memory Used: 12192MB

[22:33:20] Step 58750/125000:
  Training Loss: 1.691
  Perplexity: 5.426
  Learning Rate: 1.09e-04
  GPU Memory Used: 12136MB

[22:34:29] Step 58800/125000:
  Training Loss: 1.716
  Perplexity: 5.561
  Learning Rate: 1.09e-04
  GPU Memory Used: 12122MB

[22:35:38] Step 58850/125000:
  Training Loss: 1.650
  Perplexity: 5.205
  Learning Rate: 1.09e-04
  GPU Memory Used: 11868MB

[22:36:48] Step 58900/125000:
  Training Loss: 1.659
  Perplexity: 5.252
  Learning Rate: 1.09e-04
  GPU Memory Used: 12047MB

[22:37:58] Step 58950/125000:
  Training Loss: 1.670
  Perplexity: 5.310
  Learning Rate: 1.09e-04
  GPU Memory Used: 11974MB

[22:39:08] Step 59000/125000:
  Training Loss: 1.621
  Perplexity: 5.060
  Learning Rate: 1.09e-04
  GPU Memory Used: 12197MB


Evaluation Results:
  Validation Loss: 1.785
  Validation Perplexity: 5.959

Saving model checkpoint: proposal_model/checkpoint-59000


Epoch 119/250
==================================================
[22:40:17] Step 59050/125000:
  Training Loss: 1.741
  Perplexity: 5.704
  Learning Rate: 1.09e-04
  GPU Memory Used: 12042MB

[22:41:26] Step 59100/125000:
  Training Loss: 1.609
  Perplexity: 4.999
  Learning Rate: 1.09e-04
  GPU Memory Used: 11865MB

[22:42:35] Step 59150/125000:
  Training Loss: 1.555
  Perplexity: 4.736
  Learning Rate: 1.08e-04
  GPU Memory Used: 11963MB

[22:43:44] Step 59200/125000:
  Training Loss: 1.544
  Perplexity: 4.686
  Learning Rate: 1.08e-04
  GPU Memory Used: 12065MB

[22:44:55] Step 59250/125000:
  Training Loss: 1.638
  Perplexity: 5.147
  Learning Rate: 1.08e-04
  GPU Memory Used: 11855MB

[22:46:04] Step 59300/125000:
  Training Loss: 1.605
  Perplexity: 4.976
  Learning Rate: 1.08e-04
  GPU Memory Used: 11854MB

[22:47:12] Step 59350/125000:
  Training Loss: 1.577
  Perplexity: 4.839
  Learning Rate: 1.08e-04
  GPU Memory Used: 11962MB

[22:48:22] Step 59400/125000:
  Training Loss: 1.657
  Perplexity: 5.246
  Learning Rate: 1.08e-04
  GPU Memory Used: 12083MB

[22:49:32] Step 59450/125000:
  Training Loss: 1.704
  Perplexity: 5.496
  Learning Rate: 1.08e-04
  GPU Memory Used: 12130MB

[22:50:40] Step 59500/125000:
  Training Loss: 1.510
  Perplexity: 4.528
  Learning Rate: 1.08e-04
  GPU Memory Used: 12055MB


Evaluation Results:
  Validation Loss: 1.654
  Validation Perplexity: 5.229

Saving model checkpoint: proposal_model/checkpoint-59500


Epoch 120/250
==================================================
[22:51:47] Step 59550/125000:
  Training Loss: 1.500
  Perplexity: 4.483
  Learning Rate: 1.07e-04
  GPU Memory Used: 12038MB

[22:52:57] Step 59600/125000:
  Training Loss: 1.671
  Perplexity: 5.319
  Learning Rate: 1.07e-04
  GPU Memory Used: 12108MB

[22:54:07] Step 59650/125000:
  Training Loss: 1.700
  Perplexity: 5.476
  Learning Rate: 1.07e-04
  GPU Memory Used: 11922MB

[22:55:16] Step 59700/125000:
  Training Loss: 1.719
  Perplexity: 5.580
  Learning Rate: 1.07e-04
  GPU Memory Used: 12192MB

[22:56:22] Step 59750/125000:
  Training Loss: 1.602
  Perplexity: 4.961
  Learning Rate: 1.07e-04
  GPU Memory Used: 11812MB

[22:57:31] Step 59800/125000:
  Training Loss: 1.661
  Perplexity: 5.266
  Learning Rate: 1.07e-04
  GPU Memory Used: 12089MB

[22:58:40] Step 59850/125000:
  Training Loss: 1.509
  Perplexity: 4.521
  Learning Rate: 1.07e-04
  GPU Memory Used: 11853MB

[22:59:51] Step 59900/125000:
  Training Loss: 1.622
  Perplexity: 5.063
  Learning Rate: 1.07e-04
  GPU Memory Used: 12060MB

[23:00:59] Step 59950/125000:
  Training Loss: 1.499
  Perplexity: 4.475
  Learning Rate: 1.06e-04
  GPU Memory Used: 12200MB

[23:02:07] Step 60000/125000:
  Training Loss: 1.620
  Perplexity: 5.054
  Learning Rate: 1.06e-04
  GPU Memory Used: 12075MB


Evaluation Results:
  Validation Loss: 1.715
  Validation Perplexity: 5.559

Saving model checkpoint: proposal_model/checkpoint-60000


Epoch 121/250
==================================================
[23:03:17] Step 60050/125000:
  Training Loss: 1.775
  Perplexity: 5.900
  Learning Rate: 1.06e-04
  GPU Memory Used: 12105MB

[23:04:25] Step 60100/125000:
  Training Loss: 1.655
  Perplexity: 5.235
  Learning Rate: 1.06e-04
  GPU Memory Used: 12091MB

[23:05:35] Step 60150/125000:
  Training Loss: 1.611
  Perplexity: 5.009
  Learning Rate: 1.06e-04
  GPU Memory Used: 12124MB

[23:06:45] Step 60200/125000:
  Training Loss: 1.642
  Perplexity: 5.164
  Learning Rate: 1.06e-04
  GPU Memory Used: 11845MB

[23:07:53] Step 60250/125000:
  Training Loss: 1.667
  Perplexity: 5.295
  Learning Rate: 1.06e-04
  GPU Memory Used: 11994MB

[23:09:03] Step 60300/125000:
  Training Loss: 1.627
  Perplexity: 5.088
  Learning Rate: 1.06e-04
  GPU Memory Used: 11950MB

[23:10:12] Step 60350/125000:
  Training Loss: 1.617
  Perplexity: 5.036
  Learning Rate: 1.05e-04
  GPU Memory Used: 12003MB

[23:11:20] Step 60400/125000:
  Training Loss: 1.611
  Perplexity: 5.005
  Learning Rate: 1.05e-04
  GPU Memory Used: 12079MB

[23:12:30] Step 60450/125000:
  Training Loss: 1.652
  Perplexity: 5.219
  Learning Rate: 1.05e-04
  GPU Memory Used: 12118MB

[23:13:39] Step 60500/125000:
  Training Loss: 1.643
  Perplexity: 5.169
  Learning Rate: 1.05e-04
  GPU Memory Used: 11997MB


Evaluation Results:
  Validation Loss: 1.743
  Validation Perplexity: 5.714

Saving model checkpoint: proposal_model/checkpoint-60500


Epoch 122/250
==================================================
[23:14:47] Step 60550/125000:
  Training Loss: 1.682
  Perplexity: 5.376
  Learning Rate: 1.05e-04
  GPU Memory Used: 12048MB

[23:15:58] Step 60600/125000:
  Training Loss: 1.608
  Perplexity: 4.994
  Learning Rate: 1.05e-04
  GPU Memory Used: 11942MB

[23:17:07] Step 60650/125000:
  Training Loss: 1.634
  Perplexity: 5.127
  Learning Rate: 1.05e-04
  GPU Memory Used: 12112MB

[23:18:17] Step 60700/125000:
  Training Loss: 1.674
  Perplexity: 5.334
  Learning Rate: 1.05e-04
  GPU Memory Used: 12019MB

[23:19:26] Step 60750/125000:
  Training Loss: 1.588
  Perplexity: 4.896
  Learning Rate: 1.04e-04
  GPU Memory Used: 12053MB

[23:20:35] Step 60800/125000:
  Training Loss: 1.575
  Perplexity: 4.832
  Learning Rate: 1.04e-04
  GPU Memory Used: 11809MB

[23:21:44] Step 60850/125000:
  Training Loss: 1.661
  Perplexity: 5.266
  Learning Rate: 1.04e-04
  GPU Memory Used: 12016MB

[23:22:52] Step 60900/125000:
  Training Loss: 1.647
  Perplexity: 5.190
  Learning Rate: 1.04e-04
  GPU Memory Used: 12026MB

[23:24:01] Step 60950/125000:
  Training Loss: 1.622
  Perplexity: 5.062
  Learning Rate: 1.04e-04
  GPU Memory Used: 12184MB

[23:25:09] Step 61000/125000:
  Training Loss: 1.612
  Perplexity: 5.014
  Learning Rate: 1.04e-04
  GPU Memory Used: 11997MB


Evaluation Results:
  Validation Loss: 1.780
  Validation Perplexity: 5.929

Saving model checkpoint: proposal_model/checkpoint-61000


Epoch 123/250
==================================================
[23:26:18] Step 61050/125000:
  Training Loss: 1.685
  Perplexity: 5.392
  Learning Rate: 1.04e-04
  GPU Memory Used: 11806MB

[23:27:26] Step 61100/125000:
  Training Loss: 1.730
  Perplexity: 5.642
  Learning Rate: 1.04e-04
  GPU Memory Used: 11900MB

[23:28:36] Step 61150/125000:
  Training Loss: 1.764
  Perplexity: 5.838
  Learning Rate: 1.03e-04
  GPU Memory Used: 12193MB

[23:29:47] Step 61200/125000:
  Training Loss: 1.627
  Perplexity: 5.087
  Learning Rate: 1.03e-04
  GPU Memory Used: 11808MB

[23:30:56] Step 61250/125000:
  Training Loss: 1.489
  Perplexity: 4.434
  Learning Rate: 1.03e-04
  GPU Memory Used: 11913MB

[23:32:06] Step 61300/125000:
  Training Loss: 1.704
  Perplexity: 5.494
  Learning Rate: 1.03e-04
  GPU Memory Used: 11927MB

[23:33:15] Step 61350/125000:
  Training Loss: 1.661
  Perplexity: 5.263
  Learning Rate: 1.03e-04
  GPU Memory Used: 12129MB

[23:34:24] Step 61400/125000:
  Training Loss: 1.583
  Perplexity: 4.868
  Learning Rate: 1.03e-04
  GPU Memory Used: 11963MB

[23:35:32] Step 61450/125000:
  Training Loss: 1.551
  Perplexity: 4.718
  Learning Rate: 1.03e-04
  GPU Memory Used: 11847MB

[23:36:40] Step 61500/125000:
  Training Loss: 1.538
  Perplexity: 4.653
  Learning Rate: 1.03e-04
  GPU Memory Used: 11979MB


Evaluation Results:
  Validation Loss: 1.800
  Validation Perplexity: 6.048

Saving model checkpoint: proposal_model/checkpoint-61500


Epoch 124/250
==================================================
[23:37:47] Step 61550/125000:
  Training Loss: 1.614
  Perplexity: 5.022
  Learning Rate: 1.02e-04
  GPU Memory Used: 12100MB

[23:38:55] Step 61600/125000:
  Training Loss: 1.659
  Perplexity: 5.252
  Learning Rate: 1.02e-04
  GPU Memory Used: 11885MB

[23:40:05] Step 61650/125000:
  Training Loss: 1.682
  Perplexity: 5.374
  Learning Rate: 1.02e-04
  GPU Memory Used: 11871MB

[23:41:15] Step 61700/125000:
  Training Loss: 1.644
  Perplexity: 5.174
  Learning Rate: 1.02e-04
  GPU Memory Used: 12003MB

[23:42:22] Step 61750/125000:
  Training Loss: 1.664
  Perplexity: 5.280
  Learning Rate: 1.02e-04
  GPU Memory Used: 11852MB

[23:43:31] Step 61800/125000:
  Training Loss: 1.606
  Perplexity: 4.982
  Learning Rate: 1.02e-04
  GPU Memory Used: 11995MB

[23:44:42] Step 61850/125000:
  Training Loss: 1.627
  Perplexity: 5.089
  Learning Rate: 1.02e-04
  GPU Memory Used: 11973MB

[23:45:53] Step 61900/125000:
  Training Loss: 1.523
  Perplexity: 4.586
  Learning Rate: 1.02e-04
  GPU Memory Used: 12030MB

[23:47:04] Step 61950/125000:
  Training Loss: 1.675
  Perplexity: 5.338
  Learning Rate: 1.01e-04
  GPU Memory Used: 12086MB

[23:48:15] Step 62000/125000:
  Training Loss: 1.589
  Perplexity: 4.897
  Learning Rate: 1.01e-04
  GPU Memory Used: 11874MB


Evaluation Results:
  Validation Loss: 1.789
  Validation Perplexity: 5.982

Saving model checkpoint: proposal_model/checkpoint-62000


Epoch 125/250
==================================================
[23:49:24] Step 62050/125000:
  Training Loss: 1.680
  Perplexity: 5.366
  Learning Rate: 1.01e-04
  GPU Memory Used: 11935MB

[23:50:31] Step 62100/125000:
  Training Loss: 1.571
  Perplexity: 4.812
  Learning Rate: 1.01e-04
  GPU Memory Used: 12150MB

[23:51:41] Step 62150/125000:
  Training Loss: 1.539
  Perplexity: 4.660
  Learning Rate: 1.01e-04
  GPU Memory Used: 11849MB

[23:52:51] Step 62200/125000:
  Training Loss: 1.549
  Perplexity: 4.709
  Learning Rate: 1.01e-04
  GPU Memory Used: 11828MB

[23:53:59] Step 62250/125000:
  Training Loss: 1.651
  Perplexity: 5.213
  Learning Rate: 1.01e-04
  GPU Memory Used: 12009MB

[23:55:09] Step 62300/125000:
  Training Loss: 1.673
  Perplexity: 5.326
  Learning Rate: 1.01e-04
  GPU Memory Used: 12119MB

[23:56:18] Step 62350/125000:
  Training Loss: 1.614
  Perplexity: 5.022
  Learning Rate: 1.00e-04
  GPU Memory Used: 12112MB

[23:57:27] Step 62400/125000:
  Training Loss: 1.559
  Perplexity: 4.754
  Learning Rate: 1.00e-04
  GPU Memory Used: 11960MB

[23:58:39] Step 62450/125000:
  Training Loss: 1.596
  Perplexity: 4.931
  Learning Rate: 1.00e-04
  GPU Memory Used: 12089MB

[23:59:48] Step 62500/125000:
  Training Loss: 1.641
  Perplexity: 5.161
  Learning Rate: 1.00e-04
  GPU Memory Used: 12016MB


Evaluation Results:
  Validation Loss: 1.763
  Validation Perplexity: 5.828

Saving model checkpoint: proposal_model/checkpoint-62500


Epoch 126/250
==================================================
[24:00:56] Step 62550/125000:
  Training Loss: 1.603
  Perplexity: 4.966
  Learning Rate: 9.99e-05
  GPU Memory Used: 12155MB

[24:02:06] Step 62600/125000:
  Training Loss: 1.624
  Perplexity: 5.076
  Learning Rate: 9.97e-05
  GPU Memory Used: 11994MB

[24:03:17] Step 62650/125000:
  Training Loss: 1.578
  Perplexity: 4.847
  Learning Rate: 9.96e-05
  GPU Memory Used: 12043MB

[24:04:25] Step 62700/125000:
  Training Loss: 1.558
  Perplexity: 4.752
  Learning Rate: 9.95e-05
  GPU Memory Used: 12176MB

[24:05:36] Step 62750/125000:
  Training Loss: 1.710
  Perplexity: 5.529
  Learning Rate: 9.94e-05
  GPU Memory Used: 11940MB

[24:06:47] Step 62800/125000:
  Training Loss: 1.583
  Perplexity: 4.868
  Learning Rate: 9.92e-05
  GPU Memory Used: 11831MB

[24:07:56] Step 62850/125000:
  Training Loss: 1.573
  Perplexity: 4.819
  Learning Rate: 9.91e-05
  GPU Memory Used: 12145MB

[24:09:03] Step 62900/125000:
  Training Loss: 1.536
  Perplexity: 4.645
  Learning Rate: 9.90e-05
  GPU Memory Used: 11809MB

[24:10:12] Step 62950/125000:
  Training Loss: 1.541
  Perplexity: 4.669
  Learning Rate: 9.89e-05
  GPU Memory Used: 11839MB

[24:11:21] Step 63000/125000:
  Training Loss: 1.541
  Perplexity: 4.668
  Learning Rate: 9.87e-05
  GPU Memory Used: 12095MB


Evaluation Results:
  Validation Loss: 1.663
  Validation Perplexity: 5.273

Saving model checkpoint: proposal_model/checkpoint-63000


Epoch 127/250
==================================================
[24:12:31] Step 63050/125000:
  Training Loss: 1.591
  Perplexity: 4.910
  Learning Rate: 9.86e-05
  GPU Memory Used: 12040MB

[24:13:41] Step 63100/125000:
  Training Loss: 1.552
  Perplexity: 4.722
  Learning Rate: 9.85e-05
  GPU Memory Used: 12085MB

[24:14:49] Step 63150/125000:
  Training Loss: 1.546
  Perplexity: 4.694
  Learning Rate: 9.84e-05
  GPU Memory Used: 12007MB

[24:15:57] Step 63200/125000:
  Training Loss: 1.612
  Perplexity: 5.011
  Learning Rate: 9.82e-05
  GPU Memory Used: 12186MB

[24:17:05] Step 63250/125000:
  Training Loss: 1.588
  Perplexity: 4.892
  Learning Rate: 9.81e-05
  GPU Memory Used: 12063MB

[24:18:13] Step 63300/125000:
  Training Loss: 1.639
  Perplexity: 5.152
  Learning Rate: 9.80e-05
  GPU Memory Used: 11841MB

[24:19:22] Step 63350/125000:
  Training Loss: 1.602
  Perplexity: 4.961
  Learning Rate: 9.79e-05
  GPU Memory Used: 11958MB

[24:20:32] Step 63400/125000:
  Training Loss: 1.581
  Perplexity: 4.861
  Learning Rate: 9.77e-05
  GPU Memory Used: 11856MB

[24:21:41] Step 63450/125000:
  Training Loss: 1.635
  Perplexity: 5.130
  Learning Rate: 9.76e-05
  GPU Memory Used: 11955MB

[24:22:50] Step 63500/125000:
  Training Loss: 1.595
  Perplexity: 4.930
  Learning Rate: 9.75e-05
  GPU Memory Used: 11835MB


Evaluation Results:
  Validation Loss: 1.641
  Validation Perplexity: 5.158

Saving model checkpoint: proposal_model/checkpoint-63500


Epoch 128/250
==================================================
[24:23:59] Step 63550/125000:
  Training Loss: 1.632
  Perplexity: 5.114
  Learning Rate: 9.74e-05
  GPU Memory Used: 11815MB

[24:25:09] Step 63600/125000:
  Training Loss: 1.604
  Perplexity: 4.974
  Learning Rate: 9.72e-05
  GPU Memory Used: 11859MB

[24:26:17] Step 63650/125000:
  Training Loss: 1.624
  Perplexity: 5.075
  Learning Rate: 9.71e-05
  GPU Memory Used: 12081MB

[24:27:25] Step 63700/125000:
  Training Loss: 1.565
  Perplexity: 4.782
  Learning Rate: 9.70e-05
  GPU Memory Used: 12172MB

[24:28:34] Step 63750/125000:
  Training Loss: 1.629
  Perplexity: 5.099
  Learning Rate: 9.69e-05
  GPU Memory Used: 11816MB

[24:29:44] Step 63800/125000:
  Training Loss: 1.636
  Perplexity: 5.136
  Learning Rate: 9.67e-05
  GPU Memory Used: 11894MB

[24:30:53] Step 63850/125000:
  Training Loss: 1.664
  Perplexity: 5.281
  Learning Rate: 9.66e-05
  GPU Memory Used: 12037MB

[24:32:01] Step 63900/125000:
  Training Loss: 1.528
  Perplexity: 4.611
  Learning Rate: 9.65e-05
  GPU Memory Used: 12006MB

[24:33:11] Step 63950/125000:
  Training Loss: 1.611
  Perplexity: 5.005
  Learning Rate: 9.64e-05
  GPU Memory Used: 12038MB

[24:34:19] Step 64000/125000:
  Training Loss: 1.637
  Perplexity: 5.139
  Learning Rate: 9.62e-05
  GPU Memory Used: 11908MB


Evaluation Results:
  Validation Loss: 1.715
  Validation Perplexity: 5.555

Saving model checkpoint: proposal_model/checkpoint-64000


Epoch 129/250
==================================================
[24:35:25] Step 64050/125000:
  Training Loss: 1.646
  Perplexity: 5.187
  Learning Rate: 9.61e-05
  GPU Memory Used: 11832MB

[24:36:36] Step 64100/125000:
  Training Loss: 1.576
  Perplexity: 4.836
  Learning Rate: 9.60e-05
  GPU Memory Used: 12137MB

[24:37:44] Step 64150/125000:
  Training Loss: 1.561
  Perplexity: 4.764
  Learning Rate: 9.59e-05
  GPU Memory Used: 11897MB

[24:38:54] Step 64200/125000:
  Training Loss: 1.665
  Perplexity: 5.284
  Learning Rate: 9.57e-05
  GPU Memory Used: 12102MB

[24:40:03] Step 64250/125000:
  Training Loss: 1.635
  Perplexity: 5.131
  Learning Rate: 9.56e-05
  GPU Memory Used: 12072MB

[24:41:13] Step 64300/125000:
  Training Loss: 1.623
  Perplexity: 5.071
  Learning Rate: 9.55e-05
  GPU Memory Used: 11977MB

[24:42:23] Step 64350/125000:
  Training Loss: 1.485
  Perplexity: 4.413
  Learning Rate: 9.54e-05
  GPU Memory Used: 11897MB

[24:43:30] Step 64400/125000:
  Training Loss: 1.543
  Perplexity: 4.678
  Learning Rate: 9.52e-05
  GPU Memory Used: 11960MB

[24:44:39] Step 64450/125000:
  Training Loss: 1.520
  Perplexity: 4.570
  Learning Rate: 9.51e-05
  GPU Memory Used: 11884MB

[24:45:48] Step 64500/125000:
  Training Loss: 1.551
  Perplexity: 4.717
  Learning Rate: 9.50e-05
  GPU Memory Used: 12155MB


Evaluation Results:
  Validation Loss: 1.745
  Validation Perplexity: 5.728

Saving model checkpoint: proposal_model/checkpoint-64500


Epoch 130/250
==================================================
[24:46:58] Step 64550/125000:
  Training Loss: 1.506
  Perplexity: 4.510
  Learning Rate: 9.49e-05
  GPU Memory Used: 12190MB

[24:48:06] Step 64600/125000:
  Training Loss: 1.481
  Perplexity: 4.395
  Learning Rate: 9.47e-05
  GPU Memory Used: 12186MB

[24:49:18] Step 64650/125000:
  Training Loss: 1.746
  Perplexity: 5.729
  Learning Rate: 9.46e-05
  GPU Memory Used: 12074MB

[24:50:28] Step 64700/125000:
  Training Loss: 1.610
  Perplexity: 5.001
  Learning Rate: 9.45e-05
  GPU Memory Used: 11998MB

[24:51:37] Step 64750/125000:
  Training Loss: 1.598
  Perplexity: 4.943
  Learning Rate: 9.43e-05
  GPU Memory Used: 11890MB

[24:52:45] Step 64800/125000:
  Training Loss: 1.523
  Perplexity: 4.587
  Learning Rate: 9.42e-05
  GPU Memory Used: 11813MB

[24:53:52] Step 64850/125000:
  Training Loss: 1.546
  Perplexity: 4.694
  Learning Rate: 9.41e-05
  GPU Memory Used: 11872MB

[24:55:00] Step 64900/125000:
  Training Loss: 1.587
  Perplexity: 4.891
  Learning Rate: 9.40e-05
  GPU Memory Used: 11905MB

[24:56:08] Step 64950/125000:
  Training Loss: 1.628
  Perplexity: 5.096
  Learning Rate: 9.38e-05
  GPU Memory Used: 12016MB

[24:57:17] Step 65000/125000:
  Training Loss: 1.589
  Perplexity: 4.896
  Learning Rate: 9.37e-05
  GPU Memory Used: 12148MB


Evaluation Results:
  Validation Loss: 1.704
  Validation Perplexity: 5.497

Saving model checkpoint: proposal_model/checkpoint-65000


Epoch 131/250
==================================================
[24:58:26] Step 65050/125000:
  Training Loss: 1.660
  Perplexity: 5.259
  Learning Rate: 9.36e-05
  GPU Memory Used: 12066MB

[24:59:35] Step 65100/125000:
  Training Loss: 1.601
  Perplexity: 4.956
  Learning Rate: 9.35e-05
  GPU Memory Used: 11922MB

[25:00:44] Step 65150/125000:
  Training Loss: 1.712
  Perplexity: 5.542
  Learning Rate: 9.33e-05
  GPU Memory Used: 12135MB

[25:01:52] Step 65200/125000:
  Training Loss: 1.662
  Perplexity: 5.271
  Learning Rate: 9.32e-05
  GPU Memory Used: 12077MB

[25:03:02] Step 65250/125000:
  Training Loss: 1.555
  Perplexity: 4.733
  Learning Rate: 9.31e-05
  GPU Memory Used: 12127MB

[25:04:11] Step 65300/125000:
  Training Loss: 1.598
  Perplexity: 4.941
  Learning Rate: 9.30e-05
  GPU Memory Used: 11868MB

[25:05:19] Step 65350/125000:
  Training Loss: 1.578
  Perplexity: 4.844
  Learning Rate: 9.28e-05
  GPU Memory Used: 12026MB

[25:06:27] Step 65400/125000:
  Training Loss: 1.484
  Perplexity: 4.409
  Learning Rate: 9.27e-05
  GPU Memory Used: 12139MB

[25:07:36] Step 65450/125000:
  Training Loss: 1.579
  Perplexity: 4.852
  Learning Rate: 9.26e-05
  GPU Memory Used: 12050MB

[25:08:44] Step 65500/125000:
  Training Loss: 1.544
  Perplexity: 4.682
  Learning Rate: 9.25e-05
  GPU Memory Used: 11956MB


Evaluation Results:
  Validation Loss: 1.804
  Validation Perplexity: 6.073

Saving model checkpoint: proposal_model/checkpoint-65500


Epoch 132/250
==================================================
[25:09:52] Step 65550/125000:
  Training Loss: 1.627
  Perplexity: 5.087
  Learning Rate: 9.23e-05
  GPU Memory Used: 11960MB

[25:11:00] Step 65600/125000:
  Training Loss: 1.666
  Perplexity: 5.292
  Learning Rate: 9.22e-05
  GPU Memory Used: 11910MB

[25:12:09] Step 65650/125000:
  Training Loss: 1.600
  Perplexity: 4.953
  Learning Rate: 9.21e-05
  GPU Memory Used: 12142MB

[25:13:18] Step 65700/125000:
  Training Loss: 1.563
  Perplexity: 4.772
  Learning Rate: 9.20e-05
  GPU Memory Used: 12007MB

[25:14:27] Step 65750/125000:
  Training Loss: 1.564
  Perplexity: 4.778
  Learning Rate: 9.18e-05
  GPU Memory Used: 11804MB

[25:15:36] Step 65800/125000:
  Training Loss: 1.527
  Perplexity: 4.607
  Learning Rate: 9.17e-05
  GPU Memory Used: 11878MB

[25:16:46] Step 65850/125000:
  Training Loss: 1.519
  Perplexity: 4.567
  Learning Rate: 9.16e-05
  GPU Memory Used: 12091MB

[25:17:54] Step 65900/125000:
  Training Loss: 1.603
  Perplexity: 4.970
  Learning Rate: 9.15e-05
  GPU Memory Used: 11902MB

[25:19:04] Step 65950/125000:
  Training Loss: 1.652
  Perplexity: 5.218
  Learning Rate: 9.13e-05
  GPU Memory Used: 12016MB

[25:20:13] Step 66000/125000:
  Training Loss: 1.610
  Perplexity: 5.005
  Learning Rate: 9.12e-05
  GPU Memory Used: 12038MB


Evaluation Results:
  Validation Loss: 1.664
  Validation Perplexity: 5.283

Saving model checkpoint: proposal_model/checkpoint-66000


Epoch 133/250
==================================================
[25:21:21] Step 66050/125000:
  Training Loss: 1.471
  Perplexity: 4.355
  Learning Rate: 9.11e-05
  GPU Memory Used: 12002MB

[25:22:29] Step 66100/125000:
  Training Loss: 1.551
  Perplexity: 4.715
  Learning Rate: 9.10e-05
  GPU Memory Used: 12190MB

[25:23:39] Step 66150/125000:
  Training Loss: 1.719
  Perplexity: 5.578
  Learning Rate: 9.08e-05
  GPU Memory Used: 12004MB

[25:24:48] Step 66200/125000:
  Training Loss: 1.534
  Perplexity: 4.636
  Learning Rate: 9.07e-05
  GPU Memory Used: 12160MB

[25:25:58] Step 66250/125000:
  Training Loss: 1.675
  Perplexity: 5.337
  Learning Rate: 9.06e-05
  GPU Memory Used: 12181MB

[25:27:09] Step 66300/125000:
  Training Loss: 1.599
  Perplexity: 4.946
  Learning Rate: 9.05e-05
  GPU Memory Used: 11839MB

[25:28:19] Step 66350/125000:
  Training Loss: 1.628
  Perplexity: 5.094
  Learning Rate: 9.03e-05
  GPU Memory Used: 12128MB

[25:29:28] Step 66400/125000:
  Training Loss: 1.524
  Perplexity: 4.588
  Learning Rate: 9.02e-05
  GPU Memory Used: 11924MB

[25:30:38] Step 66450/125000:
  Training Loss: 1.559
  Perplexity: 4.753
  Learning Rate: 9.01e-05
  GPU Memory Used: 11898MB

[25:31:47] Step 66500/125000:
  Training Loss: 1.569
  Perplexity: 4.802
  Learning Rate: 9.00e-05
  GPU Memory Used: 11885MB


Evaluation Results:
  Validation Loss: 1.762
  Validation Perplexity: 5.823

Saving model checkpoint: proposal_model/checkpoint-66500


Epoch 134/250
==================================================
[25:32:55] Step 66550/125000:
  Training Loss: 1.523
  Perplexity: 4.585
  Learning Rate: 8.98e-05
  GPU Memory Used: 11890MB

[25:34:05] Step 66600/125000:
  Training Loss: 1.514
  Perplexity: 4.547
  Learning Rate: 8.97e-05
  GPU Memory Used: 11976MB

[25:35:13] Step 66650/125000:
  Training Loss: 1.646
  Perplexity: 5.188
  Learning Rate: 8.96e-05
  GPU Memory Used: 12051MB

[25:36:20] Step 66700/125000:
  Training Loss: 1.459
  Perplexity: 4.302
  Learning Rate: 8.95e-05
  GPU Memory Used: 12124MB

[25:37:30] Step 66750/125000:
  Training Loss: 1.588
  Perplexity: 4.894
  Learning Rate: 8.93e-05
  GPU Memory Used: 12125MB

[25:38:38] Step 66800/125000:
  Training Loss: 1.624
  Perplexity: 5.072
  Learning Rate: 8.92e-05
  GPU Memory Used: 11915MB

[25:39:47] Step 66850/125000:
  Training Loss: 1.521
  Perplexity: 4.577
  Learning Rate: 8.91e-05
  GPU Memory Used: 12054MB

[25:40:58] Step 66900/125000:
  Training Loss: 1.588
  Perplexity: 4.894
  Learning Rate: 8.90e-05
  GPU Memory Used: 12086MB

[25:42:08] Step 66950/125000:
  Training Loss: 1.533
  Perplexity: 4.631
  Learning Rate: 8.88e-05
  GPU Memory Used: 12052MB

[25:43:18] Step 67000/125000:
  Training Loss: 1.492
  Perplexity: 4.448
  Learning Rate: 8.87e-05
  GPU Memory Used: 11820MB


Evaluation Results:
  Validation Loss: 1.774
  Validation Perplexity: 5.892

Saving model checkpoint: proposal_model/checkpoint-67000


Epoch 135/250
==================================================
[25:44:28] Step 67050/125000:
  Training Loss: 1.532
  Perplexity: 4.629
  Learning Rate: 8.86e-05
  GPU Memory Used: 12007MB

[25:45:35] Step 67100/125000:
  Training Loss: 1.597
  Perplexity: 4.938
  Learning Rate: 8.85e-05
  GPU Memory Used: 12033MB

[25:46:45] Step 67150/125000:
  Training Loss: 1.511
  Perplexity: 4.530
  Learning Rate: 8.83e-05
  GPU Memory Used: 11972MB

[25:47:54] Step 67200/125000:
  Training Loss: 1.625
  Perplexity: 5.078
  Learning Rate: 8.82e-05
  GPU Memory Used: 12109MB

[25:49:04] Step 67250/125000:
  Training Loss: 1.541
  Perplexity: 4.669
  Learning Rate: 8.81e-05
  GPU Memory Used: 11956MB

[25:50:12] Step 67300/125000:
  Training Loss: 1.659
  Perplexity: 5.253
  Learning Rate: 8.80e-05
  GPU Memory Used: 12056MB

[25:51:20] Step 67350/125000:
  Training Loss: 1.570
  Perplexity: 4.809
  Learning Rate: 8.78e-05
  GPU Memory Used: 12015MB

[25:52:30] Step 67400/125000:
  Training Loss: 1.529
  Perplexity: 4.612
  Learning Rate: 8.77e-05
  GPU Memory Used: 11955MB

[25:53:40] Step 67450/125000:
  Training Loss: 1.590
  Perplexity: 4.902
  Learning Rate: 8.76e-05
  GPU Memory Used: 11920MB

[25:54:49] Step 67500/125000:
  Training Loss: 1.548
  Perplexity: 4.702
  Learning Rate: 8.75e-05
  GPU Memory Used: 12051MB


Evaluation Results:
  Validation Loss: 1.707
  Validation Perplexity: 5.510

Saving model checkpoint: proposal_model/checkpoint-67500


Epoch 136/250
==================================================
[25:56:00] Step 67550/125000:
  Training Loss: 1.567
  Perplexity: 4.791
  Learning Rate: 8.73e-05
  GPU Memory Used: 11896MB

[25:57:08] Step 67600/125000:
  Training Loss: 1.527
  Perplexity: 4.604
  Learning Rate: 8.72e-05
  GPU Memory Used: 12013MB

[25:58:15] Step 67650/125000:
  Training Loss: 1.443
  Perplexity: 4.235
  Learning Rate: 8.71e-05
  GPU Memory Used: 11835MB

[25:59:26] Step 67700/125000:
  Training Loss: 1.535
  Perplexity: 4.642
  Learning Rate: 8.70e-05
  GPU Memory Used: 12155MB

[26:00:36] Step 67750/125000:
  Training Loss: 1.487
  Perplexity: 4.423
  Learning Rate: 8.68e-05
  GPU Memory Used: 12066MB

[26:01:42] Step 67800/125000:
  Training Loss: 1.572
  Perplexity: 4.815
  Learning Rate: 8.67e-05
  GPU Memory Used: 12029MB

[26:02:52] Step 67850/125000:
  Training Loss: 1.498
  Perplexity: 4.473
  Learning Rate: 8.66e-05
  GPU Memory Used: 11875MB

[26:04:02] Step 67900/125000:
  Training Loss: 1.510
  Perplexity: 4.527
  Learning Rate: 8.65e-05
  GPU Memory Used: 11943MB

[26:05:09] Step 67950/125000:
  Training Loss: 1.495
  Perplexity: 4.460
  Learning Rate: 8.63e-05
  GPU Memory Used: 11889MB

[26:06:18] Step 68000/125000:
  Training Loss: 1.521
  Perplexity: 4.578
  Learning Rate: 8.62e-05
  GPU Memory Used: 12052MB


Evaluation Results:
  Validation Loss: 1.697
  Validation Perplexity: 5.460

Saving model checkpoint: proposal_model/checkpoint-68000


Epoch 137/250
==================================================
[26:07:28] Step 68050/125000:
  Training Loss: 1.654
  Perplexity: 5.230
  Learning Rate: 8.61e-05
  GPU Memory Used: 12104MB

[26:08:38] Step 68100/125000:
  Training Loss: 1.601
  Perplexity: 4.956
  Learning Rate: 8.60e-05
  GPU Memory Used: 11898MB

[26:09:48] Step 68150/125000:
  Training Loss: 1.456
  Perplexity: 4.287
  Learning Rate: 8.58e-05
  GPU Memory Used: 12125MB

[26:10:59] Step 68200/125000:
  Training Loss: 1.530
  Perplexity: 4.620
  Learning Rate: 8.57e-05
  GPU Memory Used: 11811MB

[26:12:07] Step 68250/125000:
  Training Loss: 1.548
  Perplexity: 4.701
  Learning Rate: 8.56e-05
  GPU Memory Used: 11904MB

[26:13:16] Step 68300/125000:
  Training Loss: 1.520
  Perplexity: 4.573
  Learning Rate: 8.55e-05
  GPU Memory Used: 12005MB

[26:14:26] Step 68350/125000:
  Training Loss: 1.552
  Perplexity: 4.720
  Learning Rate: 8.54e-05
  GPU Memory Used: 11870MB

[26:15:36] Step 68400/125000:
  Training Loss: 1.685
  Perplexity: 5.390
  Learning Rate: 8.52e-05
  GPU Memory Used: 11921MB

[26:16:45] Step 68450/125000:
  Training Loss: 1.489
  Perplexity: 4.433
  Learning Rate: 8.51e-05
  GPU Memory Used: 12176MB

[26:17:53] Step 68500/125000:
  Training Loss: 1.491
  Perplexity: 4.441
  Learning Rate: 8.50e-05
  GPU Memory Used: 12125MB


Evaluation Results:
  Validation Loss: 1.691
  Validation Perplexity: 5.425

Saving model checkpoint: proposal_model/checkpoint-68500


Epoch 138/250
==================================================
[26:19:03] Step 68550/125000:
  Training Loss: 1.579
  Perplexity: 4.851
  Learning Rate: 8.49e-05
  GPU Memory Used: 12142MB

[26:20:10] Step 68600/125000:
  Training Loss: 1.610
  Perplexity: 5.002
  Learning Rate: 8.47e-05
  GPU Memory Used: 12044MB

[26:21:20] Step 68650/125000:
  Training Loss: 1.609
  Perplexity: 4.999
  Learning Rate: 8.46e-05
  GPU Memory Used: 11841MB

[26:22:29] Step 68700/125000:
  Training Loss: 1.547
  Perplexity: 4.697
  Learning Rate: 8.45e-05
  GPU Memory Used: 11883MB

[26:23:37] Step 68750/125000:
  Training Loss: 1.532
  Perplexity: 4.629
  Learning Rate: 8.44e-05
  GPU Memory Used: 11835MB

[26:24:47] Step 68800/125000:
  Training Loss: 1.615
  Perplexity: 5.030
  Learning Rate: 8.42e-05
  GPU Memory Used: 12098MB

[26:25:56] Step 68850/125000:
  Training Loss: 1.510
  Perplexity: 4.528
  Learning Rate: 8.41e-05
  GPU Memory Used: 12008MB

[26:27:05] Step 68900/125000:
  Training Loss: 1.567
  Perplexity: 4.792
  Learning Rate: 8.40e-05
  GPU Memory Used: 12200MB

[26:28:14] Step 68950/125000:
  Training Loss: 1.632
  Perplexity: 5.112
  Learning Rate: 8.39e-05
  GPU Memory Used: 11892MB

[26:29:24] Step 69000/125000:
  Training Loss: 1.487
  Perplexity: 4.424
  Learning Rate: 8.37e-05
  GPU Memory Used: 12075MB


Evaluation Results:
  Validation Loss: 1.662
  Validation Perplexity: 5.268

Saving model checkpoint: proposal_model/checkpoint-69000


Epoch 139/250
==================================================
[26:30:35] Step 69050/125000:
  Training Loss: 1.574
  Perplexity: 4.828
  Learning Rate: 8.36e-05
  GPU Memory Used: 12018MB

[26:31:44] Step 69100/125000:
  Training Loss: 1.528
  Perplexity: 4.610
  Learning Rate: 8.35e-05
  GPU Memory Used: 11814MB

[26:32:54] Step 69150/125000:
  Training Loss: 1.497
  Perplexity: 4.469
  Learning Rate: 8.34e-05
  GPU Memory Used: 12077MB

[26:34:02] Step 69200/125000:
  Training Loss: 1.548
  Perplexity: 4.700
  Learning Rate: 8.32e-05
  GPU Memory Used: 12039MB

[26:35:10] Step 69250/125000:
  Training Loss: 1.597
  Perplexity: 4.938
  Learning Rate: 8.31e-05
  GPU Memory Used: 11801MB

[26:36:18] Step 69300/125000:
  Training Loss: 1.585
  Perplexity: 4.878
  Learning Rate: 8.30e-05
  GPU Memory Used: 12093MB

[26:37:28] Step 69350/125000:
  Training Loss: 1.446
  Perplexity: 4.246
  Learning Rate: 8.29e-05
  GPU Memory Used: 12178MB

[26:38:38] Step 69400/125000:
  Training Loss: 1.560
  Perplexity: 4.761
  Learning Rate: 8.27e-05
  GPU Memory Used: 12072MB

[26:39:47] Step 69450/125000:
  Training Loss: 1.494
  Perplexity: 4.455
  Learning Rate: 8.26e-05
  GPU Memory Used: 11867MB

[26:40:57] Step 69500/125000:
  Training Loss: 1.524
  Perplexity: 4.592
  Learning Rate: 8.25e-05
  GPU Memory Used: 11826MB


Evaluation Results:
  Validation Loss: 1.506
  Validation Perplexity: 4.508

Saving model checkpoint: proposal_model/checkpoint-69500


Epoch 140/250
==================================================
[26:42:05] Step 69550/125000:
  Training Loss: 1.525
  Perplexity: 4.596
  Learning Rate: 8.24e-05
  GPU Memory Used: 12011MB

[26:43:14] Step 69600/125000:
  Training Loss: 1.556
  Perplexity: 4.741
  Learning Rate: 8.23e-05
  GPU Memory Used: 12091MB

[26:44:23] Step 69650/125000:
  Training Loss: 1.612
  Perplexity: 5.014
  Learning Rate: 8.21e-05
  GPU Memory Used: 11966MB

[26:45:32] Step 69700/125000:
  Training Loss: 1.592
  Perplexity: 4.912
  Learning Rate: 8.20e-05
  GPU Memory Used: 12077MB

[26:46:42] Step 69750/125000:
  Training Loss: 1.484
  Perplexity: 4.410
  Learning Rate: 8.19e-05
  GPU Memory Used: 12032MB

[26:47:49] Step 69800/125000:
  Training Loss: 1.628
  Perplexity: 5.092
  Learning Rate: 8.18e-05
  GPU Memory Used: 12034MB

[26:48:59] Step 69850/125000:
  Training Loss: 1.519
  Perplexity: 4.567
  Learning Rate: 8.16e-05
  GPU Memory Used: 12167MB

[26:50:05] Step 69900/125000:
  Training Loss: 1.521
  Perplexity: 4.578
  Learning Rate: 8.15e-05
  GPU Memory Used: 11814MB

[26:51:14] Step 69950/125000:
  Training Loss: 1.538
  Perplexity: 4.657
  Learning Rate: 8.14e-05
  GPU Memory Used: 12160MB

[26:52:25] Step 70000/125000:
  Training Loss: 1.445
  Perplexity: 4.242
  Learning Rate: 8.13e-05
  GPU Memory Used: 11842MB


Evaluation Results:
  Validation Loss: 1.688
  Validation Perplexity: 5.411

Saving model checkpoint: proposal_model/checkpoint-70000


Epoch 141/250
==================================================
[26:53:35] Step 70050/125000:
  Training Loss: 1.534
  Perplexity: 4.639
  Learning Rate: 8.11e-05
  GPU Memory Used: 11855MB

[26:54:46] Step 70100/125000:
  Training Loss: 1.460
  Perplexity: 4.308
  Learning Rate: 8.10e-05
  GPU Memory Used: 12097MB

[26:55:55] Step 70150/125000:
  Training Loss: 1.501
  Perplexity: 4.484
  Learning Rate: 8.09e-05
  GPU Memory Used: 12184MB

[26:57:03] Step 70200/125000:
  Training Loss: 1.499
  Perplexity: 4.477
  Learning Rate: 8.08e-05
  GPU Memory Used: 12099MB

[26:58:12] Step 70250/125000:
  Training Loss: 1.554
  Perplexity: 4.729
  Learning Rate: 8.06e-05
  GPU Memory Used: 11877MB

[26:59:19] Step 70300/125000:
  Training Loss: 1.496
  Perplexity: 4.464
  Learning Rate: 8.05e-05
  GPU Memory Used: 12195MB

[27:00:29] Step 70350/125000:
  Training Loss: 1.556
  Perplexity: 4.741
  Learning Rate: 8.04e-05
  GPU Memory Used: 11823MB

[27:01:37] Step 70400/125000:
  Training Loss: 1.582
  Perplexity: 4.867
  Learning Rate: 8.03e-05
  GPU Memory Used: 12154MB

[27:02:47] Step 70450/125000:
  Training Loss: 1.624
  Perplexity: 5.075
  Learning Rate: 8.02e-05
  GPU Memory Used: 12165MB

[27:03:58] Step 70500/125000:
  Training Loss: 1.523
  Perplexity: 4.586
  Learning Rate: 8.00e-05
  GPU Memory Used: 11977MB


Evaluation Results:
  Validation Loss: 1.607
  Validation Perplexity: 4.990

Saving model checkpoint: proposal_model/checkpoint-70500


Epoch 142/250
==================================================
[27:05:09] Step 70550/125000:
  Training Loss: 1.612
  Perplexity: 5.013
  Learning Rate: 7.99e-05
  GPU Memory Used: 12046MB

[27:06:20] Step 70600/125000:
  Training Loss: 1.361
  Perplexity: 3.900
  Learning Rate: 7.98e-05
  GPU Memory Used: 12119MB

[27:07:28] Step 70650/125000:
  Training Loss: 1.534
  Perplexity: 4.638
  Learning Rate: 7.97e-05
  GPU Memory Used: 11907MB

[27:08:39] Step 70700/125000:
  Training Loss: 1.486
  Perplexity: 4.418
  Learning Rate: 7.95e-05
  GPU Memory Used: 12010MB

[27:09:49] Step 70750/125000:
  Training Loss: 1.636
  Perplexity: 5.133
  Learning Rate: 7.94e-05
  GPU Memory Used: 12139MB

[27:10:56] Step 70800/125000:
  Training Loss: 1.578
  Perplexity: 4.847
  Learning Rate: 7.93e-05
  GPU Memory Used: 11849MB

[27:12:04] Step 70850/125000:
  Training Loss: 1.721
  Perplexity: 5.590
  Learning Rate: 7.92e-05
  GPU Memory Used: 11918MB

[27:13:13] Step 70900/125000:
  Training Loss: 1.459
  Perplexity: 4.302
  Learning Rate: 7.90e-05
  GPU Memory Used: 11935MB

[27:14:22] Step 70950/125000:
  Training Loss: 1.622
  Perplexity: 5.065
  Learning Rate: 7.89e-05
  GPU Memory Used: 11931MB

[27:15:32] Step 71000/125000:
  Training Loss: 1.457
  Perplexity: 4.291
  Learning Rate: 7.88e-05
  GPU Memory Used: 12115MB


Evaluation Results:
  Validation Loss: 1.602
  Validation Perplexity: 4.963

Saving model checkpoint: proposal_model/checkpoint-71000


Epoch 143/250
==================================================
[27:16:42] Step 71050/125000:
  Training Loss: 1.490
  Perplexity: 4.437
  Learning Rate: 7.87e-05
  GPU Memory Used: 11898MB

[27:17:51] Step 71100/125000:
  Training Loss: 1.533
  Perplexity: 4.630
  Learning Rate: 7.86e-05
  GPU Memory Used: 12105MB

[27:19:00] Step 71150/125000:
  Training Loss: 1.556
  Perplexity: 4.740
  Learning Rate: 7.84e-05
  GPU Memory Used: 12005MB

[27:20:09] Step 71200/125000:
  Training Loss: 1.540
  Perplexity: 4.665
  Learning Rate: 7.83e-05
  GPU Memory Used: 12069MB

[27:21:19] Step 71250/125000:
  Training Loss: 1.428
  Perplexity: 4.172
  Learning Rate: 7.82e-05
  GPU Memory Used: 11837MB

[27:22:28] Step 71300/125000:
  Training Loss: 1.507
  Perplexity: 4.514
  Learning Rate: 7.81e-05
  GPU Memory Used: 11967MB

[27:23:39] Step 71350/125000:
  Training Loss: 1.490
  Perplexity: 4.436
  Learning Rate: 7.79e-05
  GPU Memory Used: 11955MB

[27:24:48] Step 71400/125000:
  Training Loss: 1.658
  Perplexity: 5.248
  Learning Rate: 7.78e-05
  GPU Memory Used: 11940MB

[27:25:57] Step 71450/125000:
  Training Loss: 1.564
  Perplexity: 4.777
  Learning Rate: 7.77e-05
  GPU Memory Used: 12057MB

[27:27:07] Step 71500/125000:
  Training Loss: 1.635
  Perplexity: 5.127
  Learning Rate: 7.76e-05
  GPU Memory Used: 11847MB


Evaluation Results:
  Validation Loss: 1.598
  Validation Perplexity: 4.945

Saving model checkpoint: proposal_model/checkpoint-71500


Epoch 144/250
==================================================
[27:28:13] Step 71550/125000:
  Training Loss: 1.506
  Perplexity: 4.508
  Learning Rate: 7.75e-05
  GPU Memory Used: 12072MB

[27:29:24] Step 71600/125000:
  Training Loss: 1.514
  Perplexity: 4.546
  Learning Rate: 7.73e-05
  GPU Memory Used: 12007MB

[27:30:32] Step 71650/125000:
  Training Loss: 1.543
  Perplexity: 4.679
  Learning Rate: 7.72e-05
  GPU Memory Used: 11936MB

[27:31:41] Step 71700/125000:
  Training Loss: 1.589
  Perplexity: 4.898
  Learning Rate: 7.71e-05
  GPU Memory Used: 12080MB

[27:32:50] Step 71750/125000:
  Training Loss: 1.492
  Perplexity: 4.445
  Learning Rate: 7.70e-05
  GPU Memory Used: 12147MB

[27:33:57] Step 71800/125000:
  Training Loss: 1.448
  Perplexity: 4.253
  Learning Rate: 7.68e-05
  GPU Memory Used: 11870MB

[27:35:05] Step 71850/125000:
  Training Loss: 1.507
  Perplexity: 4.513
  Learning Rate: 7.67e-05
  GPU Memory Used: 12056MB

[27:36:15] Step 71900/125000:
  Training Loss: 1.495
  Perplexity: 4.460
  Learning Rate: 7.66e-05
  GPU Memory Used: 12165MB

[27:37:26] Step 71950/125000:
  Training Loss: 1.574
  Perplexity: 4.824
  Learning Rate: 7.65e-05
  GPU Memory Used: 11981MB

[27:38:34] Step 72000/125000:
  Training Loss: 1.490
  Perplexity: 4.438
  Learning Rate: 7.64e-05
  GPU Memory Used: 11850MB


Evaluation Results:
  Validation Loss: 1.706
  Validation Perplexity: 5.509

Saving model checkpoint: proposal_model/checkpoint-72000


Epoch 145/250
==================================================
[27:39:41] Step 72050/125000:
  Training Loss: 1.561
  Perplexity: 4.762
  Learning Rate: 7.62e-05
  GPU Memory Used: 11806MB

[27:40:52] Step 72100/125000:
  Training Loss: 1.550
  Perplexity: 4.712
  Learning Rate: 7.61e-05
  GPU Memory Used: 12086MB

[27:42:01] Step 72150/125000:
  Training Loss: 1.519
  Perplexity: 4.566
  Learning Rate: 7.60e-05
  GPU Memory Used: 11963MB

[27:43:12] Step 72200/125000:
  Training Loss: 1.511
  Perplexity: 4.531
  Learning Rate: 7.59e-05
  GPU Memory Used: 11872MB

[27:44:21] Step 72250/125000:
  Training Loss: 1.473
  Perplexity: 4.363
  Learning Rate: 7.57e-05
  GPU Memory Used: 11947MB

[27:45:29] Step 72300/125000:
  Training Loss: 1.552
  Perplexity: 4.719
  Learning Rate: 7.56e-05
  GPU Memory Used: 12091MB

[27:46:39] Step 72350/125000:
  Training Loss: 1.624
  Perplexity: 5.074
  Learning Rate: 7.55e-05
  GPU Memory Used: 11807MB

[27:47:47] Step 72400/125000:
  Training Loss: 1.551
  Perplexity: 4.714
  Learning Rate: 7.54e-05
  GPU Memory Used: 11816MB

[27:48:56] Step 72450/125000:
  Training Loss: 1.519
  Perplexity: 4.567
  Learning Rate: 7.53e-05
  GPU Memory Used: 11869MB

[27:50:06] Step 72500/125000:
  Training Loss: 1.489
  Perplexity: 4.434
  Learning Rate: 7.51e-05
  GPU Memory Used: 12099MB


Evaluation Results:
  Validation Loss: 1.667
  Validation Perplexity: 5.296

Saving model checkpoint: proposal_model/checkpoint-72500


Epoch 146/250
==================================================
[27:51:13] Step 72550/125000:
  Training Loss: 1.427
  Perplexity: 4.165
  Learning Rate: 7.50e-05
  GPU Memory Used: 11885MB

[27:52:22] Step 72600/125000:
  Training Loss: 1.543
  Perplexity: 4.677
  Learning Rate: 7.49e-05
  GPU Memory Used: 11852MB

[27:53:30] Step 72650/125000:
  Training Loss: 1.513
  Perplexity: 4.538
  Learning Rate: 7.48e-05
  GPU Memory Used: 12169MB

[27:54:41] Step 72700/125000:
  Training Loss: 1.497
  Perplexity: 4.470
  Learning Rate: 7.46e-05
  GPU Memory Used: 12145MB

[27:55:50] Step 72750/125000:
  Training Loss: 1.604
  Perplexity: 4.972
  Learning Rate: 7.45e-05
  GPU Memory Used: 11910MB

[27:56:59] Step 72800/125000:
  Training Loss: 1.650
  Perplexity: 5.204
  Learning Rate: 7.44e-05
  GPU Memory Used: 11935MB

[27:58:07] Step 72850/125000:
  Training Loss: 1.473
  Perplexity: 4.360
  Learning Rate: 7.43e-05
  GPU Memory Used: 11825MB

[27:59:17] Step 72900/125000:
  Training Loss: 1.469
  Perplexity: 4.344
  Learning Rate: 7.42e-05
  GPU Memory Used: 11817MB

[28:00:28] Step 72950/125000:
  Training Loss: 1.558
  Perplexity: 4.749
  Learning Rate: 7.40e-05
  GPU Memory Used: 12117MB

[28:01:37] Step 73000/125000:
  Training Loss: 1.646
  Perplexity: 5.188
  Learning Rate: 7.39e-05
  GPU Memory Used: 11869MB


Evaluation Results:
  Validation Loss: 1.713
  Validation Perplexity: 5.546

Saving model checkpoint: proposal_model/checkpoint-73000


Epoch 147/250
==================================================
[28:02:46] Step 73050/125000:
  Training Loss: 1.484
  Perplexity: 4.411
  Learning Rate: 7.38e-05
  GPU Memory Used: 12146MB

[28:03:55] Step 73100/125000:
  Training Loss: 1.517
  Perplexity: 4.557
  Learning Rate: 7.37e-05
  GPU Memory Used: 11875MB

[28:05:05] Step 73150/125000:
  Training Loss: 1.441
  Perplexity: 4.226
  Learning Rate: 7.36e-05
  GPU Memory Used: 12184MB

[28:06:12] Step 73200/125000:
  Training Loss: 1.501
  Perplexity: 4.484
  Learning Rate: 7.34e-05
  GPU Memory Used: 11910MB

[28:07:21] Step 73250/125000:
  Training Loss: 1.422
  Perplexity: 4.146
  Learning Rate: 7.33e-05
  GPU Memory Used: 11975MB

[28:08:32] Step 73300/125000:
  Training Loss: 1.501
  Perplexity: 4.488
  Learning Rate: 7.32e-05
  GPU Memory Used: 12092MB

[28:09:44] Step 73350/125000:
  Training Loss: 1.470
  Perplexity: 4.351
  Learning Rate: 7.31e-05
  GPU Memory Used: 12025MB

[28:10:54] Step 73400/125000:
  Training Loss: 1.506
  Perplexity: 4.507
  Learning Rate: 7.29e-05
  GPU Memory Used: 11840MB

[28:12:05] Step 73450/125000:
  Training Loss: 1.582
  Perplexity: 4.862
  Learning Rate: 7.28e-05
  GPU Memory Used: 12030MB

[28:13:15] Step 73500/125000:
  Training Loss: 1.570
  Perplexity: 4.808
  Learning Rate: 7.27e-05
  GPU Memory Used: 11886MB


Evaluation Results:
  Validation Loss: 1.763
  Validation Perplexity: 5.828

Saving model checkpoint: proposal_model/checkpoint-73500


Epoch 148/250
==================================================
[28:14:23] Step 73550/125000:
  Training Loss: 1.523
  Perplexity: 4.587
  Learning Rate: 7.26e-05
  GPU Memory Used: 11919MB

[28:15:32] Step 73600/125000:
  Training Loss: 1.402
  Perplexity: 4.064
  Learning Rate: 7.25e-05
  GPU Memory Used: 12112MB

[28:16:41] Step 73650/125000:
  Training Loss: 1.612
  Perplexity: 5.011
  Learning Rate: 7.23e-05
  GPU Memory Used: 12101MB

[28:17:49] Step 73700/125000:
  Training Loss: 1.486
  Perplexity: 4.420
  Learning Rate: 7.22e-05
  GPU Memory Used: 12106MB

[28:18:58] Step 73750/125000:
  Training Loss: 1.480
  Perplexity: 4.395
  Learning Rate: 7.21e-05
  GPU Memory Used: 12088MB

[28:20:06] Step 73800/125000:
  Training Loss: 1.479
  Perplexity: 4.388
  Learning Rate: 7.20e-05
  GPU Memory Used: 12104MB

[28:21:16] Step 73850/125000:
  Training Loss: 1.508
  Perplexity: 4.518
  Learning Rate: 7.19e-05
  GPU Memory Used: 11883MB

[28:22:26] Step 73900/125000:
  Training Loss: 1.517
  Perplexity: 4.559
  Learning Rate: 7.17e-05
  GPU Memory Used: 12045MB

[28:23:36] Step 73950/125000:
  Training Loss: 1.522
  Perplexity: 4.582
  Learning Rate: 7.16e-05
  GPU Memory Used: 11862MB

[28:24:45] Step 74000/125000:
  Training Loss: 1.468
  Perplexity: 4.341
  Learning Rate: 7.15e-05
  GPU Memory Used: 11921MB


Evaluation Results:
  Validation Loss: 1.636
  Validation Perplexity: 5.133

Saving model checkpoint: proposal_model/checkpoint-74000


Epoch 149/250
==================================================
[28:25:53] Step 74050/125000:
  Training Loss: 1.515
  Perplexity: 4.549
  Learning Rate: 7.14e-05
  GPU Memory Used: 11884MB

[28:27:02] Step 74100/125000:
  Training Loss: 1.585
  Perplexity: 4.877
  Learning Rate: 7.13e-05
  GPU Memory Used: 11979MB

[28:28:13] Step 74150/125000:
  Training Loss: 1.485
  Perplexity: 4.414
  Learning Rate: 7.11e-05
  GPU Memory Used: 11900MB

[28:29:24] Step 74200/125000:
  Training Loss: 1.614
  Perplexity: 5.025
  Learning Rate: 7.10e-05
  GPU Memory Used: 11861MB

[28:30:33] Step 74250/125000:
  Training Loss: 1.528
  Perplexity: 4.609
  Learning Rate: 7.09e-05
  GPU Memory Used: 11857MB

[28:31:41] Step 74300/125000:
  Training Loss: 1.536
  Perplexity: 4.645
  Learning Rate: 7.08e-05
  GPU Memory Used: 11906MB

[28:32:51] Step 74350/125000:
  Training Loss: 1.564
  Perplexity: 4.777
  Learning Rate: 7.07e-05
  GPU Memory Used: 11846MB

[28:33:58] Step 74400/125000:
  Training Loss: 1.556
  Perplexity: 4.742
  Learning Rate: 7.05e-05
  GPU Memory Used: 12015MB

[28:35:08] Step 74450/125000:
  Training Loss: 1.494
  Perplexity: 4.453
  Learning Rate: 7.04e-05
  GPU Memory Used: 12120MB

[28:36:16] Step 74500/125000:
  Training Loss: 1.447
  Perplexity: 4.252
  Learning Rate: 7.03e-05
  GPU Memory Used: 11879MB


Evaluation Results:
  Validation Loss: 1.590
  Validation Perplexity: 4.905

Saving model checkpoint: proposal_model/checkpoint-74500


Epoch 150/250
==================================================
[28:37:26] Step 74550/125000:
  Training Loss: 1.550
  Perplexity: 4.711
  Learning Rate: 7.02e-05
  GPU Memory Used: 11992MB

[28:38:35] Step 74600/125000:
  Training Loss: 1.580
  Perplexity: 4.855
  Learning Rate: 7.01e-05
  GPU Memory Used: 11909MB

[28:39:45] Step 74650/125000:
  Training Loss: 1.590
  Perplexity: 4.902
  Learning Rate: 6.99e-05
  GPU Memory Used: 12039MB

[28:40:56] Step 74700/125000:
  Training Loss: 1.519
  Perplexity: 4.567
  Learning Rate: 6.98e-05
  GPU Memory Used: 12074MB

[28:42:06] Step 74750/125000:
  Training Loss: 1.557
  Perplexity: 4.744
  Learning Rate: 6.97e-05
  GPU Memory Used: 11951MB

[28:43:15] Step 74800/125000:
  Training Loss: 1.403
  Perplexity: 4.068
  Learning Rate: 6.96e-05
  GPU Memory Used: 12013MB

[28:44:25] Step 74850/125000:
  Training Loss: 1.499
  Perplexity: 4.476
  Learning Rate: 6.95e-05
  GPU Memory Used: 12191MB

[28:45:33] Step 74900/125000:
  Training Loss: 1.578
  Perplexity: 4.847
  Learning Rate: 6.93e-05
  GPU Memory Used: 12189MB

[28:46:41] Step 74950/125000:
  Training Loss: 1.507
  Perplexity: 4.513
  Learning Rate: 6.92e-05
  GPU Memory Used: 11904MB

[28:47:50] Step 75000/125000:
  Training Loss: 1.573
  Perplexity: 4.822
  Learning Rate: 6.91e-05
  GPU Memory Used: 11861MB


Evaluation Results:
  Validation Loss: 1.648
  Validation Perplexity: 5.197

Saving model checkpoint: proposal_model/checkpoint-75000


Epoch 151/250
==================================================
[28:48:57] Step 75050/125000:
  Training Loss: 1.598
  Perplexity: 4.941
  Learning Rate: 6.90e-05
  GPU Memory Used: 12109MB

[28:50:07] Step 75100/125000:
  Training Loss: 1.503
  Perplexity: 4.495
  Learning Rate: 6.89e-05
  GPU Memory Used: 11973MB

[28:51:17] Step 75150/125000:
  Training Loss: 1.431
  Perplexity: 4.184
  Learning Rate: 6.87e-05
  GPU Memory Used: 11978MB

[28:52:27] Step 75200/125000:
  Training Loss: 1.506
  Perplexity: 4.507
  Learning Rate: 6.86e-05
  GPU Memory Used: 12156MB

[28:53:37] Step 75250/125000:
  Training Loss: 1.533
  Perplexity: 4.634
  Learning Rate: 6.85e-05
  GPU Memory Used: 11867MB

[28:54:46] Step 75300/125000:
  Training Loss: 1.485
  Perplexity: 4.414
  Learning Rate: 6.84e-05
  GPU Memory Used: 11885MB

[28:55:55] Step 75350/125000:
  Training Loss: 1.460
  Perplexity: 4.304
  Learning Rate: 6.83e-05
  GPU Memory Used: 12134MB

[28:57:07] Step 75400/125000:
  Training Loss: 1.497
  Perplexity: 4.470
  Learning Rate: 6.81e-05
  GPU Memory Used: 11940MB

[28:58:15] Step 75450/125000:
  Training Loss: 1.481
  Perplexity: 4.398
  Learning Rate: 6.80e-05
  GPU Memory Used: 12183MB

[28:59:24] Step 75500/125000:
  Training Loss: 1.463
  Perplexity: 4.318
  Learning Rate: 6.79e-05
  GPU Memory Used: 11839MB


Evaluation Results:
  Validation Loss: 1.715
  Validation Perplexity: 5.559

Saving model checkpoint: proposal_model/checkpoint-75500


Epoch 152/250
==================================================
[29:00:33] Step 75550/125000:
  Training Loss: 1.439
  Perplexity: 4.217
  Learning Rate: 6.78e-05
  GPU Memory Used: 11897MB

[29:01:40] Step 75600/125000:
  Training Loss: 1.504
  Perplexity: 4.502
  Learning Rate: 6.77e-05
  GPU Memory Used: 11994MB

[29:02:47] Step 75650/125000:
  Training Loss: 1.453
  Perplexity: 4.275
  Learning Rate: 6.75e-05
  GPU Memory Used: 11822MB

[29:03:55] Step 75700/125000:
  Training Loss: 1.554
  Perplexity: 4.729
  Learning Rate: 6.74e-05
  GPU Memory Used: 12134MB

[29:05:04] Step 75750/125000:
  Training Loss: 1.570
  Perplexity: 4.806
  Learning Rate: 6.73e-05
  GPU Memory Used: 12083MB

[29:06:13] Step 75800/125000:
  Training Loss: 1.476
  Perplexity: 4.377
  Learning Rate: 6.72e-05
  GPU Memory Used: 12144MB

[29:07:22] Step 75850/125000:
  Training Loss: 1.593
  Perplexity: 4.919
  Learning Rate: 6.71e-05
  GPU Memory Used: 11978MB

[29:08:31] Step 75900/125000:
  Training Loss: 1.580
  Perplexity: 4.855
  Learning Rate: 6.70e-05
  GPU Memory Used: 11986MB

[29:09:39] Step 75950/125000:
  Training Loss: 1.475
  Perplexity: 4.371
  Learning Rate: 6.68e-05
  GPU Memory Used: 11989MB

[29:10:48] Step 76000/125000:
  Training Loss: 1.432
  Perplexity: 4.189
  Learning Rate: 6.67e-05
  GPU Memory Used: 12108MB


Evaluation Results:
  Validation Loss: 1.649
  Validation Perplexity: 5.200

Saving model checkpoint: proposal_model/checkpoint-76000


Epoch 153/250
==================================================
[29:11:57] Step 76050/125000:
  Training Loss: 1.454
  Perplexity: 4.278
  Learning Rate: 6.66e-05
  GPU Memory Used: 12142MB

[29:13:07] Step 76100/125000:
  Training Loss: 1.434
  Perplexity: 4.196
  Learning Rate: 6.65e-05
  GPU Memory Used: 11804MB

[29:14:16] Step 76150/125000:
  Training Loss: 1.417
  Perplexity: 4.126
  Learning Rate: 6.64e-05
  GPU Memory Used: 11941MB

[29:15:24] Step 76200/125000:
  Training Loss: 1.441
  Perplexity: 4.227
  Learning Rate: 6.62e-05
  GPU Memory Used: 12122MB

[29:16:34] Step 76250/125000:
  Training Loss: 1.499
  Perplexity: 4.477
  Learning Rate: 6.61e-05
  GPU Memory Used: 11858MB

[29:17:43] Step 76300/125000:
  Training Loss: 1.465
  Perplexity: 4.329
  Learning Rate: 6.60e-05
  GPU Memory Used: 12150MB

[29:18:54] Step 76350/125000:
  Training Loss: 1.534
  Perplexity: 4.637
  Learning Rate: 6.59e-05
  GPU Memory Used: 12199MB

[29:20:05] Step 76400/125000:
  Training Loss: 1.465
  Perplexity: 4.327
  Learning Rate: 6.58e-05
  GPU Memory Used: 12141MB

[29:21:15] Step 76450/125000:
  Training Loss: 1.564
  Perplexity: 4.778
  Learning Rate: 6.57e-05
  GPU Memory Used: 12021MB

[29:22:24] Step 76500/125000:
  Training Loss: 1.473
  Perplexity: 4.360
  Learning Rate: 6.55e-05
  GPU Memory Used: 11839MB


Evaluation Results:
  Validation Loss: 1.597
  Validation Perplexity: 4.940

Saving model checkpoint: proposal_model/checkpoint-76500


Epoch 154/250
==================================================
[29:23:32] Step 76550/125000:
  Training Loss: 1.430
  Perplexity: 4.178
  Learning Rate: 6.54e-05
  GPU Memory Used: 12195MB

[29:24:41] Step 76600/125000:
  Training Loss: 1.512
  Perplexity: 4.534
  Learning Rate: 6.53e-05
  GPU Memory Used: 11994MB

[29:25:51] Step 76650/125000:
  Training Loss: 1.516
  Perplexity: 4.554
  Learning Rate: 6.52e-05
  GPU Memory Used: 12172MB

[29:26:59] Step 76700/125000:
  Training Loss: 1.485
  Perplexity: 4.413
  Learning Rate: 6.51e-05
  GPU Memory Used: 11811MB

[29:28:06] Step 76750/125000:
  Training Loss: 1.506
  Perplexity: 4.507
  Learning Rate: 6.49e-05
  GPU Memory Used: 11982MB

[29:29:16] Step 76800/125000:
  Training Loss: 1.498
  Perplexity: 4.471
  Learning Rate: 6.48e-05
  GPU Memory Used: 12181MB

[29:30:25] Step 76850/125000:
  Training Loss: 1.446
  Perplexity: 4.246
  Learning Rate: 6.47e-05
  GPU Memory Used: 12110MB

[29:31:35] Step 76900/125000:
  Training Loss: 1.571
  Perplexity: 4.809
  Learning Rate: 6.46e-05
  GPU Memory Used: 12041MB

[29:32:45] Step 76950/125000:
  Training Loss: 1.481
  Perplexity: 4.398
  Learning Rate: 6.45e-05
  GPU Memory Used: 12183MB

[29:33:53] Step 77000/125000:
  Training Loss: 1.494
  Perplexity: 4.454
  Learning Rate: 6.44e-05
  GPU Memory Used: 11942MB


Evaluation Results:
  Validation Loss: 1.610
  Validation Perplexity: 5.004

Saving model checkpoint: proposal_model/checkpoint-77000


Epoch 155/250
==================================================
[29:35:02] Step 77050/125000:
  Training Loss: 1.551
  Perplexity: 4.716
  Learning Rate: 6.42e-05
  GPU Memory Used: 12142MB

[29:36:10] Step 77100/125000:
  Training Loss: 1.549
  Perplexity: 4.707
  Learning Rate: 6.41e-05
  GPU Memory Used: 11984MB

[29:37:20] Step 77150/125000:
  Training Loss: 1.551
  Perplexity: 4.714
  Learning Rate: 6.40e-05
  GPU Memory Used: 11812MB

[29:38:27] Step 77200/125000:
  Training Loss: 1.594
  Perplexity: 4.921
  Learning Rate: 6.39e-05
  GPU Memory Used: 11848MB

[29:39:35] Step 77250/125000:
  Training Loss: 1.567
  Perplexity: 4.791
  Learning Rate: 6.38e-05
  GPU Memory Used: 11871MB

[29:40:44] Step 77300/125000:
  Training Loss: 1.501
  Perplexity: 4.488
  Learning Rate: 6.37e-05
  GPU Memory Used: 12134MB

[29:41:53] Step 77350/125000:
  Training Loss: 1.511
  Perplexity: 4.533
  Learning Rate: 6.35e-05
  GPU Memory Used: 11970MB

[29:43:02] Step 77400/125000:
  Training Loss: 1.447
  Perplexity: 4.251
  Learning Rate: 6.34e-05
  GPU Memory Used: 12054MB

[29:44:10] Step 77450/125000:
  Training Loss: 1.507
  Perplexity: 4.512
  Learning Rate: 6.33e-05
  GPU Memory Used: 12131MB

[29:45:18] Step 77500/125000:
  Training Loss: 1.568
  Perplexity: 4.798
  Learning Rate: 6.32e-05
  GPU Memory Used: 11865MB


Evaluation Results:
  Validation Loss: 1.624
  Validation Perplexity: 5.072

Saving model checkpoint: proposal_model/checkpoint-77500


Epoch 156/250
==================================================
[29:46:27] Step 77550/125000:
  Training Loss: 1.536
  Perplexity: 4.647
  Learning Rate: 6.31e-05
  GPU Memory Used: 11889MB

[29:47:35] Step 77600/125000:
  Training Loss: 1.544
  Perplexity: 4.684
  Learning Rate: 6.30e-05
  GPU Memory Used: 12038MB

[29:48:44] Step 77650/125000:
  Training Loss: 1.506
  Perplexity: 4.511
  Learning Rate: 6.28e-05
  GPU Memory Used: 12078MB

[29:49:52] Step 77700/125000:
  Training Loss: 1.458
  Perplexity: 4.298
  Learning Rate: 6.27e-05
  GPU Memory Used: 11900MB

[29:51:01] Step 77750/125000:
  Training Loss: 1.466
  Perplexity: 4.332
  Learning Rate: 6.26e-05
  GPU Memory Used: 11973MB

[29:52:12] Step 77800/125000:
  Training Loss: 1.489
  Perplexity: 4.434
  Learning Rate: 6.25e-05
  GPU Memory Used: 12080MB

[29:53:23] Step 77850/125000:
  Training Loss: 1.402
  Perplexity: 4.064
  Learning Rate: 6.24e-05
  GPU Memory Used: 11977MB

[29:54:31] Step 77900/125000:
  Training Loss: 1.478
  Perplexity: 4.386
  Learning Rate: 6.23e-05
  GPU Memory Used: 12099MB

[29:55:39] Step 77950/125000:
  Training Loss: 1.450
  Perplexity: 4.263
  Learning Rate: 6.21e-05
  GPU Memory Used: 12053MB

[29:56:47] Step 78000/125000:
  Training Loss: 1.598
  Perplexity: 4.944
  Learning Rate: 6.20e-05
  GPU Memory Used: 11808MB


Evaluation Results:
  Validation Loss: 1.639
  Validation Perplexity: 5.150

Saving model checkpoint: proposal_model/checkpoint-78000


Epoch 157/250
==================================================
[29:57:56] Step 78050/125000:
  Training Loss: 1.512
  Perplexity: 4.535
  Learning Rate: 6.19e-05
  GPU Memory Used: 12123MB

[29:59:04] Step 78100/125000:
  Training Loss: 1.559
  Perplexity: 4.754
  Learning Rate: 6.18e-05
  GPU Memory Used: 12179MB

[30:00:14] Step 78150/125000:
  Training Loss: 1.575
  Perplexity: 4.830
  Learning Rate: 6.17e-05
  GPU Memory Used: 12188MB

[30:01:23] Step 78200/125000:
  Training Loss: 1.537
  Perplexity: 4.652
  Learning Rate: 6.16e-05
  GPU Memory Used: 11838MB

[30:02:30] Step 78250/125000:
  Training Loss: 1.414
  Perplexity: 4.110
  Learning Rate: 6.14e-05
  GPU Memory Used: 12056MB

[30:03:41] Step 78300/125000:
  Training Loss: 1.523
  Perplexity: 4.585
  Learning Rate: 6.13e-05
  GPU Memory Used: 11943MB

[30:04:49] Step 78350/125000:
  Training Loss: 1.566
  Perplexity: 4.787
  Learning Rate: 6.12e-05
  GPU Memory Used: 12026MB

[30:06:00] Step 78400/125000:
  Training Loss: 1.446
  Perplexity: 4.246
  Learning Rate: 6.11e-05
  GPU Memory Used: 12045MB

[30:07:09] Step 78450/125000:
  Training Loss: 1.518
  Perplexity: 4.565
  Learning Rate: 6.10e-05
  GPU Memory Used: 12143MB

[30:08:18] Step 78500/125000:
  Training Loss: 1.447
  Perplexity: 4.251
  Learning Rate: 6.09e-05
  GPU Memory Used: 12103MB


Evaluation Results:
  Validation Loss: 1.581
  Validation Perplexity: 4.857

Saving model checkpoint: proposal_model/checkpoint-78500


Epoch 158/250
==================================================
[30:09:29] Step 78550/125000:
  Training Loss: 1.523
  Perplexity: 4.584
  Learning Rate: 6.07e-05
  GPU Memory Used: 11808MB

[30:10:37] Step 78600/125000:
  Training Loss: 1.492
  Perplexity: 4.447
  Learning Rate: 6.06e-05
  GPU Memory Used: 11832MB

[30:11:46] Step 78650/125000:
  Training Loss: 1.514
  Perplexity: 4.544
  Learning Rate: 6.05e-05
  GPU Memory Used: 11932MB

[30:12:58] Step 78700/125000:
  Training Loss: 1.495
  Perplexity: 4.460
  Learning Rate: 6.04e-05
  GPU Memory Used: 11817MB

[30:14:09] Step 78750/125000:
  Training Loss: 1.460
  Perplexity: 4.305
  Learning Rate: 6.03e-05
  GPU Memory Used: 11916MB

[30:15:17] Step 78800/125000:
  Training Loss: 1.525
  Perplexity: 4.593
  Learning Rate: 6.02e-05
  GPU Memory Used: 11848MB

[30:16:26] Step 78850/125000:
  Training Loss: 1.553
  Perplexity: 4.726
  Learning Rate: 6.01e-05
  GPU Memory Used: 11861MB

[30:17:34] Step 78900/125000:
  Training Loss: 1.508
  Perplexity: 4.516
  Learning Rate: 5.99e-05
  GPU Memory Used: 11979MB

[30:18:43] Step 78950/125000:
  Training Loss: 1.514
  Perplexity: 4.546
  Learning Rate: 5.98e-05
  GPU Memory Used: 12054MB

[30:19:52] Step 79000/125000:
  Training Loss: 1.518
  Perplexity: 4.561
  Learning Rate: 5.97e-05
  GPU Memory Used: 12200MB


Evaluation Results:
  Validation Loss: 1.601
  Validation Perplexity: 4.958

Saving model checkpoint: proposal_model/checkpoint-79000


Epoch 159/250
==================================================
[30:21:02] Step 79050/125000:
  Training Loss: 1.493
  Perplexity: 4.451
  Learning Rate: 5.96e-05
  GPU Memory Used: 11922MB

[30:22:10] Step 79100/125000:
  Training Loss: 1.482
  Perplexity: 4.404
  Learning Rate: 5.95e-05
  GPU Memory Used: 12073MB

[30:23:16] Step 79150/125000:
  Training Loss: 1.521
  Perplexity: 4.578
  Learning Rate: 5.94e-05
  GPU Memory Used: 12030MB

[30:24:25] Step 79200/125000:
  Training Loss: 1.471
  Perplexity: 4.355
  Learning Rate: 5.92e-05
  GPU Memory Used: 11948MB

[30:25:32] Step 79250/125000:
  Training Loss: 1.533
  Perplexity: 4.634
  Learning Rate: 5.91e-05
  GPU Memory Used: 11856MB

[30:26:42] Step 79300/125000:
  Training Loss: 1.497
  Perplexity: 4.466
  Learning Rate: 5.90e-05
  GPU Memory Used: 12137MB

[30:27:54] Step 79350/125000:
  Training Loss: 1.490
  Perplexity: 4.438
  Learning Rate: 5.89e-05
  GPU Memory Used: 11988MB

[30:29:02] Step 79400/125000:
  Training Loss: 1.503
  Perplexity: 4.496
  Learning Rate: 5.88e-05
  GPU Memory Used: 11974MB

[30:30:11] Step 79450/125000:
  Training Loss: 1.505
  Perplexity: 4.504
  Learning Rate: 5.87e-05
  GPU Memory Used: 12184MB

[30:31:22] Step 79500/125000:
  Training Loss: 1.441
  Perplexity: 4.226
  Learning Rate: 5.86e-05
  GPU Memory Used: 12058MB


Evaluation Results:
  Validation Loss: 1.625
  Validation Perplexity: 5.077

Saving model checkpoint: proposal_model/checkpoint-79500


Epoch 160/250
==================================================
[30:32:32] Step 79550/125000:
  Training Loss: 1.568
  Perplexity: 4.795
  Learning Rate: 5.84e-05
  GPU Memory Used: 11962MB

[30:33:41] Step 79600/125000:
  Training Loss: 1.493
  Perplexity: 4.451
  Learning Rate: 5.83e-05
  GPU Memory Used: 11860MB

[30:34:50] Step 79650/125000:
  Training Loss: 1.474
  Perplexity: 4.365
  Learning Rate: 5.82e-05
  GPU Memory Used: 11875MB

[30:35:59] Step 79700/125000:
  Training Loss: 1.481
  Perplexity: 4.395
  Learning Rate: 5.81e-05
  GPU Memory Used: 11816MB

[30:37:08] Step 79750/125000:
  Training Loss: 1.555
  Perplexity: 4.737
  Learning Rate: 5.80e-05
  GPU Memory Used: 11847MB

[30:38:17] Step 79800/125000:
  Training Loss: 1.533
  Perplexity: 4.634
  Learning Rate: 5.79e-05
  GPU Memory Used: 12046MB

[30:39:26] Step 79850/125000:
  Training Loss: 1.477
  Perplexity: 4.378
  Learning Rate: 5.78e-05
  GPU Memory Used: 12046MB

[30:40:36] Step 79900/125000:
  Training Loss: 1.456
  Perplexity: 4.288
  Learning Rate: 5.76e-05
  GPU Memory Used: 11983MB

[30:41:45] Step 79950/125000:
  Training Loss: 1.343
  Perplexity: 3.831
  Learning Rate: 5.75e-05
  GPU Memory Used: 12052MB

[30:42:51] Step 80000/125000:
  Training Loss: 1.457
  Perplexity: 4.293
  Learning Rate: 5.74e-05
  GPU Memory Used: 11931MB


Evaluation Results:
  Validation Loss: 1.549
  Validation Perplexity: 4.708

Saving model checkpoint: proposal_model/checkpoint-80000


Epoch 161/250
==================================================
[30:43:58] Step 80050/125000:
  Training Loss: 1.477
  Perplexity: 4.378
  Learning Rate: 5.73e-05
  GPU Memory Used: 11988MB

[30:45:06] Step 80100/125000:
  Training Loss: 1.522
  Perplexity: 4.583
  Learning Rate: 5.72e-05
  GPU Memory Used: 11942MB

[30:46:13] Step 80150/125000:
  Training Loss: 1.446
  Perplexity: 4.247
  Learning Rate: 5.71e-05
  GPU Memory Used: 12084MB

[30:47:22] Step 80200/125000:
  Training Loss: 1.408
  Perplexity: 4.086
  Learning Rate: 5.70e-05
  GPU Memory Used: 11942MB

[30:48:33] Step 80250/125000:
  Training Loss: 1.482
  Perplexity: 4.402
  Learning Rate: 5.69e-05
  GPU Memory Used: 11933MB

[30:49:42] Step 80300/125000:
  Training Loss: 1.403
  Perplexity: 4.069
  Learning Rate: 5.67e-05
  GPU Memory Used: 12040MB

[30:50:51] Step 80350/125000:
  Training Loss: 1.442
  Perplexity: 4.228
  Learning Rate: 5.66e-05
  GPU Memory Used: 11824MB

[30:52:00] Step 80400/125000:
  Training Loss: 1.443
  Perplexity: 4.233
  Learning Rate: 5.65e-05
  GPU Memory Used: 12141MB

[30:53:09] Step 80450/125000:
  Training Loss: 1.461
  Perplexity: 4.312
  Learning Rate: 5.64e-05
  GPU Memory Used: 11907MB

[30:54:19] Step 80500/125000:
  Training Loss: 1.556
  Perplexity: 4.738
  Learning Rate: 5.63e-05
  GPU Memory Used: 12105MB


Evaluation Results:
  Validation Loss: 1.671
  Validation Perplexity: 5.318

Saving model checkpoint: proposal_model/checkpoint-80500


Epoch 162/250
==================================================
[30:55:28] Step 80550/125000:
  Training Loss: 1.524
  Perplexity: 4.591
  Learning Rate: 5.62e-05
  GPU Memory Used: 11876MB

[30:56:37] Step 80600/125000:
  Training Loss: 1.446
  Perplexity: 4.248
  Learning Rate: 5.61e-05
  GPU Memory Used: 12059MB

[30:57:45] Step 80650/125000:
  Training Loss: 1.486
  Perplexity: 4.419
  Learning Rate: 5.59e-05
  GPU Memory Used: 11976MB

[30:58:54] Step 80700/125000:
  Training Loss: 1.474
  Perplexity: 4.368
  Learning Rate: 5.58e-05
  GPU Memory Used: 12161MB

[31:00:03] Step 80750/125000:
  Training Loss: 1.597
  Perplexity: 4.938
  Learning Rate: 5.57e-05
  GPU Memory Used: 11996MB

[31:01:12] Step 80800/125000:
  Training Loss: 1.462
  Perplexity: 4.313
  Learning Rate: 5.56e-05
  GPU Memory Used: 11800MB

[31:02:21] Step 80850/125000:
  Training Loss: 1.431
  Perplexity: 4.181
  Learning Rate: 5.55e-05
  GPU Memory Used: 12051MB

[31:03:29] Step 80900/125000:
  Training Loss: 1.577
  Perplexity: 4.842
  Learning Rate: 5.54e-05
  GPU Memory Used: 12119MB

[31:04:38] Step 80950/125000:
  Training Loss: 1.493
  Perplexity: 4.449
  Learning Rate: 5.53e-05
  GPU Memory Used: 11829MB

[31:05:47] Step 81000/125000:
  Training Loss: 1.462
  Perplexity: 4.314
  Learning Rate: 5.52e-05
  GPU Memory Used: 11822MB


Evaluation Results:
  Validation Loss: 1.545
  Validation Perplexity: 4.687

Saving model checkpoint: proposal_model/checkpoint-81000


Epoch 163/250
==================================================
[31:06:55] Step 81050/125000:
  Training Loss: 1.548
  Perplexity: 4.700
  Learning Rate: 5.50e-05
  GPU Memory Used: 12111MB

[31:08:04] Step 81100/125000:
  Training Loss: 1.421
  Perplexity: 4.143
  Learning Rate: 5.49e-05
  GPU Memory Used: 11950MB

[31:09:12] Step 81150/125000:
  Training Loss: 1.412
  Perplexity: 4.105
  Learning Rate: 5.48e-05
  GPU Memory Used: 11810MB

[31:10:21] Step 81200/125000:
  Training Loss: 1.499
  Perplexity: 4.479
  Learning Rate: 5.47e-05
  GPU Memory Used: 11994MB

[31:11:30] Step 81250/125000:
  Training Loss: 1.456
  Perplexity: 4.288
  Learning Rate: 5.46e-05
  GPU Memory Used: 11810MB

[31:12:40] Step 81300/125000:
  Training Loss: 1.509
  Perplexity: 4.521
  Learning Rate: 5.45e-05
  GPU Memory Used: 12108MB

[31:13:50] Step 81350/125000:
  Training Loss: 1.451
  Perplexity: 4.268
  Learning Rate: 5.44e-05
  GPU Memory Used: 11806MB

[31:15:00] Step 81400/125000:
  Training Loss: 1.484
  Perplexity: 4.409
  Learning Rate: 5.43e-05
  GPU Memory Used: 11976MB

[31:16:10] Step 81450/125000:
  Training Loss: 1.460
  Perplexity: 4.306
  Learning Rate: 5.42e-05
  GPU Memory Used: 12044MB

[31:17:19] Step 81500/125000:
  Training Loss: 1.467
  Perplexity: 4.338
  Learning Rate: 5.40e-05
  GPU Memory Used: 12049MB


Evaluation Results:
  Validation Loss: 1.543
  Validation Perplexity: 4.680

Saving model checkpoint: proposal_model/checkpoint-81500


Epoch 164/250
==================================================
[31:18:28] Step 81550/125000:
  Training Loss: 1.462
  Perplexity: 4.315
  Learning Rate: 5.39e-05
  GPU Memory Used: 11989MB

[31:19:36] Step 81600/125000:
  Training Loss: 1.367
  Perplexity: 3.923
  Learning Rate: 5.38e-05
  GPU Memory Used: 11817MB

[31:20:47] Step 81650/125000:
  Training Loss: 1.481
  Perplexity: 4.397
  Learning Rate: 5.37e-05
  GPU Memory Used: 11881MB

[31:21:56] Step 81700/125000:
  Training Loss: 1.483
  Perplexity: 4.406
  Learning Rate: 5.36e-05
  GPU Memory Used: 12113MB

[31:23:04] Step 81750/125000:
  Training Loss: 1.453
  Perplexity: 4.276
  Learning Rate: 5.35e-05
  GPU Memory Used: 12125MB

[31:24:13] Step 81800/125000:
  Training Loss: 1.500
  Perplexity: 4.484
  Learning Rate: 5.34e-05
  GPU Memory Used: 11924MB

[31:25:24] Step 81850/125000:
  Training Loss: 1.472
  Perplexity: 4.358
  Learning Rate: 5.33e-05
  GPU Memory Used: 11836MB

[31:26:31] Step 81900/125000:
  Training Loss: 1.454
  Perplexity: 4.282
  Learning Rate: 5.32e-05
  GPU Memory Used: 12082MB

[31:27:41] Step 81950/125000:
  Training Loss: 1.387
  Perplexity: 4.003
  Learning Rate: 5.30e-05
  GPU Memory Used: 12046MB

[31:28:49] Step 82000/125000:
  Training Loss: 1.391
  Perplexity: 4.020
  Learning Rate: 5.29e-05
  GPU Memory Used: 11997MB


Evaluation Results:
  Validation Loss: 1.521
  Validation Perplexity: 4.579

Saving model checkpoint: proposal_model/checkpoint-82000


Epoch 165/250
==================================================
[31:29:59] Step 82050/125000:
  Training Loss: 1.522
  Perplexity: 4.581
  Learning Rate: 5.28e-05
  GPU Memory Used: 11957MB

[31:31:09] Step 82100/125000:
  Training Loss: 1.472
  Perplexity: 4.358
  Learning Rate: 5.27e-05
  GPU Memory Used: 12004MB

[31:32:20] Step 82150/125000:
  Training Loss: 1.593
  Perplexity: 4.916
  Learning Rate: 5.26e-05
  GPU Memory Used: 11944MB

[31:33:28] Step 82200/125000:
  Training Loss: 1.499
  Perplexity: 4.476
  Learning Rate: 5.25e-05
  GPU Memory Used: 11829MB

[31:34:38] Step 82250/125000:
  Training Loss: 1.430
  Perplexity: 4.179
  Learning Rate: 5.24e-05
  GPU Memory Used: 11932MB

[31:35:47] Step 82300/125000:
  Training Loss: 1.397
  Perplexity: 4.042
  Learning Rate: 5.23e-05
  GPU Memory Used: 11902MB

[31:36:54] Step 82350/125000:
  Training Loss: 1.406
  Perplexity: 4.080
  Learning Rate: 5.22e-05
  GPU Memory Used: 12139MB

[31:38:03] Step 82400/125000:
  Training Loss: 1.425
  Perplexity: 4.158
  Learning Rate: 5.20e-05
  GPU Memory Used: 11861MB

[31:39:11] Step 82450/125000:
  Training Loss: 1.504
  Perplexity: 4.501
  Learning Rate: 5.19e-05
  GPU Memory Used: 12186MB

[31:40:22] Step 82500/125000:
  Training Loss: 1.449
  Perplexity: 4.257
  Learning Rate: 5.18e-05
  GPU Memory Used: 12168MB


Evaluation Results:
  Validation Loss: 1.594
  Validation Perplexity: 4.923

Saving model checkpoint: proposal_model/checkpoint-82500


Epoch 166/250
==================================================
[31:41:32] Step 82550/125000:
  Training Loss: 1.434
  Perplexity: 4.197
  Learning Rate: 5.17e-05
  GPU Memory Used: 11808MB

[31:42:42] Step 82600/125000:
  Training Loss: 1.471
  Perplexity: 4.354
  Learning Rate: 5.16e-05
  GPU Memory Used: 12039MB

[31:43:52] Step 82650/125000:
  Training Loss: 1.466
  Perplexity: 4.332
  Learning Rate: 5.15e-05
  GPU Memory Used: 11815MB

[31:45:01] Step 82700/125000:
  Training Loss: 1.563
  Perplexity: 4.771
  Learning Rate: 5.14e-05
  GPU Memory Used: 12118MB

[31:46:11] Step 82750/125000:
  Training Loss: 1.424
  Perplexity: 4.155
  Learning Rate: 5.13e-05
  GPU Memory Used: 12177MB

[31:47:22] Step 82800/125000:
  Training Loss: 1.444
  Perplexity: 4.237
  Learning Rate: 5.12e-05
  GPU Memory Used: 11986MB

[31:48:31] Step 82850/125000:
  Training Loss: 1.502
  Perplexity: 4.492
  Learning Rate: 5.11e-05
  GPU Memory Used: 12137MB

[31:49:39] Step 82900/125000:
  Training Loss: 1.467
  Perplexity: 4.338
  Learning Rate: 5.09e-05
  GPU Memory Used: 11959MB

[31:50:50] Step 82950/125000:
  Training Loss: 1.512
  Perplexity: 4.535
  Learning Rate: 5.08e-05
  GPU Memory Used: 12069MB

[31:52:00] Step 83000/125000:
  Training Loss: 1.470
  Perplexity: 4.349
  Learning Rate: 5.07e-05
  GPU Memory Used: 12001MB


Evaluation Results:
  Validation Loss: 1.650
  Validation Perplexity: 5.209

Saving model checkpoint: proposal_model/checkpoint-83000


Epoch 167/250
==================================================
[31:53:10] Step 83050/125000:
  Training Loss: 1.470
  Perplexity: 4.351
  Learning Rate: 5.06e-05
  GPU Memory Used: 12183MB

[31:54:20] Step 83100/125000:
  Training Loss: 1.470
  Perplexity: 4.351
  Learning Rate: 5.05e-05
  GPU Memory Used: 11875MB

[31:55:30] Step 83150/125000:
  Training Loss: 1.561
  Perplexity: 4.762
  Learning Rate: 5.04e-05
  GPU Memory Used: 11966MB

[31:56:41] Step 83200/125000:
  Training Loss: 1.416
  Perplexity: 4.122
  Learning Rate: 5.03e-05
  GPU Memory Used: 11997MB

[31:57:48] Step 83250/125000:
  Training Loss: 1.421
  Perplexity: 4.141
  Learning Rate: 5.02e-05
  GPU Memory Used: 11890MB

[31:58:57] Step 83300/125000:
  Training Loss: 1.473
  Perplexity: 4.363
  Learning Rate: 5.01e-05
  GPU Memory Used: 12048MB

[32:00:07] Step 83350/125000:
  Training Loss: 1.415
  Perplexity: 4.115
  Learning Rate: 5.00e-05
  GPU Memory Used: 11920MB

[32:01:16] Step 83400/125000:
  Training Loss: 1.360
  Perplexity: 3.898
  Learning Rate: 4.99e-05
  GPU Memory Used: 11817MB

[32:02:26] Step 83450/125000:
  Training Loss: 1.507
  Perplexity: 4.515
  Learning Rate: 4.97e-05
  GPU Memory Used: 12074MB

[32:03:37] Step 83500/125000:
  Training Loss: 1.518
  Perplexity: 4.562
  Learning Rate: 4.96e-05
  GPU Memory Used: 12128MB


Evaluation Results:
  Validation Loss: 1.607
  Validation Perplexity: 4.990

Saving model checkpoint: proposal_model/checkpoint-83500


Epoch 168/250
==================================================
[32:04:43] Step 83550/125000:
  Training Loss: 1.514
  Perplexity: 4.547
  Learning Rate: 4.95e-05
  GPU Memory Used: 11812MB

[32:05:54] Step 83600/125000:
  Training Loss: 1.468
  Perplexity: 4.339
  Learning Rate: 4.94e-05
  GPU Memory Used: 11905MB

[32:07:04] Step 83650/125000:
  Training Loss: 1.544
  Perplexity: 4.683
  Learning Rate: 4.93e-05
  GPU Memory Used: 12133MB

[32:08:13] Step 83700/125000:
  Training Loss: 1.426
  Perplexity: 4.161
  Learning Rate: 4.92e-05
  GPU Memory Used: 11804MB

[32:09:22] Step 83750/125000:
  Training Loss: 1.469
  Perplexity: 4.345
  Learning Rate: 4.91e-05
  GPU Memory Used: 12146MB

[32:10:32] Step 83800/125000:
  Training Loss: 1.427
  Perplexity: 4.165
  Learning Rate: 4.90e-05
  GPU Memory Used: 12111MB

[32:11:39] Step 83850/125000:
  Training Loss: 1.471
  Perplexity: 4.356
  Learning Rate: 4.89e-05
  GPU Memory Used: 11904MB

[32:12:47] Step 83900/125000:
  Training Loss: 1.420
  Perplexity: 4.137
  Learning Rate: 4.88e-05
  GPU Memory Used: 12151MB

[32:13:56] Step 83950/125000:
  Training Loss: 1.464
  Perplexity: 4.324
  Learning Rate: 4.87e-05
  GPU Memory Used: 11933MB

[32:15:05] Step 84000/125000:
  Training Loss: 1.521
  Perplexity: 4.577
  Learning Rate: 4.86e-05
  GPU Memory Used: 12048MB


Evaluation Results:
  Validation Loss: 1.622
  Validation Perplexity: 5.061

Saving model checkpoint: proposal_model/checkpoint-84000


Epoch 169/250
==================================================
[32:16:14] Step 84050/125000:
  Training Loss: 1.541
  Perplexity: 4.670
  Learning Rate: 4.84e-05
  GPU Memory Used: 11959MB

[32:17:21] Step 84100/125000:
  Training Loss: 1.454
  Perplexity: 4.279
  Learning Rate: 4.83e-05
  GPU Memory Used: 11903MB

[32:18:30] Step 84150/125000:
  Training Loss: 1.453
  Perplexity: 4.278
  Learning Rate: 4.82e-05
  GPU Memory Used: 12001MB

[32:19:40] Step 84200/125000:
  Training Loss: 1.399
  Perplexity: 4.050
  Learning Rate: 4.81e-05
  GPU Memory Used: 12004MB

[32:20:47] Step 84250/125000:
  Training Loss: 1.477
  Perplexity: 4.380
  Learning Rate: 4.80e-05
  GPU Memory Used: 11904MB

[32:21:54] Step 84300/125000:
  Training Loss: 1.431
  Perplexity: 4.182
  Learning Rate: 4.79e-05
  GPU Memory Used: 12033MB

[32:23:04] Step 84350/125000:
  Training Loss: 1.345
  Perplexity: 3.837
  Learning Rate: 4.78e-05
  GPU Memory Used: 12084MB

[32:24:13] Step 84400/125000:
  Training Loss: 1.421
  Perplexity: 4.142
  Learning Rate: 4.77e-05
  GPU Memory Used: 12038MB

[32:25:24] Step 84450/125000:
  Training Loss: 1.406
  Perplexity: 4.079
  Learning Rate: 4.76e-05
  GPU Memory Used: 12086MB

[32:26:32] Step 84500/125000:
  Training Loss: 1.438
  Perplexity: 4.213
  Learning Rate: 4.75e-05
  GPU Memory Used: 11835MB


Evaluation Results:
  Validation Loss: 1.531
  Validation Perplexity: 4.622

Saving model checkpoint: proposal_model/checkpoint-84500


Epoch 170/250
==================================================
[32:27:42] Step 84550/125000:
  Training Loss: 1.525
  Perplexity: 4.595
  Learning Rate: 4.74e-05
  GPU Memory Used: 11908MB

[32:28:50] Step 84600/125000:
  Training Loss: 1.330
  Perplexity: 3.782
  Learning Rate: 4.73e-05
  GPU Memory Used: 12183MB

[32:30:00] Step 84650/125000:
  Training Loss: 1.521
  Perplexity: 4.575
  Learning Rate: 4.72e-05
  GPU Memory Used: 11835MB

[32:31:10] Step 84700/125000:
  Training Loss: 1.455
  Perplexity: 4.285
  Learning Rate: 4.71e-05
  GPU Memory Used: 12087MB

[32:32:18] Step 84750/125000:
  Training Loss: 1.458
  Perplexity: 4.296
  Learning Rate: 4.69e-05
  GPU Memory Used: 11942MB

[32:33:26] Step 84800/125000:
  Training Loss: 1.429
  Perplexity: 4.176
  Learning Rate: 4.68e-05
  GPU Memory Used: 12111MB

[32:34:35] Step 84850/125000:
  Training Loss: 1.470
  Perplexity: 4.350
  Learning Rate: 4.67e-05
  GPU Memory Used: 11844MB

[32:35:45] Step 84900/125000:
  Training Loss: 1.447
  Perplexity: 4.249
  Learning Rate: 4.66e-05
  GPU Memory Used: 12183MB

[32:36:53] Step 84950/125000:
  Training Loss: 1.414
  Perplexity: 4.113
  Learning Rate: 4.65e-05
  GPU Memory Used: 11909MB

[32:38:01] Step 85000/125000:
  Training Loss: 1.427
  Perplexity: 4.165
  Learning Rate: 4.64e-05
  GPU Memory Used: 12172MB


Evaluation Results:
  Validation Loss: 1.556
  Validation Perplexity: 4.739

Saving model checkpoint: proposal_model/checkpoint-85000


Epoch 171/250
==================================================
[32:39:10] Step 85050/125000:
  Training Loss: 1.454
  Perplexity: 4.282
  Learning Rate: 4.63e-05
  GPU Memory Used: 11920MB

[32:40:19] Step 85100/125000:
  Training Loss: 1.472
  Perplexity: 4.358
  Learning Rate: 4.62e-05
  GPU Memory Used: 12194MB

[32:41:28] Step 85150/125000:
  Training Loss: 1.432
  Perplexity: 4.188
  Learning Rate: 4.61e-05
  GPU Memory Used: 11939MB

[32:42:38] Step 85200/125000:
  Training Loss: 1.477
  Perplexity: 4.380
  Learning Rate: 4.60e-05
  GPU Memory Used: 12129MB

[32:43:46] Step 85250/125000:
  Training Loss: 1.441
  Perplexity: 4.226
  Learning Rate: 4.59e-05
  GPU Memory Used: 12180MB

[32:44:54] Step 85300/125000:
  Training Loss: 1.524
  Perplexity: 4.588
  Learning Rate: 4.58e-05
  GPU Memory Used: 12134MB

[32:46:03] Step 85350/125000:
  Training Loss: 1.418
  Perplexity: 4.130
  Learning Rate: 4.57e-05
  GPU Memory Used: 12173MB

[32:47:11] Step 85400/125000:
  Training Loss: 1.432
  Perplexity: 4.187
  Learning Rate: 4.56e-05
  GPU Memory Used: 12113MB

[32:48:19] Step 85450/125000:
  Training Loss: 1.500
  Perplexity: 4.480
  Learning Rate: 4.55e-05
  GPU Memory Used: 12046MB

[32:49:28] Step 85500/125000:
  Training Loss: 1.456
  Perplexity: 4.290
  Learning Rate: 4.54e-05
  GPU Memory Used: 11808MB


Evaluation Results:
  Validation Loss: 1.592
  Validation Perplexity: 4.914

Saving model checkpoint: proposal_model/checkpoint-85500


Epoch 172/250
==================================================
[32:50:37] Step 85550/125000:
  Training Loss: 1.422
  Perplexity: 4.146
  Learning Rate: 4.53e-05
  GPU Memory Used: 12087MB

[32:51:48] Step 85600/125000:
  Training Loss: 1.398
  Perplexity: 4.049
  Learning Rate: 4.52e-05
  GPU Memory Used: 12005MB

[32:52:56] Step 85650/125000:
  Training Loss: 1.455
  Perplexity: 4.287
  Learning Rate: 4.50e-05
  GPU Memory Used: 12159MB

[32:54:06] Step 85700/125000:
  Training Loss: 1.446
  Perplexity: 4.246
  Learning Rate: 4.49e-05
  GPU Memory Used: 11859MB

[32:55:13] Step 85750/125000:
  Training Loss: 1.431
  Perplexity: 4.183
  Learning Rate: 4.48e-05
  GPU Memory Used: 11992MB

[32:56:21] Step 85800/125000:
  Training Loss: 1.518
  Perplexity: 4.562
  Learning Rate: 4.47e-05
  GPU Memory Used: 11973MB

[32:57:32] Step 85850/125000:
  Training Loss: 1.422
  Perplexity: 4.144
  Learning Rate: 4.46e-05
  GPU Memory Used: 12146MB

[32:58:42] Step 85900/125000:
  Training Loss: 1.445
  Perplexity: 4.243
  Learning Rate: 4.45e-05
  GPU Memory Used: 11868MB

[32:59:50] Step 85950/125000:
  Training Loss: 1.507
  Perplexity: 4.515
  Learning Rate: 4.44e-05
  GPU Memory Used: 12055MB

[33:00:59] Step 86000/125000:
  Training Loss: 1.459
  Perplexity: 4.300
  Learning Rate: 4.43e-05
  GPU Memory Used: 12131MB


Evaluation Results:
  Validation Loss: 1.605
  Validation Perplexity: 4.979

Saving model checkpoint: proposal_model/checkpoint-86000


Epoch 173/250
==================================================
[33:02:07] Step 86050/125000:
  Training Loss: 1.476
  Perplexity: 4.376
  Learning Rate: 4.42e-05
  GPU Memory Used: 12195MB

[33:03:14] Step 86100/125000:
  Training Loss: 1.488
  Perplexity: 4.429
  Learning Rate: 4.41e-05
  GPU Memory Used: 11924MB

[33:04:23] Step 86150/125000:
  Training Loss: 1.466
  Perplexity: 4.331
  Learning Rate: 4.40e-05
  GPU Memory Used: 11929MB

[33:05:32] Step 86200/125000:
  Training Loss: 1.469
  Perplexity: 4.344
  Learning Rate: 4.39e-05
  GPU Memory Used: 12197MB

[33:06:43] Step 86250/125000:
  Training Loss: 1.494
  Perplexity: 4.454
  Learning Rate: 4.38e-05
  GPU Memory Used: 11811MB

[33:07:51] Step 86300/125000:
  Training Loss: 1.431
  Perplexity: 4.181
  Learning Rate: 4.37e-05
  GPU Memory Used: 12118MB

[33:09:01] Step 86350/125000:
  Training Loss: 1.428
  Perplexity: 4.170
  Learning Rate: 4.36e-05
  GPU Memory Used: 12111MB

[33:10:09] Step 86400/125000:
  Training Loss: 1.403
  Perplexity: 4.069
  Learning Rate: 4.35e-05
  GPU Memory Used: 12140MB

[33:11:20] Step 86450/125000:
  Training Loss: 1.495
  Perplexity: 4.458
  Learning Rate: 4.34e-05
  GPU Memory Used: 12013MB

[33:12:31] Step 86500/125000:
  Training Loss: 1.436
  Perplexity: 4.203
  Learning Rate: 4.33e-05
  GPU Memory Used: 12043MB


Evaluation Results:
  Validation Loss: 1.592
  Validation Perplexity: 4.912

Saving model checkpoint: proposal_model/checkpoint-86500


Epoch 174/250
==================================================
[33:13:39] Step 86550/125000:
  Training Loss: 1.397
  Perplexity: 4.042
  Learning Rate: 4.32e-05
  GPU Memory Used: 11984MB

[33:14:50] Step 86600/125000:
  Training Loss: 1.437
  Perplexity: 4.208
  Learning Rate: 4.31e-05
  GPU Memory Used: 12110MB

[33:16:00] Step 86650/125000:
  Training Loss: 1.442
  Perplexity: 4.229
  Learning Rate: 4.30e-05
  GPU Memory Used: 11817MB

[33:17:06] Step 86700/125000:
  Training Loss: 1.419
  Perplexity: 4.131
  Learning Rate: 4.29e-05
  GPU Memory Used: 11820MB

[33:18:15] Step 86750/125000:
  Training Loss: 1.510
  Perplexity: 4.529
  Learning Rate: 4.28e-05
  GPU Memory Used: 12192MB

[33:19:24] Step 86800/125000:
  Training Loss: 1.462
  Perplexity: 4.313
  Learning Rate: 4.27e-05
  GPU Memory Used: 12058MB

[33:20:34] Step 86850/125000:
  Training Loss: 1.379
  Perplexity: 3.972
  Learning Rate: 4.26e-05
  GPU Memory Used: 11802MB

[33:21:43] Step 86900/125000:
  Training Loss: 1.409
  Perplexity: 4.091
  Learning Rate: 4.24e-05
  GPU Memory Used: 11960MB

[33:22:51] Step 86950/125000:
  Training Loss: 1.404
  Perplexity: 4.071
  Learning Rate: 4.23e-05
  GPU Memory Used: 12200MB

[33:24:00] Step 87000/125000:
  Training Loss: 1.532
  Perplexity: 4.626
  Learning Rate: 4.22e-05
  GPU Memory Used: 12169MB


Evaluation Results:
  Validation Loss: 1.591
  Validation Perplexity: 4.908

Saving model checkpoint: proposal_model/checkpoint-87000


Epoch 175/250
==================================================
[33:25:10] Step 87050/125000:
  Training Loss: 1.396
  Perplexity: 4.038
  Learning Rate: 4.21e-05
  GPU Memory Used: 12108MB

[33:26:18] Step 87100/125000:
  Training Loss: 1.304
  Perplexity: 3.683
  Learning Rate: 4.20e-05
  GPU Memory Used: 11935MB

[33:27:26] Step 87150/125000:
  Training Loss: 1.401
  Perplexity: 4.061
  Learning Rate: 4.19e-05
  GPU Memory Used: 11994MB

[33:28:33] Step 87200/125000:
  Training Loss: 1.472
  Perplexity: 4.356
  Learning Rate: 4.18e-05
  GPU Memory Used: 12014MB

[33:29:45] Step 87250/125000:
  Training Loss: 1.488
  Perplexity: 4.426
  Learning Rate: 4.17e-05
  GPU Memory Used: 12172MB

[33:30:54] Step 87300/125000:
  Training Loss: 1.481
  Perplexity: 4.399
  Learning Rate: 4.16e-05
  GPU Memory Used: 12096MB

[33:32:05] Step 87350/125000:
  Training Loss: 1.480
  Perplexity: 4.392
  Learning Rate: 4.15e-05
  GPU Memory Used: 11918MB

[33:33:12] Step 87400/125000:
  Training Loss: 1.420
  Perplexity: 4.136
  Learning Rate: 4.14e-05
  GPU Memory Used: 12200MB

[33:34:21] Step 87450/125000:
  Training Loss: 1.461
  Perplexity: 4.312
  Learning Rate: 4.13e-05
  GPU Memory Used: 11824MB

[33:35:28] Step 87500/125000:
  Training Loss: 1.440
  Perplexity: 4.220
  Learning Rate: 4.12e-05
  GPU Memory Used: 12106MB


Evaluation Results:
  Validation Loss: 1.561
  Validation Perplexity: 4.764

Saving model checkpoint: proposal_model/checkpoint-87500


Epoch 176/250
==================================================
[33:36:37] Step 87550/125000:
  Training Loss: 1.475
  Perplexity: 4.369
  Learning Rate: 4.11e-05
  GPU Memory Used: 12179MB

[33:37:49] Step 87600/125000:
  Training Loss: 1.490
  Perplexity: 4.438
  Learning Rate: 4.10e-05
  GPU Memory Used: 11824MB

[33:38:59] Step 87650/125000:
  Training Loss: 1.364
  Perplexity: 3.912
  Learning Rate: 4.09e-05
  GPU Memory Used: 11936MB

[33:40:09] Step 87700/125000:
  Training Loss: 1.450
  Perplexity: 4.264
  Learning Rate: 4.08e-05
  GPU Memory Used: 12070MB

[33:41:18] Step 87750/125000:
  Training Loss: 1.431
  Perplexity: 4.184
  Learning Rate: 4.07e-05
  GPU Memory Used: 12177MB

[33:42:25] Step 87800/125000:
  Training Loss: 1.503
  Perplexity: 4.496
  Learning Rate: 4.06e-05
  GPU Memory Used: 12161MB

[33:43:33] Step 87850/125000:
  Training Loss: 1.435
  Perplexity: 4.200
  Learning Rate: 4.05e-05
  GPU Memory Used: 11990MB

[33:44:42] Step 87900/125000:
  Training Loss: 1.472
  Perplexity: 4.357
  Learning Rate: 4.04e-05
  GPU Memory Used: 11906MB

[33:45:52] Step 87950/125000:
  Training Loss: 1.455
  Perplexity: 4.286
  Learning Rate: 4.03e-05
  GPU Memory Used: 11945MB

[33:47:02] Step 88000/125000:
  Training Loss: 1.459
  Perplexity: 4.303
  Learning Rate: 4.02e-05
  GPU Memory Used: 11837MB


Evaluation Results:
  Validation Loss: 1.591
  Validation Perplexity: 4.907

Saving model checkpoint: proposal_model/checkpoint-88000


Epoch 177/250
==================================================
[33:48:11] Step 88050/125000:
  Training Loss: 1.429
  Perplexity: 4.174
  Learning Rate: 4.01e-05
  GPU Memory Used: 12027MB

[33:49:20] Step 88100/125000:
  Training Loss: 1.423
  Perplexity: 4.151
  Learning Rate: 4.00e-05
  GPU Memory Used: 12143MB

[33:50:27] Step 88150/125000:
  Training Loss: 1.503
  Perplexity: 4.494
  Learning Rate: 3.99e-05
  GPU Memory Used: 12074MB

[33:51:34] Step 88200/125000:
  Training Loss: 1.464
  Perplexity: 4.321
  Learning Rate: 3.98e-05
  GPU Memory Used: 11898MB

[33:52:43] Step 88250/125000:
  Training Loss: 1.420
  Perplexity: 4.139
  Learning Rate: 3.97e-05
  GPU Memory Used: 12017MB

[33:53:53] Step 88300/125000:
  Training Loss: 1.468
  Perplexity: 4.342
  Learning Rate: 3.96e-05
  GPU Memory Used: 11926MB

[33:55:02] Step 88350/125000:
  Training Loss: 1.312
  Perplexity: 3.712
  Learning Rate: 3.95e-05
  GPU Memory Used: 11873MB

[33:56:12] Step 88400/125000:
  Training Loss: 1.401
  Perplexity: 4.060
  Learning Rate: 3.94e-05
  GPU Memory Used: 12136MB

[33:57:21] Step 88450/125000:
  Training Loss: 1.422
  Perplexity: 4.147
  Learning Rate: 3.93e-05
  GPU Memory Used: 11999MB

[33:58:30] Step 88500/125000:
  Training Loss: 1.378
  Perplexity: 3.966
  Learning Rate: 3.92e-05
  GPU Memory Used: 11973MB


Evaluation Results:
  Validation Loss: 1.575
  Validation Perplexity: 4.829

Saving model checkpoint: proposal_model/checkpoint-88500


Epoch 178/250
==================================================
[33:59:40] Step 88550/125000:
  Training Loss: 1.417
  Perplexity: 4.126
  Learning Rate: 3.91e-05
  GPU Memory Used: 12159MB

[34:00:51] Step 88600/125000:
  Training Loss: 1.494
  Perplexity: 4.454
  Learning Rate: 3.90e-05
  GPU Memory Used: 12107MB

[34:02:00] Step 88650/125000:
  Training Loss: 1.457
  Perplexity: 4.295
  Learning Rate: 3.89e-05
  GPU Memory Used: 12100MB

[34:03:07] Step 88700/125000:
  Training Loss: 1.515
  Perplexity: 4.548
  Learning Rate: 3.88e-05
  GPU Memory Used: 11956MB

[34:04:17] Step 88750/125000:
  Training Loss: 1.460
  Perplexity: 4.305
  Learning Rate: 3.87e-05
  GPU Memory Used: 11830MB

[34:05:26] Step 88800/125000:
  Training Loss: 1.485
  Perplexity: 4.417
  Learning Rate: 3.86e-05
  GPU Memory Used: 11966MB

[34:06:34] Step 88850/125000:
  Training Loss: 1.371
  Perplexity: 3.940
  Learning Rate: 3.85e-05
  GPU Memory Used: 11857MB

[34:07:43] Step 88900/125000:
  Training Loss: 1.382
  Perplexity: 3.982
  Learning Rate: 3.84e-05
  GPU Memory Used: 12198MB

[34:08:52] Step 88950/125000:
  Training Loss: 1.419
  Perplexity: 4.134
  Learning Rate: 3.83e-05
  GPU Memory Used: 11820MB

[34:10:00] Step 89000/125000:
  Training Loss: 1.430
  Perplexity: 4.177
  Learning Rate: 3.82e-05
  GPU Memory Used: 12082MB


Evaluation Results:
  Validation Loss: 1.517
  Validation Perplexity: 4.558

Saving model checkpoint: proposal_model/checkpoint-89000


Epoch 179/250
==================================================
[34:11:09] Step 89050/125000:
  Training Loss: 1.400
  Perplexity: 4.056
  Learning Rate: 3.81e-05
  GPU Memory Used: 12000MB

[34:12:19] Step 89100/125000:
  Training Loss: 1.462
  Perplexity: 4.313
  Learning Rate: 3.80e-05
  GPU Memory Used: 11804MB

[34:13:28] Step 89150/125000:
  Training Loss: 1.532
  Perplexity: 4.628
  Learning Rate: 3.79e-05
  GPU Memory Used: 11884MB

[34:14:34] Step 89200/125000:
  Training Loss: 1.436
  Perplexity: 4.204
  Learning Rate: 3.78e-05
  GPU Memory Used: 11961MB

[34:15:43] Step 89250/125000:
  Training Loss: 1.398
  Perplexity: 4.048
  Learning Rate: 3.77e-05
  GPU Memory Used: 11814MB

[34:16:49] Step 89300/125000:
  Training Loss: 1.451
  Perplexity: 4.269
  Learning Rate: 3.76e-05
  GPU Memory Used: 11950MB

[34:17:59] Step 89350/125000:
  Training Loss: 1.456
  Perplexity: 4.290
  Learning Rate: 3.75e-05
  GPU Memory Used: 12156MB

[34:19:09] Step 89400/125000:
  Training Loss: 1.466
  Perplexity: 4.331
  Learning Rate: 3.74e-05
  GPU Memory Used: 11927MB

[34:20:18] Step 89450/125000:
  Training Loss: 1.404
  Perplexity: 4.071
  Learning Rate: 3.73e-05
  GPU Memory Used: 12055MB

[34:21:30] Step 89500/125000:
  Training Loss: 1.474
  Perplexity: 4.367
  Learning Rate: 3.72e-05
  GPU Memory Used: 11901MB


Evaluation Results:
  Validation Loss: 1.557
  Validation Perplexity: 4.745

Saving model checkpoint: proposal_model/checkpoint-89500


Epoch 180/250
==================================================
[34:22:40] Step 89550/125000:
  Training Loss: 1.416
  Perplexity: 4.120
  Learning Rate: 3.71e-05
  GPU Memory Used: 12040MB

[34:23:50] Step 89600/125000:
  Training Loss: 1.381
  Perplexity: 3.977
  Learning Rate: 3.70e-05
  GPU Memory Used: 11964MB

[34:25:00] Step 89650/125000:
  Training Loss: 1.435
  Perplexity: 4.202
  Learning Rate: 3.69e-05
  GPU Memory Used: 11876MB

[34:26:09] Step 89700/125000:
  Training Loss: 1.542
  Perplexity: 4.676
  Learning Rate: 3.68e-05
  GPU Memory Used: 11966MB

[34:27:18] Step 89750/125000:
  Training Loss: 1.400
  Perplexity: 4.054
  Learning Rate: 3.67e-05
  GPU Memory Used: 11845MB

[34:28:25] Step 89800/125000:
  Training Loss: 1.435
  Perplexity: 4.198
  Learning Rate: 3.66e-05
  GPU Memory Used: 11895MB

[34:29:34] Step 89850/125000:
  Training Loss: 1.438
  Perplexity: 4.213
  Learning Rate: 3.65e-05
  GPU Memory Used: 12167MB

[34:30:42] Step 89900/125000:
  Training Loss: 1.488
  Perplexity: 4.428
  Learning Rate: 3.65e-05
  GPU Memory Used: 11814MB

[34:31:51] Step 89950/125000:
  Training Loss: 1.430
  Perplexity: 4.179
  Learning Rate: 3.64e-05
  GPU Memory Used: 11819MB

[34:33:00] Step 90000/125000:
  Training Loss: 1.439
  Perplexity: 4.218
  Learning Rate: 3.63e-05
  GPU Memory Used: 11972MB


Evaluation Results:
  Validation Loss: 1.615
  Validation Perplexity: 5.029

Saving model checkpoint: proposal_model/checkpoint-90000


Epoch 181/250
==================================================
[34:34:09] Step 90050/125000:
  Training Loss: 1.380
  Perplexity: 3.974
  Learning Rate: 3.62e-05
  GPU Memory Used: 12083MB

[34:35:19] Step 90100/125000:
  Training Loss: 1.430
  Perplexity: 4.179
  Learning Rate: 3.61e-05
  GPU Memory Used: 11914MB

[34:36:29] Step 90150/125000:
  Training Loss: 1.460
  Perplexity: 4.306
  Learning Rate: 3.60e-05
  GPU Memory Used: 11809MB

[34:37:38] Step 90200/125000:
  Training Loss: 1.405
  Perplexity: 4.074
  Learning Rate: 3.59e-05
  GPU Memory Used: 11955MB

[34:38:50] Step 90250/125000:
  Training Loss: 1.464
  Perplexity: 4.324
  Learning Rate: 3.58e-05
  GPU Memory Used: 12049MB

[34:39:59] Step 90300/125000:
  Training Loss: 1.429
  Perplexity: 4.174
  Learning Rate: 3.57e-05
  GPU Memory Used: 12190MB

[34:41:07] Step 90350/125000:
  Training Loss: 1.489
  Perplexity: 4.434
  Learning Rate: 3.56e-05
  GPU Memory Used: 11864MB

[34:42:17] Step 90400/125000:
  Training Loss: 1.332
  Perplexity: 3.790
  Learning Rate: 3.55e-05
  GPU Memory Used: 12024MB

[34:43:27] Step 90450/125000:
  Training Loss: 1.433
  Perplexity: 4.192
  Learning Rate: 3.54e-05
  GPU Memory Used: 11953MB

[34:44:35] Step 90500/125000:
  Training Loss: 1.398
  Perplexity: 4.048
  Learning Rate: 3.53e-05
  GPU Memory Used: 11943MB


Evaluation Results:
  Validation Loss: 1.534
  Validation Perplexity: 4.635

Saving model checkpoint: proposal_model/checkpoint-90500


Epoch 182/250
==================================================
[34:45:46] Step 90550/125000:
  Training Loss: 1.381
  Perplexity: 3.977
  Learning Rate: 3.52e-05
  GPU Memory Used: 11836MB

[34:46:52] Step 90600/125000:
  Training Loss: 1.417
  Perplexity: 4.124
  Learning Rate: 3.51e-05
  GPU Memory Used: 11913MB

[34:48:01] Step 90650/125000:
  Training Loss: 1.334
  Perplexity: 3.797
  Learning Rate: 3.50e-05
  GPU Memory Used: 11945MB

[34:49:11] Step 90700/125000:
  Training Loss: 1.450
  Perplexity: 4.262
  Learning Rate: 3.49e-05
  GPU Memory Used: 12096MB

[34:50:21] Step 90750/125000:
  Training Loss: 1.435
  Perplexity: 4.199
  Learning Rate: 3.48e-05
  GPU Memory Used: 11950MB

[34:51:30] Step 90800/125000:
  Training Loss: 1.416
  Perplexity: 4.121
  Learning Rate: 3.47e-05
  GPU Memory Used: 12180MB

[34:52:42] Step 90850/125000:
  Training Loss: 1.390
  Perplexity: 4.016
  Learning Rate: 3.46e-05
  GPU Memory Used: 11909MB

[34:53:51] Step 90900/125000:
  Training Loss: 1.450
  Perplexity: 4.265
  Learning Rate: 3.45e-05
  GPU Memory Used: 12085MB

[34:55:00] Step 90950/125000:
  Training Loss: 1.478
  Perplexity: 4.386
  Learning Rate: 3.44e-05
  GPU Memory Used: 12113MB

[34:56:10] Step 91000/125000:
  Training Loss: 1.403
  Perplexity: 4.069
  Learning Rate: 3.43e-05
  GPU Memory Used: 12037MB


Evaluation Results:
  Validation Loss: 1.542
  Validation Perplexity: 4.673

Saving model checkpoint: proposal_model/checkpoint-91000


Epoch 183/250
==================================================
[34:57:21] Step 91050/125000:
  Training Loss: 1.458
  Perplexity: 4.296
  Learning Rate: 3.42e-05
  GPU Memory Used: 11912MB

[34:58:32] Step 91100/125000:
  Training Loss: 1.477
  Perplexity: 4.379
  Learning Rate: 3.42e-05
  GPU Memory Used: 12115MB

[34:59:41] Step 91150/125000:
  Training Loss: 1.427
  Perplexity: 4.167
  Learning Rate: 3.41e-05
  GPU Memory Used: 11864MB

[35:00:50] Step 91200/125000:
  Training Loss: 1.496
  Perplexity: 4.464
  Learning Rate: 3.40e-05
  GPU Memory Used: 12006MB

[35:01:59] Step 91250/125000:
  Training Loss: 1.430
  Perplexity: 4.179
  Learning Rate: 3.39e-05
  GPU Memory Used: 11945MB

[35:03:09] Step 91300/125000:
  Training Loss: 1.533
  Perplexity: 4.632
  Learning Rate: 3.38e-05
  GPU Memory Used: 12004MB

[35:04:17] Step 91350/125000:
  Training Loss: 1.439
  Perplexity: 4.216
  Learning Rate: 3.37e-05
  GPU Memory Used: 11901MB

[35:05:25] Step 91400/125000:
  Training Loss: 1.347
  Perplexity: 3.846
  Learning Rate: 3.36e-05
  GPU Memory Used: 12126MB

[35:06:34] Step 91450/125000:
  Training Loss: 1.392
  Perplexity: 4.024
  Learning Rate: 3.35e-05
  GPU Memory Used: 12085MB

[35:07:42] Step 91500/125000:
  Training Loss: 1.483
  Perplexity: 4.405
  Learning Rate: 3.34e-05
  GPU Memory Used: 12195MB


Evaluation Results:
  Validation Loss: 1.551
  Validation Perplexity: 4.717

Saving model checkpoint: proposal_model/checkpoint-91500


Epoch 184/250
==================================================
[35:08:50] Step 91550/125000:
  Training Loss: 1.406
  Perplexity: 4.081
  Learning Rate: 3.33e-05
  GPU Memory Used: 11897MB

[35:10:01] Step 91600/125000:
  Training Loss: 1.435
  Perplexity: 4.200
  Learning Rate: 3.32e-05
  GPU Memory Used: 11949MB

[35:11:11] Step 91650/125000:
  Training Loss: 1.334
  Perplexity: 3.797
  Learning Rate: 3.31e-05
  GPU Memory Used: 11926MB

[35:12:19] Step 91700/125000:
  Training Loss: 1.483
  Perplexity: 4.406
  Learning Rate: 3.30e-05
  GPU Memory Used: 12170MB

[35:13:29] Step 91750/125000:
  Training Loss: 1.439
  Perplexity: 4.218
  Learning Rate: 3.29e-05
  GPU Memory Used: 12193MB

[35:14:38] Step 91800/125000:
  Training Loss: 1.466
  Perplexity: 4.333
  Learning Rate: 3.28e-05
  GPU Memory Used: 11859MB

[35:15:45] Step 91850/125000:
  Training Loss: 1.363
  Perplexity: 3.909
  Learning Rate: 3.27e-05
  GPU Memory Used: 11981MB

[35:16:52] Step 91900/125000:
  Training Loss: 1.432
  Perplexity: 4.188
  Learning Rate: 3.27e-05
  GPU Memory Used: 11889MB

[35:18:01] Step 91950/125000:
  Training Loss: 1.423
  Perplexity: 4.148
  Learning Rate: 3.26e-05
  GPU Memory Used: 12134MB

[35:19:12] Step 92000/125000:
  Training Loss: 1.402
  Perplexity: 4.063
  Learning Rate: 3.25e-05
  GPU Memory Used: 11863MB


Evaluation Results:
  Validation Loss: 1.521
  Validation Perplexity: 4.577

Saving model checkpoint: proposal_model/checkpoint-92000


Epoch 185/250
==================================================
[35:20:19] Step 92050/125000:
  Training Loss: 1.457
  Perplexity: 4.291
  Learning Rate: 3.24e-05
  GPU Memory Used: 11864MB

[35:21:29] Step 92100/125000:
  Training Loss: 1.414
  Perplexity: 4.112
  Learning Rate: 3.23e-05
  GPU Memory Used: 11894MB

[35:22:36] Step 92150/125000:
  Training Loss: 1.397
  Perplexity: 4.044
  Learning Rate: 3.22e-05
  GPU Memory Used: 12076MB

[35:23:45] Step 92200/125000:
  Training Loss: 1.376
  Perplexity: 3.959
  Learning Rate: 3.21e-05
  GPU Memory Used: 12044MB

[35:24:53] Step 92250/125000:
  Training Loss: 1.360
  Perplexity: 3.898
  Learning Rate: 3.20e-05
  GPU Memory Used: 11880MB

[35:26:02] Step 92300/125000:
  Training Loss: 1.470
  Perplexity: 4.347
  Learning Rate: 3.19e-05
  GPU Memory Used: 11964MB

[35:27:10] Step 92350/125000:
  Training Loss: 1.416
  Perplexity: 4.120
  Learning Rate: 3.18e-05
  GPU Memory Used: 12063MB

[35:28:21] Step 92400/125000:
  Training Loss: 1.440
  Perplexity: 4.221
  Learning Rate: 3.17e-05
  GPU Memory Used: 11836MB

[35:29:31] Step 92450/125000:
  Training Loss: 1.438
  Perplexity: 4.213
  Learning Rate: 3.16e-05
  GPU Memory Used: 12015MB

[35:30:38] Step 92500/125000:
  Training Loss: 1.378
  Perplexity: 3.969
  Learning Rate: 3.15e-05
  GPU Memory Used: 11822MB


Evaluation Results:
  Validation Loss: 1.525
  Validation Perplexity: 4.595

Saving model checkpoint: proposal_model/checkpoint-92500


Epoch 186/250
==================================================
[35:31:46] Step 92550/125000:
  Training Loss: 1.332
  Perplexity: 3.789
  Learning Rate: 3.15e-05
  GPU Memory Used: 11976MB

[35:32:57] Step 92600/125000:
  Training Loss: 1.407
  Perplexity: 4.083
  Learning Rate: 3.14e-05
  GPU Memory Used: 11947MB

[35:34:05] Step 92650/125000:
  Training Loss: 1.386
  Perplexity: 3.999
  Learning Rate: 3.13e-05
  GPU Memory Used: 12065MB

[35:35:15] Step 92700/125000:
  Training Loss: 1.379
  Perplexity: 3.971
  Learning Rate: 3.12e-05
  GPU Memory Used: 11994MB

[35:36:23] Step 92750/125000:
  Training Loss: 1.431
  Perplexity: 4.184
  Learning Rate: 3.11e-05
  GPU Memory Used: 11975MB

[35:37:32] Step 92800/125000:
  Training Loss: 1.345
  Perplexity: 3.838
  Learning Rate: 3.10e-05
  GPU Memory Used: 11953MB

[35:38:39] Step 92850/125000:
  Training Loss: 1.377
  Perplexity: 3.964
  Learning Rate: 3.09e-05
  GPU Memory Used: 12124MB

[35:39:48] Step 92900/125000:
  Training Loss: 1.381
  Perplexity: 3.978
  Learning Rate: 3.08e-05
  GPU Memory Used: 12190MB

[35:40:55] Step 92950/125000:
  Training Loss: 1.365
  Perplexity: 3.914
  Learning Rate: 3.07e-05
  GPU Memory Used: 11934MB

[35:42:05] Step 93000/125000:
  Training Loss: 1.401
  Perplexity: 4.058
  Learning Rate: 3.06e-05
  GPU Memory Used: 11955MB


Evaluation Results:
  Validation Loss: 1.581
  Validation Perplexity: 4.859

Saving model checkpoint: proposal_model/checkpoint-93000


Epoch 187/250
==================================================
[35:43:14] Step 93050/125000:
  Training Loss: 1.360
  Perplexity: 3.895
  Learning Rate: 3.05e-05
  GPU Memory Used: 12118MB

[35:44:24] Step 93100/125000:
  Training Loss: 1.454
  Perplexity: 4.282
  Learning Rate: 3.05e-05
  GPU Memory Used: 12135MB

[35:45:35] Step 93150/125000:
  Training Loss: 1.428
  Perplexity: 4.169
  Learning Rate: 3.04e-05
  GPU Memory Used: 11998MB

[35:46:41] Step 93200/125000:
  Training Loss: 1.414
  Perplexity: 4.111
  Learning Rate: 3.03e-05
  GPU Memory Used: 11953MB

[35:47:49] Step 93250/125000:
  Training Loss: 1.492
  Perplexity: 4.447
  Learning Rate: 3.02e-05
  GPU Memory Used: 11937MB

[35:48:59] Step 93300/125000:
  Training Loss: 1.400
  Perplexity: 4.054
  Learning Rate: 3.01e-05
  GPU Memory Used: 12048MB

[35:50:10] Step 93350/125000:
  Training Loss: 1.467
  Perplexity: 4.336
  Learning Rate: 3.00e-05
  GPU Memory Used: 11926MB

[35:51:20] Step 93400/125000:
  Training Loss: 1.405
  Perplexity: 4.077
  Learning Rate: 2.99e-05
  GPU Memory Used: 12171MB

[35:52:28] Step 93450/125000:
  Training Loss: 1.391
  Perplexity: 4.021
  Learning Rate: 2.98e-05
  GPU Memory Used: 11849MB

[35:53:36] Step 93500/125000:
  Training Loss: 1.340
  Perplexity: 3.817
  Learning Rate: 2.97e-05
  GPU Memory Used: 12062MB


Evaluation Results:
  Validation Loss: 1.562
  Validation Perplexity: 4.770

Saving model checkpoint: proposal_model/checkpoint-93500


Epoch 188/250
==================================================
[35:54:45] Step 93550/125000:
  Training Loss: 1.415
  Perplexity: 4.116
  Learning Rate: 2.96e-05
  GPU Memory Used: 12142MB

[35:55:54] Step 93600/125000:
  Training Loss: 1.430
  Perplexity: 4.178
  Learning Rate: 2.96e-05
  GPU Memory Used: 11966MB

[35:57:02] Step 93650/125000:
  Training Loss: 1.427
  Perplexity: 4.166
  Learning Rate: 2.95e-05
  GPU Memory Used: 12123MB

[35:58:10] Step 93700/125000:
  Training Loss: 1.457
  Perplexity: 4.294
  Learning Rate: 2.94e-05
  GPU Memory Used: 11962MB

[35:59:22] Step 93750/125000:
  Training Loss: 1.384
  Perplexity: 3.989
  Learning Rate: 2.93e-05
  GPU Memory Used: 12113MB

[36:00:31] Step 93800/125000:
  Training Loss: 1.401
  Perplexity: 4.060
  Learning Rate: 2.92e-05
  GPU Memory Used: 12182MB

[36:01:39] Step 93850/125000:
  Training Loss: 1.412
  Perplexity: 4.104
  Learning Rate: 2.91e-05
  GPU Memory Used: 11810MB

[36:02:46] Step 93900/125000:
  Training Loss: 1.436
  Perplexity: 4.204
  Learning Rate: 2.90e-05
  GPU Memory Used: 12051MB

[36:03:55] Step 93950/125000:
  Training Loss: 1.374
  Perplexity: 3.950
  Learning Rate: 2.89e-05
  GPU Memory Used: 12135MB

[36:05:02] Step 94000/125000:
  Training Loss: 1.424
  Perplexity: 4.155
  Learning Rate: 2.88e-05
  GPU Memory Used: 12141MB


Evaluation Results:
  Validation Loss: 1.537
  Validation Perplexity: 4.650

Saving model checkpoint: proposal_model/checkpoint-94000


Epoch 189/250
==================================================
[36:06:10] Step 94050/125000:
  Training Loss: 1.378
  Perplexity: 3.966
  Learning Rate: 2.88e-05
  GPU Memory Used: 11884MB

[36:07:17] Step 94100/125000:
  Training Loss: 1.411
  Perplexity: 4.099
  Learning Rate: 2.87e-05
  GPU Memory Used: 11938MB

[36:08:26] Step 94150/125000:
  Training Loss: 1.383
  Perplexity: 3.989
  Learning Rate: 2.86e-05
  GPU Memory Used: 12075MB

[36:09:37] Step 94200/125000:
  Training Loss: 1.420
  Perplexity: 4.136
  Learning Rate: 2.85e-05
  GPU Memory Used: 12152MB

[36:10:47] Step 94250/125000:
  Training Loss: 1.404
  Perplexity: 4.071
  Learning Rate: 2.84e-05
  GPU Memory Used: 12168MB

[36:11:56] Step 94300/125000:
  Training Loss: 1.436
  Perplexity: 4.204
  Learning Rate: 2.83e-05
  GPU Memory Used: 12015MB

[36:13:08] Step 94350/125000:
  Training Loss: 1.425
  Perplexity: 4.156
  Learning Rate: 2.82e-05
  GPU Memory Used: 11844MB

[36:14:18] Step 94400/125000:
  Training Loss: 1.432
  Perplexity: 4.187
  Learning Rate: 2.81e-05
  GPU Memory Used: 11816MB

[36:15:27] Step 94450/125000:
  Training Loss: 1.463
  Perplexity: 4.319
  Learning Rate: 2.81e-05
  GPU Memory Used: 11978MB

[36:16:36] Step 94500/125000:
  Training Loss: 1.394
  Perplexity: 4.032
  Learning Rate: 2.80e-05
  GPU Memory Used: 12143MB


Evaluation Results:
  Validation Loss: 1.558
  Validation Perplexity: 4.749

Saving model checkpoint: proposal_model/checkpoint-94500


Epoch 190/250
==================================================
[36:17:47] Step 94550/125000:
  Training Loss: 1.385
  Perplexity: 3.995
  Learning Rate: 2.79e-05
  GPU Memory Used: 12007MB

[36:18:57] Step 94600/125000:
  Training Loss: 1.354
  Perplexity: 3.871
  Learning Rate: 2.78e-05
  GPU Memory Used: 11805MB

[36:20:07] Step 94650/125000:
  Training Loss: 1.408
  Perplexity: 4.088
  Learning Rate: 2.77e-05
  GPU Memory Used: 12089MB

[36:21:15] Step 94700/125000:
  Training Loss: 1.420
  Perplexity: 4.138
  Learning Rate: 2.76e-05
  GPU Memory Used: 12173MB

[36:22:22] Step 94750/125000:
  Training Loss: 1.426
  Perplexity: 4.162
  Learning Rate: 2.75e-05
  GPU Memory Used: 11844MB

[36:23:31] Step 94800/125000:
  Training Loss: 1.415
  Perplexity: 4.116
  Learning Rate: 2.74e-05
  GPU Memory Used: 11960MB

[36:24:38] Step 94850/125000:
  Training Loss: 1.415
  Perplexity: 4.116
  Learning Rate: 2.74e-05
  GPU Memory Used: 12128MB

[36:25:47] Step 94900/125000:
  Training Loss: 1.425
  Perplexity: 4.158
  Learning Rate: 2.73e-05
  GPU Memory Used: 11890MB

[36:26:58] Step 94950/125000:
  Training Loss: 1.366
  Perplexity: 3.920
  Learning Rate: 2.72e-05
  GPU Memory Used: 11814MB

[36:28:07] Step 95000/125000:
  Training Loss: 1.399
  Perplexity: 4.049
  Learning Rate: 2.71e-05
  GPU Memory Used: 12192MB


Evaluation Results:
  Validation Loss: 1.550
  Validation Perplexity: 4.713

Saving model checkpoint: proposal_model/checkpoint-95000


Epoch 191/250
==================================================
[36:29:16] Step 95050/125000:
  Training Loss: 1.375
  Perplexity: 3.956
  Learning Rate: 2.70e-05
  GPU Memory Used: 12160MB

[36:30:26] Step 95100/125000:
  Training Loss: 1.413
  Perplexity: 4.108
  Learning Rate: 2.69e-05
  GPU Memory Used: 12152MB

[36:31:36] Step 95150/125000:
  Training Loss: 1.425
  Perplexity: 4.157
  Learning Rate: 2.68e-05
  GPU Memory Used: 11837MB

[36:32:46] Step 95200/125000:
  Training Loss: 1.399
  Perplexity: 4.053
  Learning Rate: 2.68e-05
  GPU Memory Used: 12031MB

[36:33:55] Step 95250/125000:
  Training Loss: 1.392
  Perplexity: 4.022
  Learning Rate: 2.67e-05
  GPU Memory Used: 11836MB

[36:35:05] Step 95300/125000:
  Training Loss: 1.371
  Perplexity: 3.938
  Learning Rate: 2.66e-05
  GPU Memory Used: 12159MB

[36:36:13] Step 95350/125000:
  Training Loss: 1.376
  Perplexity: 3.957
  Learning Rate: 2.65e-05
  GPU Memory Used: 11953MB

[36:37:23] Step 95400/125000:
  Training Loss: 1.426
  Perplexity: 4.162
  Learning Rate: 2.64e-05
  GPU Memory Used: 12035MB

[36:38:33] Step 95450/125000:
  Training Loss: 1.408
  Perplexity: 4.089
  Learning Rate: 2.63e-05
  GPU Memory Used: 12122MB

[36:39:43] Step 95500/125000:
  Training Loss: 1.407
  Perplexity: 4.083
  Learning Rate: 2.62e-05
  GPU Memory Used: 12113MB


Evaluation Results:
  Validation Loss: 1.529
  Validation Perplexity: 4.614

Saving model checkpoint: proposal_model/checkpoint-95500


Epoch 192/250
==================================================
[36:40:52] Step 95550/125000:
  Training Loss: 1.465
  Perplexity: 4.326
  Learning Rate: 2.62e-05
  GPU Memory Used: 12046MB

[36:42:01] Step 95600/125000:
  Training Loss: 1.387
  Perplexity: 4.004
  Learning Rate: 2.61e-05
  GPU Memory Used: 12077MB

[36:43:11] Step 95650/125000:
  Training Loss: 1.406
  Perplexity: 4.079
  Learning Rate: 2.60e-05
  GPU Memory Used: 11822MB

[36:44:18] Step 95700/125000:
  Training Loss: 1.455
  Perplexity: 4.284
  Learning Rate: 2.59e-05
  GPU Memory Used: 11861MB

[36:45:28] Step 95750/125000:
  Training Loss: 1.382
  Perplexity: 3.984
  Learning Rate: 2.58e-05
  GPU Memory Used: 11807MB

[36:46:36] Step 95800/125000:
  Training Loss: 1.448
  Perplexity: 4.254
  Learning Rate: 2.57e-05
  GPU Memory Used: 12085MB

[36:47:45] Step 95850/125000:
  Training Loss: 1.426
  Perplexity: 4.161
  Learning Rate: 2.57e-05
  GPU Memory Used: 11816MB

[36:48:54] Step 95900/125000:
  Training Loss: 1.372
  Perplexity: 3.942
  Learning Rate: 2.56e-05
  GPU Memory Used: 12000MB

[36:50:02] Step 95950/125000:
  Training Loss: 1.468
  Perplexity: 4.341
  Learning Rate: 2.55e-05
  GPU Memory Used: 12043MB

[36:51:10] Step 96000/125000:
  Training Loss: 1.451
  Perplexity: 4.267
  Learning Rate: 2.54e-05
  GPU Memory Used: 12019MB


Evaluation Results:
  Validation Loss: 1.480
  Validation Perplexity: 4.391

Saving model checkpoint: proposal_model/checkpoint-96000


Epoch 193/250
==================================================
[36:52:20] Step 96050/125000:
  Training Loss: 1.419
  Perplexity: 4.133
  Learning Rate: 2.53e-05
  GPU Memory Used: 11900MB

[36:53:29] Step 96100/125000:
  Training Loss: 1.437
  Perplexity: 4.210
  Learning Rate: 2.52e-05
  GPU Memory Used: 12017MB

[36:54:37] Step 96150/125000:
  Training Loss: 1.403
  Perplexity: 4.068
  Learning Rate: 2.52e-05
  GPU Memory Used: 11801MB

[36:55:46] Step 96200/125000:
  Training Loss: 1.393
  Perplexity: 4.028
  Learning Rate: 2.51e-05
  GPU Memory Used: 11831MB

[36:56:53] Step 96250/125000:
  Training Loss: 1.369
  Perplexity: 3.931
  Learning Rate: 2.50e-05
  GPU Memory Used: 11916MB

[36:58:04] Step 96300/125000:
  Training Loss: 1.415
  Perplexity: 4.117
  Learning Rate: 2.49e-05
  GPU Memory Used: 11921MB

[36:59:12] Step 96350/125000:
  Training Loss: 1.437
  Perplexity: 4.209
  Learning Rate: 2.48e-05
  GPU Memory Used: 11924MB

[37:00:23] Step 96400/125000:
  Training Loss: 1.381
  Perplexity: 3.979
  Learning Rate: 2.47e-05
  GPU Memory Used: 12092MB

[37:01:32] Step 96450/125000:
  Training Loss: 1.439
  Perplexity: 4.214
  Learning Rate: 2.47e-05
  GPU Memory Used: 11803MB

[37:02:42] Step 96500/125000:
  Training Loss: 1.407
  Perplexity: 4.084
  Learning Rate: 2.46e-05
  GPU Memory Used: 11871MB


Evaluation Results:
  Validation Loss: 1.586
  Validation Perplexity: 4.882

Saving model checkpoint: proposal_model/checkpoint-96500


Epoch 194/250
==================================================
[37:03:52] Step 96550/125000:
  Training Loss: 1.462
  Perplexity: 4.315
  Learning Rate: 2.45e-05
  GPU Memory Used: 12135MB

[37:05:02] Step 96600/125000:
  Training Loss: 1.369
  Perplexity: 3.930
  Learning Rate: 2.44e-05
  GPU Memory Used: 12032MB

[37:06:12] Step 96650/125000:
  Training Loss: 1.427
  Perplexity: 4.167
  Learning Rate: 2.43e-05
  GPU Memory Used: 12069MB

[37:07:22] Step 96700/125000:
  Training Loss: 1.438
  Perplexity: 4.213
  Learning Rate: 2.42e-05
  GPU Memory Used: 12148MB

[37:08:30] Step 96750/125000:
  Training Loss: 1.458
  Perplexity: 4.299
  Learning Rate: 2.42e-05
  GPU Memory Used: 11973MB

[37:09:37] Step 96800/125000:
  Training Loss: 1.398
  Perplexity: 4.045
  Learning Rate: 2.41e-05
  GPU Memory Used: 12132MB

[37:10:48] Step 96850/125000:
  Training Loss: 1.425
  Perplexity: 4.160
  Learning Rate: 2.40e-05
  GPU Memory Used: 12152MB

[37:11:58] Step 96900/125000:
  Training Loss: 1.400
  Perplexity: 4.055
  Learning Rate: 2.39e-05
  GPU Memory Used: 11844MB

[37:13:07] Step 96950/125000:
  Training Loss: 1.389
  Perplexity: 4.012
  Learning Rate: 2.38e-05
  GPU Memory Used: 12062MB

[37:14:18] Step 97000/125000:
  Training Loss: 1.418
  Perplexity: 4.129
  Learning Rate: 2.38e-05
  GPU Memory Used: 11912MB


Evaluation Results:
  Validation Loss: 1.540
  Validation Perplexity: 4.662

Saving model checkpoint: proposal_model/checkpoint-97000


Epoch 195/250
==================================================
[37:15:28] Step 97050/125000:
  Training Loss: 1.462
  Perplexity: 4.315
  Learning Rate: 2.37e-05
  GPU Memory Used: 12171MB

[37:16:37] Step 97100/125000:
  Training Loss: 1.380
  Perplexity: 3.976
  Learning Rate: 2.36e-05
  GPU Memory Used: 12081MB

[37:17:45] Step 97150/125000:
  Training Loss: 1.389
  Perplexity: 4.010
  Learning Rate: 2.35e-05
  GPU Memory Used: 11850MB

[37:18:54] Step 97200/125000:
  Training Loss: 1.349
  Perplexity: 3.854
  Learning Rate: 2.34e-05
  GPU Memory Used: 12179MB

[37:20:03] Step 97250/125000:
  Training Loss: 1.427
  Perplexity: 4.167
  Learning Rate: 2.34e-05
  GPU Memory Used: 12137MB

[37:21:11] Step 97300/125000:
  Training Loss: 1.447
  Perplexity: 4.251
  Learning Rate: 2.33e-05
  GPU Memory Used: 11922MB

[37:22:21] Step 97350/125000:
  Training Loss: 1.461
  Perplexity: 4.310
  Learning Rate: 2.32e-05
  GPU Memory Used: 12006MB

[37:23:31] Step 97400/125000:
  Training Loss: 1.383
  Perplexity: 3.988
  Learning Rate: 2.31e-05
  GPU Memory Used: 11838MB

[37:24:39] Step 97450/125000:
  Training Loss: 1.375
  Perplexity: 3.956
  Learning Rate: 2.30e-05
  GPU Memory Used: 11843MB

[37:25:47] Step 97500/125000:
  Training Loss: 1.452
  Perplexity: 4.271
  Learning Rate: 2.29e-05
  GPU Memory Used: 12018MB


Evaluation Results:
  Validation Loss: 1.564
  Validation Perplexity: 4.779

Saving model checkpoint: proposal_model/checkpoint-97500


Epoch 196/250
==================================================
[37:26:57] Step 97550/125000:
  Training Loss: 1.403
  Perplexity: 4.066
  Learning Rate: 2.29e-05
  GPU Memory Used: 11908MB

[37:28:06] Step 97600/125000:
  Training Loss: 1.353
  Perplexity: 3.871
  Learning Rate: 2.28e-05
  GPU Memory Used: 12149MB

[37:29:16] Step 97650/125000:
  Training Loss: 1.479
  Perplexity: 4.387
  Learning Rate: 2.27e-05
  GPU Memory Used: 12118MB

[37:30:25] Step 97700/125000:
  Training Loss: 1.345
  Perplexity: 3.839
  Learning Rate: 2.26e-05
  GPU Memory Used: 12200MB

[37:31:33] Step 97750/125000:
  Training Loss: 1.343
  Perplexity: 3.832
  Learning Rate: 2.25e-05
  GPU Memory Used: 11817MB

[37:32:44] Step 97800/125000:
  Training Loss: 1.348
  Perplexity: 3.849
  Learning Rate: 2.25e-05
  GPU Memory Used: 11887MB

[37:33:53] Step 97850/125000:
  Training Loss: 1.386
  Perplexity: 3.997
  Learning Rate: 2.24e-05
  GPU Memory Used: 12150MB

[37:35:01] Step 97900/125000:
  Training Loss: 1.416
  Perplexity: 4.119
  Learning Rate: 2.23e-05
  GPU Memory Used: 12084MB

[37:36:11] Step 97950/125000:
  Training Loss: 1.392
  Perplexity: 4.023
  Learning Rate: 2.22e-05
  GPU Memory Used: 11874MB

[37:37:21] Step 98000/125000:
  Training Loss: 1.347
  Perplexity: 3.847
  Learning Rate: 2.22e-05
  GPU Memory Used: 12014MB


Evaluation Results:
  Validation Loss: 1.555
  Validation Perplexity: 4.736

Saving model checkpoint: proposal_model/checkpoint-98000


Epoch 197/250
==================================================
[37:38:30] Step 98050/125000:
  Training Loss: 1.438
  Perplexity: 4.211
  Learning Rate: 2.21e-05
  GPU Memory Used: 11861MB

[37:39:41] Step 98100/125000:
  Training Loss: 1.429
  Perplexity: 4.174
  Learning Rate: 2.20e-05
  GPU Memory Used: 11859MB

[37:40:52] Step 98150/125000:
  Training Loss: 1.402
  Perplexity: 4.061
  Learning Rate: 2.19e-05
  GPU Memory Used: 12167MB

[37:42:02] Step 98200/125000:
  Training Loss: 1.327
  Perplexity: 3.771
  Learning Rate: 2.18e-05
  GPU Memory Used: 11867MB

[37:43:11] Step 98250/125000:
  Training Loss: 1.401
  Perplexity: 4.059
  Learning Rate: 2.18e-05
  GPU Memory Used: 11993MB

[37:44:21] Step 98300/125000:
  Training Loss: 1.453
  Perplexity: 4.277
  Learning Rate: 2.17e-05
  GPU Memory Used: 11987MB

[37:45:30] Step 98350/125000:
  Training Loss: 1.387
  Perplexity: 4.004
  Learning Rate: 2.16e-05
  GPU Memory Used: 11912MB

[37:46:38] Step 98400/125000:
  Training Loss: 1.357
  Perplexity: 3.883
  Learning Rate: 2.15e-05
  GPU Memory Used: 12060MB

[37:47:50] Step 98450/125000:
  Training Loss: 1.354
  Perplexity: 3.871
  Learning Rate: 2.14e-05
  GPU Memory Used: 12181MB

[37:48:59] Step 98500/125000:
  Training Loss: 1.405
  Perplexity: 4.077
  Learning Rate: 2.14e-05
  GPU Memory Used: 12032MB


Evaluation Results:
  Validation Loss: 1.526
  Validation Perplexity: 4.598

Saving model checkpoint: proposal_model/checkpoint-98500


Epoch 198/250
==================================================
[37:50:10] Step 98550/125000:
  Training Loss: 1.345
  Perplexity: 3.839
  Learning Rate: 2.13e-05
  GPU Memory Used: 12170MB

[37:51:20] Step 98600/125000:
  Training Loss: 1.337
  Perplexity: 3.807
  Learning Rate: 2.12e-05
  GPU Memory Used: 11900MB

[37:52:26] Step 98650/125000:
  Training Loss: 1.392
  Perplexity: 4.023
  Learning Rate: 2.11e-05
  GPU Memory Used: 12026MB

[37:53:35] Step 98700/125000:
  Training Loss: 1.374
  Perplexity: 3.951
  Learning Rate: 2.11e-05
  GPU Memory Used: 11838MB

[37:54:45] Step 98750/125000:
  Training Loss: 1.390
  Perplexity: 4.016
  Learning Rate: 2.10e-05
  GPU Memory Used: 11990MB

[37:55:52] Step 98800/125000:
  Training Loss: 1.425
  Perplexity: 4.157
  Learning Rate: 2.09e-05
  GPU Memory Used: 12098MB

[37:57:00] Step 98850/125000:
  Training Loss: 1.372
  Perplexity: 3.942
  Learning Rate: 2.08e-05
  GPU Memory Used: 12193MB

[37:58:10] Step 98900/125000:
  Training Loss: 1.401
  Perplexity: 4.059
  Learning Rate: 2.08e-05
  GPU Memory Used: 11920MB

[37:59:17] Step 98950/125000:
  Training Loss: 1.373
  Perplexity: 3.948
  Learning Rate: 2.07e-05
  GPU Memory Used: 12169MB

[38:00:28] Step 99000/125000:
  Training Loss: 1.439
  Perplexity: 4.217
  Learning Rate: 2.06e-05
  GPU Memory Used: 11963MB


Evaluation Results:
  Validation Loss: 1.558
  Validation Perplexity: 4.752

Saving model checkpoint: proposal_model/checkpoint-99000


Epoch 199/250
==================================================
[38:01:37] Step 99050/125000:
  Training Loss: 1.341
  Perplexity: 3.821
  Learning Rate: 2.05e-05
  GPU Memory Used: 12032MB

[38:02:49] Step 99100/125000:
  Training Loss: 1.393
  Perplexity: 4.028
  Learning Rate: 2.04e-05
  GPU Memory Used: 11891MB

[38:03:56] Step 99150/125000:
  Training Loss: 1.434
  Perplexity: 4.194
  Learning Rate: 2.04e-05
  GPU Memory Used: 11882MB

[38:05:04] Step 99200/125000:
  Training Loss: 1.382
  Perplexity: 3.982
  Learning Rate: 2.03e-05
  GPU Memory Used: 11981MB

[38:06:15] Step 99250/125000:
  Training Loss: 1.349
  Perplexity: 3.853
  Learning Rate: 2.02e-05
  GPU Memory Used: 11903MB

[38:07:23] Step 99300/125000:
  Training Loss: 1.358
  Perplexity: 3.889
  Learning Rate: 2.01e-05
  GPU Memory Used: 11982MB

[38:08:31] Step 99350/125000:
  Training Loss: 1.339
  Perplexity: 3.816
  Learning Rate: 2.01e-05
  GPU Memory Used: 12041MB

[38:09:42] Step 99400/125000:
  Training Loss: 1.406
  Perplexity: 4.079
  Learning Rate: 2.00e-05
  GPU Memory Used: 11950MB

[38:10:51] Step 99450/125000:
  Training Loss: 1.431
  Perplexity: 4.185
  Learning Rate: 1.99e-05
  GPU Memory Used: 12136MB

[38:12:02] Step 99500/125000:
  Training Loss: 1.447
  Perplexity: 4.251
  Learning Rate: 1.98e-05
  GPU Memory Used: 11925MB


Evaluation Results:
  Validation Loss: 1.568
  Validation Perplexity: 4.797

Saving model checkpoint: proposal_model/checkpoint-99500


Epoch 200/250
==================================================
[38:13:10] Step 99550/125000:
  Training Loss: 1.385
  Perplexity: 3.994
  Learning Rate: 1.98e-05
  GPU Memory Used: 11836MB

[38:14:22] Step 99600/125000:
  Training Loss: 1.336
  Perplexity: 3.802
  Learning Rate: 1.97e-05
  GPU Memory Used: 12019MB

[38:15:32] Step 99650/125000:
  Training Loss: 1.376
  Perplexity: 3.959
  Learning Rate: 1.96e-05
  GPU Memory Used: 12040MB

[38:16:40] Step 99700/125000:
  Training Loss: 1.350
  Perplexity: 3.859
  Learning Rate: 1.95e-05
  GPU Memory Used: 11820MB

[38:17:50] Step 99750/125000:
  Training Loss: 1.385
  Perplexity: 3.994
  Learning Rate: 1.95e-05
  GPU Memory Used: 11946MB

[38:18:59] Step 99800/125000:
  Training Loss: 1.410
  Perplexity: 4.098
  Learning Rate: 1.94e-05
  GPU Memory Used: 11875MB

[38:20:08] Step 99850/125000:
  Training Loss: 1.330
  Perplexity: 3.779
  Learning Rate: 1.93e-05
  GPU Memory Used: 11872MB

[38:21:17] Step 99900/125000:
  Training Loss: 1.362
  Perplexity: 3.905
  Learning Rate: 1.92e-05
  GPU Memory Used: 11955MB

[38:22:31] Step 99950/125000:
  Training Loss: 1.397
  Perplexity: 4.043
  Learning Rate: 1.92e-05
  GPU Memory Used: 11958MB

[38:23:42] Step 100000/125000:
  Training Loss: 1.402
  Perplexity: 4.061
  Learning Rate: 1.91e-05
  GPU Memory Used: 11915MB


Evaluation Results:
  Validation Loss: 1.526
  Validation Perplexity: 4.600

Saving model checkpoint: proposal_model/checkpoint-100000


Epoch 201/250
==================================================
[38:24:52] Step 100050/125000:
  Training Loss: 1.378
  Perplexity: 3.968
  Learning Rate: 1.90e-05
  GPU Memory Used: 12116MB

[38:26:00] Step 100100/125000:
  Training Loss: 1.350
  Perplexity: 3.858
  Learning Rate: 1.90e-05
  GPU Memory Used: 12044MB

[38:27:09] Step 100150/125000:
  Training Loss: 1.399
  Perplexity: 4.049
  Learning Rate: 1.89e-05
  GPU Memory Used: 12138MB

[38:28:20] Step 100200/125000:
  Training Loss: 1.407
  Perplexity: 4.086
  Learning Rate: 1.88e-05
  GPU Memory Used: 11928MB

[38:29:31] Step 100250/125000:
  Training Loss: 1.384
  Perplexity: 3.992
  Learning Rate: 1.87e-05
  GPU Memory Used: 11891MB

[38:30:39] Step 100300/125000:
  Training Loss: 1.432
  Perplexity: 4.189
  Learning Rate: 1.87e-05
  GPU Memory Used: 12192MB

[38:31:50] Step 100350/125000:
  Training Loss: 1.360
  Perplexity: 3.896
  Learning Rate: 1.86e-05
  GPU Memory Used: 11807MB

[38:33:01] Step 100400/125000:
  Training Loss: 1.351
  Perplexity: 3.862
  Learning Rate: 1.85e-05
  GPU Memory Used: 12019MB

[38:34:10] Step 100450/125000:
  Training Loss: 1.380
  Perplexity: 3.975
  Learning Rate: 1.84e-05
  GPU Memory Used: 12037MB

[38:35:20] Step 100500/125000:
  Training Loss: 1.470
  Perplexity: 4.350
  Learning Rate: 1.84e-05
  GPU Memory Used: 12198MB


Evaluation Results:
  Validation Loss: 1.539
  Validation Perplexity: 4.662

Saving model checkpoint: proposal_model/checkpoint-100500


Epoch 202/250
==================================================
[38:36:28] Step 100550/125000:
  Training Loss: 1.315
  Perplexity: 3.724
  Learning Rate: 1.83e-05
  GPU Memory Used: 12159MB

[38:37:38] Step 100600/125000:
  Training Loss: 1.355
  Perplexity: 3.876
  Learning Rate: 1.82e-05
  GPU Memory Used: 12023MB

[38:38:49] Step 100650/125000:
  Training Loss: 1.399
  Perplexity: 4.051
  Learning Rate: 1.81e-05
  GPU Memory Used: 11928MB

[38:39:59] Step 100700/125000:
  Training Loss: 1.342
  Perplexity: 3.827
  Learning Rate: 1.81e-05
  GPU Memory Used: 11874MB

[38:41:09] Step 100750/125000:
  Training Loss: 1.407
  Perplexity: 4.083
  Learning Rate: 1.80e-05
  GPU Memory Used: 12058MB

[38:42:17] Step 100800/125000:
  Training Loss: 1.430
  Perplexity: 4.179
  Learning Rate: 1.79e-05
  GPU Memory Used: 11855MB

[38:43:27] Step 100850/125000:
  Training Loss: 1.320
  Perplexity: 3.744
  Learning Rate: 1.79e-05
  GPU Memory Used: 12185MB

[38:44:34] Step 100900/125000:
  Training Loss: 1.433
  Perplexity: 4.193
  Learning Rate: 1.78e-05
  GPU Memory Used: 11996MB

[38:45:43] Step 100950/125000:
  Training Loss: 1.325
  Perplexity: 3.762
  Learning Rate: 1.77e-05
  GPU Memory Used: 12026MB

[38:46:52] Step 101000/125000:
  Training Loss: 1.324
  Perplexity: 3.760
  Learning Rate: 1.76e-05
  GPU Memory Used: 11804MB


Evaluation Results:
  Validation Loss: 1.577
  Validation Perplexity: 4.840

Saving model checkpoint: proposal_model/checkpoint-101000


Epoch 203/250
==================================================
[38:48:02] Step 101050/125000:
  Training Loss: 1.443
  Perplexity: 4.231
  Learning Rate: 1.76e-05
  GPU Memory Used: 11818MB

[38:49:11] Step 101100/125000:
  Training Loss: 1.374
  Perplexity: 3.953
  Learning Rate: 1.75e-05
  GPU Memory Used: 12191MB

[38:50:20] Step 101150/125000:
  Training Loss: 1.403
  Perplexity: 4.068
  Learning Rate: 1.74e-05
  GPU Memory Used: 12093MB

[38:51:27] Step 101200/125000:
  Training Loss: 1.315
  Perplexity: 3.726
  Learning Rate: 1.74e-05
  GPU Memory Used: 12054MB

[38:52:36] Step 101250/125000:
  Training Loss: 1.408
  Perplexity: 4.086
  Learning Rate: 1.73e-05
  GPU Memory Used: 12107MB

[38:53:46] Step 101300/125000:
  Training Loss: 1.367
  Perplexity: 3.924
  Learning Rate: 1.72e-05
  GPU Memory Used: 11910MB

[38:54:55] Step 101350/125000:
  Training Loss: 1.411
  Perplexity: 4.100
  Learning Rate: 1.72e-05
  GPU Memory Used: 12097MB

[38:56:03] Step 101400/125000:
  Training Loss: 1.359
  Perplexity: 3.893
  Learning Rate: 1.71e-05
  GPU Memory Used: 11892MB

[38:57:12] Step 101450/125000:
  Training Loss: 1.421
  Perplexity: 4.142
  Learning Rate: 1.70e-05
  GPU Memory Used: 11886MB

[38:58:22] Step 101500/125000:
  Training Loss: 1.359
  Perplexity: 3.890
  Learning Rate: 1.69e-05
  GPU Memory Used: 12124MB


Evaluation Results:
  Validation Loss: 1.484
  Validation Perplexity: 4.409

Saving model checkpoint: proposal_model/checkpoint-101500


Epoch 204/250
==================================================
[38:59:32] Step 101550/125000:
  Training Loss: 1.396
  Perplexity: 4.041
  Learning Rate: 1.69e-05
  GPU Memory Used: 12192MB

[39:00:42] Step 101600/125000:
  Training Loss: 1.359
  Perplexity: 3.892
  Learning Rate: 1.68e-05
  GPU Memory Used: 12062MB

[39:01:50] Step 101650/125000:
  Training Loss: 1.432
  Perplexity: 4.189
  Learning Rate: 1.67e-05
  GPU Memory Used: 12023MB

[39:03:01] Step 101700/125000:
  Training Loss: 1.356
  Perplexity: 3.881
  Learning Rate: 1.67e-05
  GPU Memory Used: 12144MB

[39:04:10] Step 101750/125000:
  Training Loss: 1.353
  Perplexity: 3.869
  Learning Rate: 1.66e-05
  GPU Memory Used: 12179MB

[39:05:20] Step 101800/125000:
  Training Loss: 1.335
  Perplexity: 3.801
  Learning Rate: 1.65e-05
  GPU Memory Used: 12003MB

[39:06:30] Step 101850/125000:
  Training Loss: 1.398
  Perplexity: 4.048
  Learning Rate: 1.65e-05
  GPU Memory Used: 11808MB

[39:07:37] Step 101900/125000:
  Training Loss: 1.355
  Perplexity: 3.876
  Learning Rate: 1.64e-05
  GPU Memory Used: 11894MB

[39:08:46] Step 101950/125000:
  Training Loss: 1.398
  Perplexity: 4.046
  Learning Rate: 1.63e-05
  GPU Memory Used: 12139MB

[39:09:55] Step 102000/125000:
  Training Loss: 1.369
  Perplexity: 3.931
  Learning Rate: 1.62e-05
  GPU Memory Used: 12165MB


Evaluation Results:
  Validation Loss: 1.438
  Validation Perplexity: 4.211

Saving model checkpoint: proposal_model/checkpoint-102000


Epoch 205/250
==================================================
[39:11:04] Step 102050/125000:
  Training Loss: 1.384
  Perplexity: 3.992
  Learning Rate: 1.62e-05
  GPU Memory Used: 11994MB

[39:12:14] Step 102100/125000:
  Training Loss: 1.398
  Perplexity: 4.046
  Learning Rate: 1.61e-05
  GPU Memory Used: 11849MB

[39:13:23] Step 102150/125000:
  Training Loss: 1.356
  Perplexity: 3.882
  Learning Rate: 1.60e-05
  GPU Memory Used: 11919MB

[39:14:31] Step 102200/125000:
  Training Loss: 1.410
  Perplexity: 4.096
  Learning Rate: 1.60e-05
  GPU Memory Used: 11843MB

[39:15:40] Step 102250/125000:
  Training Loss: 1.399
  Perplexity: 4.049
  Learning Rate: 1.59e-05
  GPU Memory Used: 12114MB

[39:16:50] Step 102300/125000:
  Training Loss: 1.361
  Perplexity: 3.899
  Learning Rate: 1.58e-05
  GPU Memory Used: 11941MB

[39:17:57] Step 102350/125000:
  Training Loss: 1.365
  Perplexity: 3.914
  Learning Rate: 1.58e-05
  GPU Memory Used: 11993MB

[39:19:07] Step 102400/125000:
  Training Loss: 1.437
  Perplexity: 4.206
  Learning Rate: 1.57e-05
  GPU Memory Used: 12041MB

[39:20:19] Step 102450/125000:
  Training Loss: 1.409
  Perplexity: 4.091
  Learning Rate: 1.56e-05
  GPU Memory Used: 12080MB

[39:21:29] Step 102500/125000:
  Training Loss: 1.399
  Perplexity: 4.052
  Learning Rate: 1.56e-05
  GPU Memory Used: 12076MB


Evaluation Results:
  Validation Loss: 1.504
  Validation Perplexity: 4.499

Saving model checkpoint: proposal_model/checkpoint-102500


Epoch 206/250
==================================================
[39:22:38] Step 102550/125000:
  Training Loss: 1.311
  Perplexity: 3.711
  Learning Rate: 1.55e-05
  GPU Memory Used: 12141MB

[39:23:46] Step 102600/125000:
  Training Loss: 1.353
  Perplexity: 3.869
  Learning Rate: 1.54e-05
  GPU Memory Used: 11937MB

[39:24:55] Step 102650/125000:
  Training Loss: 1.397
  Perplexity: 4.044
  Learning Rate: 1.54e-05
  GPU Memory Used: 12128MB

[39:26:02] Step 102700/125000:
  Training Loss: 1.393
  Perplexity: 4.027
  Learning Rate: 1.53e-05
  GPU Memory Used: 12023MB

[39:27:11] Step 102750/125000:
  Training Loss: 1.416
  Perplexity: 4.119
  Learning Rate: 1.52e-05
  GPU Memory Used: 12160MB

[39:28:20] Step 102800/125000:
  Training Loss: 1.340
  Perplexity: 3.817
  Learning Rate: 1.52e-05
  GPU Memory Used: 11941MB

[39:29:28] Step 102850/125000:
  Training Loss: 1.356
  Perplexity: 3.881
  Learning Rate: 1.51e-05
  GPU Memory Used: 12099MB

[39:30:40] Step 102900/125000:
  Training Loss: 1.319
  Perplexity: 3.738
  Learning Rate: 1.50e-05
  GPU Memory Used: 12011MB

[39:31:48] Step 102950/125000:
  Training Loss: 1.332
  Perplexity: 3.789
  Learning Rate: 1.50e-05
  GPU Memory Used: 11880MB

[39:32:56] Step 103000/125000:
  Training Loss: 1.420
  Perplexity: 4.138
  Learning Rate: 1.49e-05
  GPU Memory Used: 11849MB


Evaluation Results:
  Validation Loss: 1.536
  Validation Perplexity: 4.647

Saving model checkpoint: proposal_model/checkpoint-103000


Epoch 207/250
==================================================
[39:34:06] Step 103050/125000:
  Training Loss: 1.353
  Perplexity: 3.869
  Learning Rate: 1.48e-05
  GPU Memory Used: 11818MB

[39:35:17] Step 103100/125000:
  Training Loss: 1.379
  Perplexity: 3.971
  Learning Rate: 1.48e-05
  GPU Memory Used: 12127MB

[39:36:26] Step 103150/125000:
  Training Loss: 1.372
  Perplexity: 3.943
  Learning Rate: 1.47e-05
  GPU Memory Used: 12119MB

[39:37:38] Step 103200/125000:
  Training Loss: 1.434
  Perplexity: 4.195
  Learning Rate: 1.46e-05
  GPU Memory Used: 11813MB

[39:38:47] Step 103250/125000:
  Training Loss: 1.363
  Perplexity: 3.906
  Learning Rate: 1.46e-05
  GPU Memory Used: 12070MB

[39:39:58] Step 103300/125000:
  Training Loss: 1.366
  Perplexity: 3.921
  Learning Rate: 1.45e-05
  GPU Memory Used: 12000MB

[39:41:08] Step 103350/125000:
  Training Loss: 1.367
  Perplexity: 3.923
  Learning Rate: 1.44e-05
  GPU Memory Used: 12187MB

[39:42:17] Step 103400/125000:
  Training Loss: 1.442
  Perplexity: 4.229
  Learning Rate: 1.44e-05
  GPU Memory Used: 11881MB

[39:43:23] Step 103450/125000:
  Training Loss: 1.381
  Perplexity: 3.978
  Learning Rate: 1.43e-05
  GPU Memory Used: 12200MB

[39:44:34] Step 103500/125000:
  Training Loss: 1.367
  Perplexity: 3.923
  Learning Rate: 1.42e-05
  GPU Memory Used: 12062MB


Evaluation Results:
  Validation Loss: 1.478
  Validation Perplexity: 4.385

Saving model checkpoint: proposal_model/checkpoint-103500


Epoch 208/250
==================================================
[39:45:43] Step 103550/125000:
  Training Loss: 1.395
  Perplexity: 4.036
  Learning Rate: 1.42e-05
  GPU Memory Used: 12112MB

[39:46:53] Step 103600/125000:
  Training Loss: 1.368
  Perplexity: 3.927
  Learning Rate: 1.41e-05
  GPU Memory Used: 11838MB

[39:48:03] Step 103650/125000:
  Training Loss: 1.357
  Perplexity: 3.884
  Learning Rate: 1.41e-05
  GPU Memory Used: 12073MB

[39:49:11] Step 103700/125000:
  Training Loss: 1.310
  Perplexity: 3.707
  Learning Rate: 1.40e-05
  GPU Memory Used: 11828MB

[39:50:20] Step 103750/125000:
  Training Loss: 1.412
  Perplexity: 4.105
  Learning Rate: 1.39e-05
  GPU Memory Used: 12071MB

[39:51:30] Step 103800/125000:
  Training Loss: 1.389
  Perplexity: 4.011
  Learning Rate: 1.39e-05
  GPU Memory Used: 11986MB

[39:52:39] Step 103850/125000:
  Training Loss: 1.348
  Perplexity: 3.848
  Learning Rate: 1.38e-05
  GPU Memory Used: 11970MB

[39:53:47] Step 103900/125000:
  Training Loss: 1.421
  Perplexity: 4.141
  Learning Rate: 1.37e-05
  GPU Memory Used: 12188MB

[39:54:56] Step 103950/125000:
  Training Loss: 1.328
  Perplexity: 3.774
  Learning Rate: 1.37e-05
  GPU Memory Used: 11802MB

[39:56:05] Step 104000/125000:
  Training Loss: 1.318
  Perplexity: 3.737
  Learning Rate: 1.36e-05
  GPU Memory Used: 11996MB


Evaluation Results:
  Validation Loss: 1.520
  Validation Perplexity: 4.571

Saving model checkpoint: proposal_model/checkpoint-104000


Epoch 209/250
==================================================
[39:57:13] Step 104050/125000:
  Training Loss: 1.333
  Perplexity: 3.791
  Learning Rate: 1.35e-05
  GPU Memory Used: 11957MB

[39:58:23] Step 104100/125000:
  Training Loss: 1.370
  Perplexity: 3.936
  Learning Rate: 1.35e-05
  GPU Memory Used: 11913MB

[39:59:32] Step 104150/125000:
  Training Loss: 1.348
  Perplexity: 3.849
  Learning Rate: 1.34e-05
  GPU Memory Used: 11919MB

[40:00:44] Step 104200/125000:
  Training Loss: 1.332
  Perplexity: 3.788
  Learning Rate: 1.34e-05
  GPU Memory Used: 12035MB

[40:01:52] Step 104250/125000:
  Training Loss: 1.384
  Perplexity: 3.993
  Learning Rate: 1.33e-05
  GPU Memory Used: 12133MB

[40:03:03] Step 104300/125000:
  Training Loss: 1.347
  Perplexity: 3.848
  Learning Rate: 1.32e-05
  GPU Memory Used: 12131MB

[40:04:13] Step 104350/125000:
  Training Loss: 1.405
  Perplexity: 4.076
  Learning Rate: 1.32e-05
  GPU Memory Used: 12132MB

[40:05:21] Step 104400/125000:
  Training Loss: 1.368
  Perplexity: 3.928
  Learning Rate: 1.31e-05
  GPU Memory Used: 12012MB

[40:06:32] Step 104450/125000:
  Training Loss: 1.326
  Perplexity: 3.767
  Learning Rate: 1.30e-05
  GPU Memory Used: 12176MB

[40:07:41] Step 104500/125000:
  Training Loss: 1.398
  Perplexity: 4.048
  Learning Rate: 1.30e-05
  GPU Memory Used: 12066MB


Evaluation Results:
  Validation Loss: 1.498
  Validation Perplexity: 4.471

Saving model checkpoint: proposal_model/checkpoint-104500


Epoch 210/250
==================================================
[40:08:51] Step 104550/125000:
  Training Loss: 1.341
  Perplexity: 3.823
  Learning Rate: 1.29e-05
  GPU Memory Used: 11967MB

[40:09:58] Step 104600/125000:
  Training Loss: 1.359
  Perplexity: 3.891
  Learning Rate: 1.29e-05
  GPU Memory Used: 12007MB

[40:11:07] Step 104650/125000:
  Training Loss: 1.411
  Perplexity: 4.100
  Learning Rate: 1.28e-05
  GPU Memory Used: 12064MB

[40:12:16] Step 104700/125000:
  Training Loss: 1.312
  Perplexity: 3.712
  Learning Rate: 1.27e-05
  GPU Memory Used: 11866MB

[40:13:27] Step 104750/125000:
  Training Loss: 1.328
  Perplexity: 3.774
  Learning Rate: 1.27e-05
  GPU Memory Used: 12089MB

[40:14:37] Step 104800/125000:
  Training Loss: 1.374
  Perplexity: 3.953
  Learning Rate: 1.26e-05
  GPU Memory Used: 12019MB

[40:15:47] Step 104850/125000:
  Training Loss: 1.351
  Perplexity: 3.863
  Learning Rate: 1.26e-05
  GPU Memory Used: 11871MB

[40:16:55] Step 104900/125000:
  Training Loss: 1.384
  Perplexity: 3.991
  Learning Rate: 1.25e-05
  GPU Memory Used: 12173MB

[40:18:01] Step 104950/125000:
  Training Loss: 1.415
  Perplexity: 4.115
  Learning Rate: 1.24e-05
  GPU Memory Used: 12183MB

[40:19:11] Step 105000/125000:
  Training Loss: 1.381
  Perplexity: 3.980
  Learning Rate: 1.24e-05
  GPU Memory Used: 12099MB


Evaluation Results:
  Validation Loss: 1.558
  Validation Perplexity: 4.748

Saving model checkpoint: proposal_model/checkpoint-105000


Epoch 211/250
==================================================
[40:20:21] Step 105050/125000:
  Training Loss: 1.399
  Perplexity: 4.051
  Learning Rate: 1.23e-05
  GPU Memory Used: 11885MB

[40:21:29] Step 105100/125000:
  Training Loss: 1.362
  Perplexity: 3.905
  Learning Rate: 1.22e-05
  GPU Memory Used: 11921MB

[40:22:37] Step 105150/125000:
  Training Loss: 1.379
  Perplexity: 3.970
  Learning Rate: 1.22e-05
  GPU Memory Used: 11941MB

[40:23:48] Step 105200/125000:
  Training Loss: 1.384
  Perplexity: 3.990
  Learning Rate: 1.21e-05
  GPU Memory Used: 11871MB

[40:24:57] Step 105250/125000:
  Training Loss: 1.340
  Perplexity: 3.820
  Learning Rate: 1.21e-05
  GPU Memory Used: 11877MB

[40:26:05] Step 105300/125000:
  Training Loss: 1.428
  Perplexity: 4.169
  Learning Rate: 1.20e-05
  GPU Memory Used: 11827MB

[40:27:16] Step 105350/125000:
  Training Loss: 1.433
  Perplexity: 4.191
  Learning Rate: 1.19e-05
  GPU Memory Used: 12078MB

[40:28:25] Step 105400/125000:
  Training Loss: 1.361
  Perplexity: 3.900
  Learning Rate: 1.19e-05
  GPU Memory Used: 11972MB

[40:29:34] Step 105450/125000:
  Training Loss: 1.383
  Perplexity: 3.988
  Learning Rate: 1.18e-05
  GPU Memory Used: 12038MB

[40:30:41] Step 105500/125000:
  Training Loss: 1.372
  Perplexity: 3.943
  Learning Rate: 1.18e-05
  GPU Memory Used: 11949MB


Evaluation Results:
  Validation Loss: 1.533
  Validation Perplexity: 4.631

Saving model checkpoint: proposal_model/checkpoint-105500


Epoch 212/250
==================================================
[40:31:50] Step 105550/125000:
  Training Loss: 1.349
  Perplexity: 3.855
  Learning Rate: 1.17e-05
  GPU Memory Used: 12193MB

[40:33:01] Step 105600/125000:
  Training Loss: 1.382
  Perplexity: 3.984
  Learning Rate: 1.17e-05
  GPU Memory Used: 11985MB

[40:34:09] Step 105650/125000:
  Training Loss: 1.402
  Perplexity: 4.063
  Learning Rate: 1.16e-05
  GPU Memory Used: 11859MB

[40:35:19] Step 105700/125000:
  Training Loss: 1.448
  Perplexity: 4.255
  Learning Rate: 1.15e-05
  GPU Memory Used: 12011MB

[40:36:28] Step 105750/125000:
  Training Loss: 1.291
  Perplexity: 3.638
  Learning Rate: 1.15e-05
  GPU Memory Used: 12093MB

[40:37:38] Step 105800/125000:
  Training Loss: 1.376
  Perplexity: 3.959
  Learning Rate: 1.14e-05
  GPU Memory Used: 11966MB

[40:38:46] Step 105850/125000:
  Training Loss: 1.369
  Perplexity: 3.931
  Learning Rate: 1.14e-05
  GPU Memory Used: 12079MB

[40:39:56] Step 105900/125000:
  Training Loss: 1.331
  Perplexity: 3.785
  Learning Rate: 1.13e-05
  GPU Memory Used: 11836MB

[40:41:06] Step 105950/125000:
  Training Loss: 1.346
  Perplexity: 3.842
  Learning Rate: 1.12e-05
  GPU Memory Used: 11997MB

[40:42:15] Step 106000/125000:
  Training Loss: 1.360
  Perplexity: 3.895
  Learning Rate: 1.12e-05
  GPU Memory Used: 12189MB


Evaluation Results:
  Validation Loss: 1.494
  Validation Perplexity: 4.456

Saving model checkpoint: proposal_model/checkpoint-106000


Epoch 213/250
==================================================
[40:43:24] Step 106050/125000:
  Training Loss: 1.366
  Perplexity: 3.920
  Learning Rate: 1.11e-05
  GPU Memory Used: 11865MB

[40:44:35] Step 106100/125000:
  Training Loss: 1.341
  Perplexity: 3.821
  Learning Rate: 1.11e-05
  GPU Memory Used: 11821MB

[40:45:45] Step 106150/125000:
  Training Loss: 1.380
  Perplexity: 3.974
  Learning Rate: 1.10e-05
  GPU Memory Used: 12150MB

[40:46:54] Step 106200/125000:
  Training Loss: 1.342
  Perplexity: 3.825
  Learning Rate: 1.10e-05
  GPU Memory Used: 12036MB

[40:48:06] Step 106250/125000:
  Training Loss: 1.423
  Perplexity: 4.150
  Learning Rate: 1.09e-05
  GPU Memory Used: 11965MB

[40:49:15] Step 106300/125000:
  Training Loss: 1.383
  Perplexity: 3.985
  Learning Rate: 1.08e-05
  GPU Memory Used: 12029MB

[40:50:24] Step 106350/125000:
  Training Loss: 1.370
  Perplexity: 3.933
  Learning Rate: 1.08e-05
  GPU Memory Used: 12070MB

[40:51:33] Step 106400/125000:
  Training Loss: 1.385
  Perplexity: 3.994
  Learning Rate: 1.07e-05
  GPU Memory Used: 11980MB

[40:52:43] Step 106450/125000:
  Training Loss: 1.386
  Perplexity: 3.999
  Learning Rate: 1.07e-05
  GPU Memory Used: 11952MB

[40:53:52] Step 106500/125000:
  Training Loss: 1.373
  Perplexity: 3.946
  Learning Rate: 1.06e-05
  GPU Memory Used: 12077MB


Evaluation Results:
  Validation Loss: 1.557
  Validation Perplexity: 4.746

Saving model checkpoint: proposal_model/checkpoint-106500


Epoch 214/250
==================================================
[40:55:01] Step 106550/125000:
  Training Loss: 1.332
  Perplexity: 3.788
  Learning Rate: 1.06e-05
  GPU Memory Used: 12116MB

[40:56:11] Step 106600/125000:
  Training Loss: 1.336
  Perplexity: 3.804
  Learning Rate: 1.05e-05
  GPU Memory Used: 12086MB

[40:57:18] Step 106650/125000:
  Training Loss: 1.365
  Perplexity: 3.916
  Learning Rate: 1.04e-05
  GPU Memory Used: 11819MB

[40:58:28] Step 106700/125000:
  Training Loss: 1.379
  Perplexity: 3.969
  Learning Rate: 1.04e-05
  GPU Memory Used: 11833MB

[40:59:37] Step 106750/125000:
  Training Loss: 1.335
  Perplexity: 3.801
  Learning Rate: 1.03e-05
  GPU Memory Used: 11860MB

[41:00:47] Step 106800/125000:
  Training Loss: 1.314
  Perplexity: 3.720
  Learning Rate: 1.03e-05
  GPU Memory Used: 11965MB

[41:01:55] Step 106850/125000:
  Training Loss: 1.339
  Perplexity: 3.816
  Learning Rate: 1.02e-05
  GPU Memory Used: 11994MB

[41:03:05] Step 106900/125000:
  Training Loss: 1.353
  Perplexity: 3.871
  Learning Rate: 1.02e-05
  GPU Memory Used: 12100MB

[41:04:14] Step 106950/125000:
  Training Loss: 1.417
  Perplexity: 4.126
  Learning Rate: 1.01e-05
  GPU Memory Used: 12090MB

[41:05:21] Step 107000/125000:
  Training Loss: 1.401
  Perplexity: 4.059
  Learning Rate: 1.01e-05
  GPU Memory Used: 11842MB


Evaluation Results:
  Validation Loss: 1.480
  Validation Perplexity: 4.393

Saving model checkpoint: proposal_model/checkpoint-107000


Epoch 215/250
==================================================
[41:06:30] Step 107050/125000:
  Training Loss: 1.327
  Perplexity: 3.770
  Learning Rate: 1.00e-05
  GPU Memory Used: 11853MB

[41:07:41] Step 107100/125000:
  Training Loss: 1.383
  Perplexity: 3.987
  Learning Rate: 9.95e-06
  GPU Memory Used: 11823MB

[41:08:50] Step 107150/125000:
  Training Loss: 1.340
  Perplexity: 3.818
  Learning Rate: 9.90e-06
  GPU Memory Used: 11944MB

[41:10:02] Step 107200/125000:
  Training Loss: 1.374
  Perplexity: 3.952
  Learning Rate: 9.84e-06
  GPU Memory Used: 12012MB

[41:11:11] Step 107250/125000:
  Training Loss: 1.406
  Perplexity: 4.080
  Learning Rate: 9.79e-06
  GPU Memory Used: 11941MB

[41:12:20] Step 107300/125000:
  Training Loss: 1.362
  Perplexity: 3.903
  Learning Rate: 9.73e-06
  GPU Memory Used: 11889MB

[41:13:30] Step 107350/125000:
  Training Loss: 1.375
  Perplexity: 3.954
  Learning Rate: 9.68e-06
  GPU Memory Used: 11928MB

[41:14:41] Step 107400/125000:
  Training Loss: 1.425
  Perplexity: 4.157
  Learning Rate: 9.62e-06
  GPU Memory Used: 11935MB

[41:15:48] Step 107450/125000:
  Training Loss: 1.412
  Perplexity: 4.106
  Learning Rate: 9.57e-06
  GPU Memory Used: 12085MB

[41:16:57] Step 107500/125000:
  Training Loss: 1.378
  Perplexity: 3.966
  Learning Rate: 9.52e-06
  GPU Memory Used: 12134MB


Evaluation Results:
  Validation Loss: 1.469
  Validation Perplexity: 4.345

Saving model checkpoint: proposal_model/checkpoint-107500


Epoch 216/250
==================================================
[41:18:05] Step 107550/125000:
  Training Loss: 1.400
  Perplexity: 4.055
  Learning Rate: 9.46e-06
  GPU Memory Used: 11916MB

[41:19:15] Step 107600/125000:
  Training Loss: 1.357
  Perplexity: 3.884
  Learning Rate: 9.41e-06
  GPU Memory Used: 11982MB

[41:20:24] Step 107650/125000:
  Training Loss: 1.324
  Perplexity: 3.757
  Learning Rate: 9.36e-06
  GPU Memory Used: 12157MB

[41:21:33] Step 107700/125000:
  Training Loss: 1.358
  Perplexity: 3.889
  Learning Rate: 9.30e-06
  GPU Memory Used: 12005MB

[41:22:41] Step 107750/125000:
  Training Loss: 1.331
  Perplexity: 3.786
  Learning Rate: 9.25e-06
  GPU Memory Used: 11985MB

[41:23:51] Step 107800/125000:
  Training Loss: 1.384
  Perplexity: 3.992
  Learning Rate: 9.20e-06
  GPU Memory Used: 11899MB

[41:24:59] Step 107850/125000:
  Training Loss: 1.355
  Perplexity: 3.878
  Learning Rate: 9.15e-06
  GPU Memory Used: 12012MB

[41:26:06] Step 107900/125000:
  Training Loss: 1.404
  Perplexity: 4.071
  Learning Rate: 9.09e-06
  GPU Memory Used: 12019MB

[41:27:13] Step 107950/125000:
  Training Loss: 1.358
  Perplexity: 3.889
  Learning Rate: 9.04e-06
  GPU Memory Used: 11934MB

[41:28:22] Step 108000/125000:
  Training Loss: 1.316
  Perplexity: 3.728
  Learning Rate: 8.99e-06
  GPU Memory Used: 11826MB


Evaluation Results:
  Validation Loss: 1.504
  Validation Perplexity: 4.500

Saving model checkpoint: proposal_model/checkpoint-108000


Epoch 217/250
==================================================
[41:29:32] Step 108050/125000:
  Training Loss: 1.406
  Perplexity: 4.079
  Learning Rate: 8.94e-06
  GPU Memory Used: 12181MB

[41:30:41] Step 108100/125000:
  Training Loss: 1.358
  Perplexity: 3.888
  Learning Rate: 8.89e-06
  GPU Memory Used: 12025MB

[41:31:51] Step 108150/125000:
  Training Loss: 1.402
  Perplexity: 4.064
  Learning Rate: 8.83e-06
  GPU Memory Used: 11987MB

[41:33:01] Step 108200/125000:
  Training Loss: 1.322
  Perplexity: 3.752
  Learning Rate: 8.78e-06
  GPU Memory Used: 11979MB

[41:34:08] Step 108250/125000:
  Training Loss: 1.390
  Perplexity: 4.015
  Learning Rate: 8.73e-06
  GPU Memory Used: 12104MB

[41:35:18] Step 108300/125000:
  Training Loss: 1.392
  Perplexity: 4.022
  Learning Rate: 8.68e-06
  GPU Memory Used: 12098MB

[41:36:27] Step 108350/125000:
  Training Loss: 1.335
  Perplexity: 3.799
  Learning Rate: 8.63e-06
  GPU Memory Used: 12125MB

[41:37:36] Step 108400/125000:
  Training Loss: 1.341
  Perplexity: 3.822
  Learning Rate: 8.58e-06
  GPU Memory Used: 11862MB

[41:38:45] Step 108450/125000:
  Training Loss: 1.360
  Perplexity: 3.898
  Learning Rate: 8.53e-06
  GPU Memory Used: 12184MB

[41:39:53] Step 108500/125000:
  Training Loss: 1.325
  Perplexity: 3.762
  Learning Rate: 8.48e-06
  GPU Memory Used: 12049MB


Evaluation Results:
  Validation Loss: 1.488
  Validation Perplexity: 4.427

Saving model checkpoint: proposal_model/checkpoint-108500


Epoch 218/250
==================================================
[41:41:04] Step 108550/125000:
  Training Loss: 1.395
  Perplexity: 4.037
  Learning Rate: 8.43e-06
  GPU Memory Used: 12057MB

[41:42:14] Step 108600/125000:
  Training Loss: 1.390
  Perplexity: 4.016
  Learning Rate: 8.37e-06
  GPU Memory Used: 12166MB

[41:43:23] Step 108650/125000:
  Training Loss: 1.332
  Perplexity: 3.790
  Learning Rate: 8.32e-06
  GPU Memory Used: 11924MB

[41:44:34] Step 108700/125000:
  Training Loss: 1.386
  Perplexity: 3.999
  Learning Rate: 8.27e-06
  GPU Memory Used: 12120MB

[41:45:44] Step 108750/125000:
  Training Loss: 1.338
  Perplexity: 3.810
  Learning Rate: 8.22e-06
  GPU Memory Used: 11849MB

[41:46:54] Step 108800/125000:
  Training Loss: 1.345
  Perplexity: 3.838
  Learning Rate: 8.17e-06
  GPU Memory Used: 12060MB

[41:48:04] Step 108850/125000:
  Training Loss: 1.326
  Perplexity: 3.766
  Learning Rate: 8.13e-06
  GPU Memory Used: 11927MB

[41:49:14] Step 108900/125000:
  Training Loss: 1.337
  Perplexity: 3.807
  Learning Rate: 8.08e-06
  GPU Memory Used: 11828MB

[41:50:22] Step 108950/125000:
  Training Loss: 1.362
  Perplexity: 3.903
  Learning Rate: 8.03e-06
  GPU Memory Used: 11995MB

[41:51:31] Step 109000/125000:
  Training Loss: 1.415
  Perplexity: 4.115
  Learning Rate: 7.98e-06
  GPU Memory Used: 11958MB


Evaluation Results:
  Validation Loss: 1.460
  Validation Perplexity: 4.305

Saving model checkpoint: proposal_model/checkpoint-109000


Epoch 219/250
==================================================
[41:52:42] Step 109050/125000:
  Training Loss: 1.352
  Perplexity: 3.864
  Learning Rate: 7.93e-06
  GPU Memory Used: 12147MB

[41:53:50] Step 109100/125000:
  Training Loss: 1.352
  Perplexity: 3.866
  Learning Rate: 7.88e-06
  GPU Memory Used: 12014MB

[41:54:58] Step 109150/125000:
  Training Loss: 1.383
  Perplexity: 3.987
  Learning Rate: 7.83e-06
  GPU Memory Used: 11998MB

[41:56:08] Step 109200/125000:
  Training Loss: 1.412
  Perplexity: 4.103
  Learning Rate: 7.78e-06
  GPU Memory Used: 12168MB

[41:57:16] Step 109250/125000:
  Training Loss: 1.333
  Perplexity: 3.794
  Learning Rate: 7.73e-06
  GPU Memory Used: 12194MB

[41:58:27] Step 109300/125000:
  Training Loss: 1.393
  Perplexity: 4.026
  Learning Rate: 7.68e-06
  GPU Memory Used: 12184MB

[41:59:37] Step 109350/125000:
  Training Loss: 1.385
  Perplexity: 3.995
  Learning Rate: 7.64e-06
  GPU Memory Used: 12040MB

[42:00:48] Step 109400/125000:
  Training Loss: 1.355
  Perplexity: 3.876
  Learning Rate: 7.59e-06
  GPU Memory Used: 11936MB

[42:01:57] Step 109450/125000:
  Training Loss: 1.363
  Perplexity: 3.909
  Learning Rate: 7.54e-06
  GPU Memory Used: 11917MB

[42:03:07] Step 109500/125000:
  Training Loss: 1.400
  Perplexity: 4.055
  Learning Rate: 7.49e-06
  GPU Memory Used: 11954MB


Evaluation Results:
  Validation Loss: 1.478
  Validation Perplexity: 4.382

Saving model checkpoint: proposal_model/checkpoint-109500


Epoch 220/250
==================================================
[42:04:18] Step 109550/125000:
  Training Loss: 1.352
  Perplexity: 3.866
  Learning Rate: 7.44e-06
  GPU Memory Used: 11858MB

[42:05:27] Step 109600/125000:
  Training Loss: 1.313
  Perplexity: 3.717
  Learning Rate: 7.40e-06
  GPU Memory Used: 12030MB

[42:06:35] Step 109650/125000:
  Training Loss: 1.326
  Perplexity: 3.765
  Learning Rate: 7.35e-06
  GPU Memory Used: 11820MB

[42:07:45] Step 109700/125000:
  Training Loss: 1.309
  Perplexity: 3.703
  Learning Rate: 7.30e-06
  GPU Memory Used: 11997MB

[42:08:54] Step 109750/125000:
  Training Loss: 1.374
  Perplexity: 3.950
  Learning Rate: 7.26e-06
  GPU Memory Used: 11873MB

[42:10:02] Step 109800/125000:
  Training Loss: 1.378
  Perplexity: 3.965
  Learning Rate: 7.21e-06
  GPU Memory Used: 11922MB

[42:11:11] Step 109850/125000:
  Training Loss: 1.388
  Perplexity: 4.005
  Learning Rate: 7.16e-06
  GPU Memory Used: 11964MB

[42:12:21] Step 109900/125000:
  Training Loss: 1.419
  Perplexity: 4.132
  Learning Rate: 7.12e-06
  GPU Memory Used: 12051MB

[42:13:31] Step 109950/125000:
  Training Loss: 1.400
  Perplexity: 4.057
  Learning Rate: 7.07e-06
  GPU Memory Used: 11814MB

[42:14:39] Step 110000/125000:
  Training Loss: 1.364
  Perplexity: 3.913
  Learning Rate: 7.02e-06
  GPU Memory Used: 12186MB


Evaluation Results:
  Validation Loss: 1.532
  Validation Perplexity: 4.628

Saving model checkpoint: proposal_model/checkpoint-110000


Epoch 221/250
==================================================
[42:15:47] Step 110050/125000:
  Training Loss: 1.357
  Perplexity: 3.883
  Learning Rate: 6.98e-06
  GPU Memory Used: 12109MB

[42:16:56] Step 110100/125000:
  Training Loss: 1.341
  Perplexity: 3.825
  Learning Rate: 6.93e-06
  GPU Memory Used: 11861MB

[42:18:07] Step 110150/125000:
  Training Loss: 1.356
  Perplexity: 3.879
  Learning Rate: 6.88e-06
  GPU Memory Used: 11963MB

[42:19:16] Step 110200/125000:
  Training Loss: 1.413
  Perplexity: 4.106
  Learning Rate: 6.84e-06
  GPU Memory Used: 11942MB

[42:20:25] Step 110250/125000:
  Training Loss: 1.369
  Perplexity: 3.930
  Learning Rate: 6.79e-06
  GPU Memory Used: 12069MB

[42:21:34] Step 110300/125000:
  Training Loss: 1.359
  Perplexity: 3.891
  Learning Rate: 6.75e-06
  GPU Memory Used: 12142MB

[42:22:41] Step 110350/125000:
  Training Loss: 1.334
  Perplexity: 3.795
  Learning Rate: 6.70e-06
  GPU Memory Used: 11882MB

[42:23:49] Step 110400/125000:
  Training Loss: 1.338
  Perplexity: 3.810
  Learning Rate: 6.66e-06
  GPU Memory Used: 11870MB

[42:24:59] Step 110450/125000:
  Training Loss: 1.336
  Perplexity: 3.803
  Learning Rate: 6.61e-06
  GPU Memory Used: 12090MB

[42:26:10] Step 110500/125000:
  Training Loss: 1.362
  Perplexity: 3.905
  Learning Rate: 6.57e-06
  GPU Memory Used: 12043MB


Evaluation Results:
  Validation Loss: 1.496
  Validation Perplexity: 4.465

Saving model checkpoint: proposal_model/checkpoint-110500


Epoch 222/250
==================================================
[42:27:20] Step 110550/125000:
  Training Loss: 1.364
  Perplexity: 3.912
  Learning Rate: 6.52e-06
  GPU Memory Used: 11901MB

[42:28:28] Step 110600/125000:
  Training Loss: 1.393
  Perplexity: 4.025
  Learning Rate: 6.48e-06
  GPU Memory Used: 11820MB

[42:29:38] Step 110650/125000:
  Training Loss: 1.336
  Perplexity: 3.803
  Learning Rate: 6.43e-06
  GPU Memory Used: 11812MB

[42:30:46] Step 110700/125000:
  Training Loss: 1.381
  Perplexity: 3.978
  Learning Rate: 6.39e-06
  GPU Memory Used: 11893MB

[42:31:56] Step 110750/125000:
  Training Loss: 1.360
  Perplexity: 3.896
  Learning Rate: 6.35e-06
  GPU Memory Used: 12170MB

[42:33:04] Step 110800/125000:
  Training Loss: 1.369
  Perplexity: 3.930
  Learning Rate: 6.30e-06
  GPU Memory Used: 12181MB

[42:34:14] Step 110850/125000:
  Training Loss: 1.369
  Perplexity: 3.932
  Learning Rate: 6.26e-06
  GPU Memory Used: 12061MB

[42:35:24] Step 110900/125000:
  Training Loss: 1.378
  Perplexity: 3.968
  Learning Rate: 6.21e-06
  GPU Memory Used: 11942MB

[42:36:33] Step 110950/125000:
  Training Loss: 1.406
  Perplexity: 4.080
  Learning Rate: 6.17e-06
  GPU Memory Used: 11855MB

[42:37:43] Step 111000/125000:
  Training Loss: 1.346
  Perplexity: 3.841
  Learning Rate: 6.13e-06
  GPU Memory Used: 11839MB


Evaluation Results:
  Validation Loss: 1.494
  Validation Perplexity: 4.456

Saving model checkpoint: proposal_model/checkpoint-111000


Epoch 223/250
==================================================
[42:38:50] Step 111050/125000:
  Training Loss: 1.391
  Perplexity: 4.018
  Learning Rate: 6.08e-06
  GPU Memory Used: 12038MB

[42:39:58] Step 111100/125000:
  Training Loss: 1.285
  Perplexity: 3.615
  Learning Rate: 6.04e-06
  GPU Memory Used: 12102MB

[42:41:05] Step 111150/125000:
  Training Loss: 1.385
  Perplexity: 3.997
  Learning Rate: 6.00e-06
  GPU Memory Used: 12156MB

[42:42:13] Step 111200/125000:
  Training Loss: 1.319
  Perplexity: 3.741
  Learning Rate: 5.95e-06
  GPU Memory Used: 11988MB

[42:43:25] Step 111250/125000:
  Training Loss: 1.350
  Perplexity: 3.859
  Learning Rate: 5.91e-06
  GPU Memory Used: 12064MB

[42:44:34] Step 111300/125000:
  Training Loss: 1.328
  Perplexity: 3.772
  Learning Rate: 5.87e-06
  GPU Memory Used: 12102MB

[42:45:42] Step 111350/125000:
  Training Loss: 1.330
  Perplexity: 3.779
  Learning Rate: 5.83e-06
  GPU Memory Used: 12118MB

[42:46:52] Step 111400/125000:
  Training Loss: 1.360
  Perplexity: 3.895
  Learning Rate: 5.78e-06
  GPU Memory Used: 12078MB

[42:48:00] Step 111450/125000:
  Training Loss: 1.362
  Perplexity: 3.903
  Learning Rate: 5.74e-06
  GPU Memory Used: 11968MB

[42:49:09] Step 111500/125000:
  Training Loss: 1.401
  Perplexity: 4.059
  Learning Rate: 5.70e-06
  GPU Memory Used: 11912MB


Evaluation Results:
  Validation Loss: 1.504
  Validation Perplexity: 4.499

Saving model checkpoint: proposal_model/checkpoint-111500


Epoch 224/250
==================================================
[42:50:16] Step 111550/125000:
  Training Loss: 1.409
  Perplexity: 4.090
  Learning Rate: 5.66e-06
  GPU Memory Used: 11901MB

[42:51:26] Step 111600/125000:
  Training Loss: 1.367
  Perplexity: 3.924
  Learning Rate: 5.62e-06
  GPU Memory Used: 12158MB

[42:52:35] Step 111650/125000:
  Training Loss: 1.293
  Perplexity: 3.643
  Learning Rate: 5.58e-06
  GPU Memory Used: 11958MB

[42:53:44] Step 111700/125000:
  Training Loss: 1.316
  Perplexity: 3.729
  Learning Rate: 5.53e-06
  GPU Memory Used: 11979MB

[42:54:54] Step 111750/125000:
  Training Loss: 1.360
  Perplexity: 3.895
  Learning Rate: 5.49e-06
  GPU Memory Used: 11910MB

[42:56:03] Step 111800/125000:
  Training Loss: 1.387
  Perplexity: 4.004
  Learning Rate: 5.45e-06
  GPU Memory Used: 11919MB

[42:57:14] Step 111850/125000:
  Training Loss: 1.339
  Perplexity: 3.815
  Learning Rate: 5.41e-06
  GPU Memory Used: 12066MB

[42:58:24] Step 111900/125000:
  Training Loss: 1.353
  Perplexity: 3.868
  Learning Rate: 5.37e-06
  GPU Memory Used: 11856MB

[42:59:35] Step 111950/125000:
  Training Loss: 1.361
  Perplexity: 3.898
  Learning Rate: 5.33e-06
  GPU Memory Used: 11971MB

[43:00:46] Step 112000/125000:
  Training Loss: 1.359
  Perplexity: 3.891
  Learning Rate: 5.29e-06
  GPU Memory Used: 11812MB


Evaluation Results:
  Validation Loss: 1.434
  Validation Perplexity: 4.194

Saving model checkpoint: proposal_model/checkpoint-112000


Epoch 225/250
==================================================
[43:01:56] Step 112050/125000:
  Training Loss: 1.356
  Perplexity: 3.882
  Learning Rate: 5.25e-06
  GPU Memory Used: 11921MB

[43:03:07] Step 112100/125000:
  Training Loss: 1.393
  Perplexity: 4.028
  Learning Rate: 5.21e-06
  GPU Memory Used: 12133MB

[43:04:17] Step 112150/125000:
  Training Loss: 1.377
  Perplexity: 3.962
  Learning Rate: 5.17e-06
  GPU Memory Used: 11862MB

[43:05:24] Step 112200/125000:
  Training Loss: 1.403
  Perplexity: 4.068
  Learning Rate: 5.13e-06
  GPU Memory Used: 12178MB

[43:06:33] Step 112250/125000:
  Training Loss: 1.287
  Perplexity: 3.620
  Learning Rate: 5.09e-06
  GPU Memory Used: 11981MB

[43:07:43] Step 112300/125000:
  Training Loss: 1.350
  Perplexity: 3.856
  Learning Rate: 5.05e-06
  GPU Memory Used: 12007MB

[43:08:51] Step 112350/125000:
  Training Loss: 1.357
  Perplexity: 3.886
  Learning Rate: 5.01e-06
  GPU Memory Used: 12198MB

[43:10:01] Step 112400/125000:
  Training Loss: 1.336
  Perplexity: 3.803
  Learning Rate: 4.97e-06
  GPU Memory Used: 11835MB

[43:11:07] Step 112450/125000:
  Training Loss: 1.373
  Perplexity: 3.945
  Learning Rate: 4.93e-06
  GPU Memory Used: 11914MB

[43:12:16] Step 112500/125000:
  Training Loss: 1.366
  Perplexity: 3.919
  Learning Rate: 4.89e-06
  GPU Memory Used: 12193MB


Evaluation Results:
  Validation Loss: 1.520
  Validation Perplexity: 4.573

Saving model checkpoint: proposal_model/checkpoint-112500


Epoch 226/250
==================================================
[43:13:23] Step 112550/125000:
  Training Loss: 1.392
  Perplexity: 4.024
  Learning Rate: 4.86e-06
  GPU Memory Used: 12045MB

[43:14:33] Step 112600/125000:
  Training Loss: 1.389
  Perplexity: 4.012
  Learning Rate: 4.82e-06
  GPU Memory Used: 11953MB

[43:15:41] Step 112650/125000:
  Training Loss: 1.316
  Perplexity: 3.727
  Learning Rate: 4.78e-06
  GPU Memory Used: 12042MB

[43:16:51] Step 112700/125000:
  Training Loss: 1.336
  Perplexity: 3.804
  Learning Rate: 4.74e-06
  GPU Memory Used: 12044MB

[43:18:03] Step 112750/125000:
  Training Loss: 1.308
  Perplexity: 3.698
  Learning Rate: 4.70e-06
  GPU Memory Used: 11909MB

[43:19:11] Step 112800/125000:
  Training Loss: 1.287
  Perplexity: 3.623
  Learning Rate: 4.66e-06
  GPU Memory Used: 11842MB

[43:20:19] Step 112850/125000:
  Training Loss: 1.341
  Perplexity: 3.823
  Learning Rate: 4.63e-06
  GPU Memory Used: 11802MB

[43:21:29] Step 112900/125000:
  Training Loss: 1.336
  Perplexity: 3.803
  Learning Rate: 4.59e-06
  GPU Memory Used: 12058MB

[43:22:38] Step 112950/125000:
  Training Loss: 1.316
  Perplexity: 3.727
  Learning Rate: 4.55e-06
  GPU Memory Used: 12178MB

[43:23:46] Step 113000/125000:
  Training Loss: 1.347
  Perplexity: 3.845
  Learning Rate: 4.51e-06
  GPU Memory Used: 11996MB


Evaluation Results:
  Validation Loss: 1.535
  Validation Perplexity: 4.641

Saving model checkpoint: proposal_model/checkpoint-113000


Epoch 227/250
==================================================
[43:24:55] Step 113050/125000:
  Training Loss: 1.379
  Perplexity: 3.973
  Learning Rate: 4.48e-06
  GPU Memory Used: 11862MB

[43:26:04] Step 113100/125000:
  Training Loss: 1.332
  Perplexity: 3.788
  Learning Rate: 4.44e-06
  GPU Memory Used: 12062MB

[43:27:15] Step 113150/125000:
  Training Loss: 1.392
  Perplexity: 4.023
  Learning Rate: 4.40e-06
  GPU Memory Used: 11943MB

[43:28:23] Step 113200/125000:
  Training Loss: 1.326
  Perplexity: 3.766
  Learning Rate: 4.37e-06
  GPU Memory Used: 11827MB

[43:29:32] Step 113250/125000:
  Training Loss: 1.361
  Perplexity: 3.902
  Learning Rate: 4.33e-06
  GPU Memory Used: 12041MB

[43:30:41] Step 113300/125000:
  Training Loss: 1.369
  Perplexity: 3.933
  Learning Rate: 4.29e-06
  GPU Memory Used: 11913MB

[43:31:50] Step 113350/125000:
  Training Loss: 1.322
  Perplexity: 3.751
  Learning Rate: 4.26e-06
  GPU Memory Used: 12106MB

[43:33:00] Step 113400/125000:
  Training Loss: 1.344
  Perplexity: 3.835
  Learning Rate: 4.22e-06
  GPU Memory Used: 12000MB

[43:34:11] Step 113450/125000:
  Training Loss: 1.351
  Perplexity: 3.860
  Learning Rate: 4.18e-06
  GPU Memory Used: 12013MB

[43:35:19] Step 113500/125000:
  Training Loss: 1.330
  Perplexity: 3.780
  Learning Rate: 4.15e-06
  GPU Memory Used: 11923MB


Evaluation Results:
  Validation Loss: 1.493
  Validation Perplexity: 4.450

Saving model checkpoint: proposal_model/checkpoint-113500


Epoch 228/250
==================================================
[43:36:27] Step 113550/125000:
  Training Loss: 1.363
  Perplexity: 3.910
  Learning Rate: 4.11e-06
  GPU Memory Used: 11976MB

[43:37:36] Step 113600/125000:
  Training Loss: 1.345
  Perplexity: 3.839
  Learning Rate: 4.08e-06
  GPU Memory Used: 12075MB

[43:38:44] Step 113650/125000:
  Training Loss: 1.302
  Perplexity: 3.676
  Learning Rate: 4.04e-06
  GPU Memory Used: 11937MB

[43:39:53] Step 113700/125000:
  Training Loss: 1.325
  Perplexity: 3.761
  Learning Rate: 4.01e-06
  GPU Memory Used: 11952MB

[43:41:03] Step 113750/125000:
  Training Loss: 1.341
  Perplexity: 3.822
  Learning Rate: 3.97e-06
  GPU Memory Used: 12164MB

[43:42:11] Step 113800/125000:
  Training Loss: 1.327
  Perplexity: 3.768
  Learning Rate: 3.94e-06
  GPU Memory Used: 11993MB

[43:43:21] Step 113850/125000:
  Training Loss: 1.368
  Perplexity: 3.927
  Learning Rate: 3.90e-06
  GPU Memory Used: 12155MB

[43:44:31] Step 113900/125000:
  Training Loss: 1.333
  Perplexity: 3.793
  Learning Rate: 3.87e-06
  GPU Memory Used: 11837MB

[43:45:38] Step 113950/125000:
  Training Loss: 1.343
  Perplexity: 3.831
  Learning Rate: 3.83e-06
  GPU Memory Used: 12182MB

[43:46:49] Step 114000/125000:
  Training Loss: 1.341
  Perplexity: 3.821
  Learning Rate: 3.80e-06
  GPU Memory Used: 11919MB


Evaluation Results:
  Validation Loss: 1.483
  Validation Perplexity: 4.404

Saving model checkpoint: proposal_model/checkpoint-114000


Epoch 229/250
==================================================
[43:47:59] Step 114050/125000:
  Training Loss: 1.362
  Perplexity: 3.906
  Learning Rate: 3.76e-06
  GPU Memory Used: 12091MB

[43:49:07] Step 114100/125000:
  Training Loss: 1.324
  Perplexity: 3.757
  Learning Rate: 3.73e-06
  GPU Memory Used: 12075MB

[43:50:17] Step 114150/125000:
  Training Loss: 1.325
  Perplexity: 3.762
  Learning Rate: 3.70e-06
  GPU Memory Used: 12187MB

[43:51:28] Step 114200/125000:
  Training Loss: 1.352
  Perplexity: 3.863
  Learning Rate: 3.66e-06
  GPU Memory Used: 11895MB

[43:52:36] Step 114250/125000:
  Training Loss: 1.392
  Perplexity: 4.025
  Learning Rate: 3.63e-06
  GPU Memory Used: 11969MB

[43:53:45] Step 114300/125000:
  Training Loss: 1.360
  Perplexity: 3.898
  Learning Rate: 3.59e-06
  GPU Memory Used: 11955MB

[43:54:55] Step 114350/125000:
  Training Loss: 1.350
  Perplexity: 3.859
  Learning Rate: 3.56e-06
  GPU Memory Used: 12059MB

[43:56:02] Step 114400/125000:
  Training Loss: 1.318
  Perplexity: 3.736
  Learning Rate: 3.53e-06
  GPU Memory Used: 11934MB

[43:57:12] Step 114450/125000:
  Training Loss: 1.337
  Perplexity: 3.809
  Learning Rate: 3.49e-06
  GPU Memory Used: 12020MB

[43:58:20] Step 114500/125000:
  Training Loss: 1.375
  Perplexity: 3.954
  Learning Rate: 3.46e-06
  GPU Memory Used: 11869MB


Evaluation Results:
  Validation Loss: 1.455
  Validation Perplexity: 4.286

Saving model checkpoint: proposal_model/checkpoint-114500


Epoch 230/250
==================================================
[43:59:29] Step 114550/125000:
  Training Loss: 1.312
  Perplexity: 3.713
  Learning Rate: 3.43e-06
  GPU Memory Used: 11837MB

[44:00:38] Step 114600/125000:
  Training Loss: 1.334
  Perplexity: 3.797
  Learning Rate: 3.40e-06
  GPU Memory Used: 11864MB

[44:01:47] Step 114650/125000:
  Training Loss: 1.332
  Perplexity: 3.789
  Learning Rate: 3.36e-06
  GPU Memory Used: 11972MB

[44:02:57] Step 114700/125000:
  Training Loss: 1.315
  Perplexity: 3.724
  Learning Rate: 3.33e-06
  GPU Memory Used: 11826MB

[44:04:05] Step 114750/125000:
  Training Loss: 1.348
  Perplexity: 3.851
  Learning Rate: 3.30e-06
  GPU Memory Used: 12134MB

[44:05:14] Step 114800/125000:
  Training Loss: 1.331
  Perplexity: 3.784
  Learning Rate: 3.27e-06
  GPU Memory Used: 11867MB

[44:06:23] Step 114850/125000:
  Training Loss: 1.326
  Perplexity: 3.767
  Learning Rate: 3.24e-06
  GPU Memory Used: 12100MB

[44:07:32] Step 114900/125000:
  Training Loss: 1.363
  Perplexity: 3.908
  Learning Rate: 3.20e-06
  GPU Memory Used: 11847MB

[44:08:42] Step 114950/125000:
  Training Loss: 1.358
  Perplexity: 3.888
  Learning Rate: 3.17e-06
  GPU Memory Used: 12033MB

[44:09:52] Step 115000/125000:
  Training Loss: 1.344
  Perplexity: 3.836
  Learning Rate: 3.14e-06
  GPU Memory Used: 11979MB


Evaluation Results:
  Validation Loss: 1.477
  Validation Perplexity: 4.381

Saving model checkpoint: proposal_model/checkpoint-115000


Epoch 231/250
==================================================
[44:11:00] Step 115050/125000:
  Training Loss: 1.368
  Perplexity: 3.928
  Learning Rate: 3.11e-06
  GPU Memory Used: 12044MB

[44:12:09] Step 115100/125000:
  Training Loss: 1.294
  Perplexity: 3.648
  Learning Rate: 3.08e-06
  GPU Memory Used: 11975MB

[44:13:17] Step 115150/125000:
  Training Loss: 1.329
  Perplexity: 3.776
  Learning Rate: 3.05e-06
  GPU Memory Used: 11953MB

[44:14:27] Step 115200/125000:
  Training Loss: 1.352
  Perplexity: 3.864
  Learning Rate: 3.02e-06
  GPU Memory Used: 12113MB

[44:15:34] Step 115250/125000:
  Training Loss: 1.301
  Perplexity: 3.672
  Learning Rate: 2.99e-06
  GPU Memory Used: 12146MB

[44:16:41] Step 115300/125000:
  Training Loss: 1.307
  Perplexity: 3.695
  Learning Rate: 2.96e-06
  GPU Memory Used: 11981MB

[44:17:47] Step 115350/125000:
  Training Loss: 1.369
  Perplexity: 3.930
  Learning Rate: 2.93e-06
  GPU Memory Used: 12146MB

[44:18:54] Step 115400/125000:
  Training Loss: 1.356
  Perplexity: 3.880
  Learning Rate: 2.90e-06
  GPU Memory Used: 12099MB

[44:20:02] Step 115450/125000:
  Training Loss: 1.282
  Perplexity: 3.602
  Learning Rate: 2.87e-06
  GPU Memory Used: 12161MB

[44:21:14] Step 115500/125000:
  Training Loss: 1.360
  Perplexity: 3.897
  Learning Rate: 2.84e-06
  GPU Memory Used: 12020MB


Evaluation Results:
  Validation Loss: 1.500
  Validation Perplexity: 4.484

Saving model checkpoint: proposal_model/checkpoint-115500


Epoch 232/250
==================================================
[44:22:24] Step 115550/125000:
  Training Loss: 1.371
  Perplexity: 3.941
  Learning Rate: 2.81e-06
  GPU Memory Used: 12155MB

[44:23:33] Step 115600/125000:
  Training Loss: 1.361
  Perplexity: 3.902
  Learning Rate: 2.78e-06
  GPU Memory Used: 11865MB

[44:24:42] Step 115650/125000:
  Training Loss: 1.359
  Perplexity: 3.891
  Learning Rate: 2.75e-06
  GPU Memory Used: 11911MB

[44:25:51] Step 115700/125000:
  Training Loss: 1.300
  Perplexity: 3.670
  Learning Rate: 2.72e-06
  GPU Memory Used: 12017MB

[44:27:00] Step 115750/125000:
  Training Loss: 1.356
  Perplexity: 3.880
  Learning Rate: 2.69e-06
  GPU Memory Used: 12161MB

[44:28:10] Step 115800/125000:
  Training Loss: 1.362
  Perplexity: 3.906
  Learning Rate: 2.66e-06
  GPU Memory Used: 11845MB

[44:29:18] Step 115850/125000:
  Training Loss: 1.315
  Perplexity: 3.726
  Learning Rate: 2.63e-06
  GPU Memory Used: 11934MB

[44:30:28] Step 115900/125000:
  Training Loss: 1.373
  Perplexity: 3.948
  Learning Rate: 2.60e-06
  GPU Memory Used: 11828MB

[44:31:36] Step 115950/125000:
  Training Loss: 1.331
  Perplexity: 3.785
  Learning Rate: 2.58e-06
  GPU Memory Used: 12014MB

[44:32:45] Step 116000/125000:
  Training Loss: 1.343
  Perplexity: 3.830
  Learning Rate: 2.55e-06
  GPU Memory Used: 11898MB


Evaluation Results:
  Validation Loss: 1.516
  Validation Perplexity: 4.555

Saving model checkpoint: proposal_model/checkpoint-116000


Epoch 233/250
==================================================
[44:33:50] Step 116050/125000:
  Training Loss: 1.308
  Perplexity: 3.701
  Learning Rate: 2.52e-06
  GPU Memory Used: 12105MB

[44:35:00] Step 116100/125000:
  Training Loss: 1.323
  Perplexity: 3.754
  Learning Rate: 2.49e-06
  GPU Memory Used: 11853MB

[44:36:08] Step 116150/125000:
  Training Loss: 1.365
  Perplexity: 3.914
  Learning Rate: 2.46e-06
  GPU Memory Used: 12100MB

[44:37:17] Step 116200/125000:
  Training Loss: 1.296
  Perplexity: 3.655
  Learning Rate: 2.44e-06
  GPU Memory Used: 12008MB

[44:38:26] Step 116250/125000:
  Training Loss: 1.335
  Perplexity: 3.800
  Learning Rate: 2.41e-06
  GPU Memory Used: 11808MB

[44:39:35] Step 116300/125000:
  Training Loss: 1.357
  Perplexity: 3.884
  Learning Rate: 2.38e-06
  GPU Memory Used: 12081MB

[44:40:44] Step 116350/125000:
  Training Loss: 1.358
  Perplexity: 3.887
  Learning Rate: 2.35e-06
  GPU Memory Used: 11914MB

[44:41:55] Step 116400/125000:
  Training Loss: 1.327
  Perplexity: 3.771
  Learning Rate: 2.33e-06
  GPU Memory Used: 11906MB

[44:43:05] Step 116450/125000:
  Training Loss: 1.279
  Perplexity: 3.592
  Learning Rate: 2.30e-06
  GPU Memory Used: 12189MB

[44:44:13] Step 116500/125000:
  Training Loss: 1.339
  Perplexity: 3.815
  Learning Rate: 2.27e-06
  GPU Memory Used: 12177MB


Evaluation Results:
  Validation Loss: 1.480
  Validation Perplexity: 4.391

Saving model checkpoint: proposal_model/checkpoint-116500


Epoch 234/250
==================================================
[44:45:22] Step 116550/125000:
  Training Loss: 1.363
  Perplexity: 3.909
  Learning Rate: 2.25e-06
  GPU Memory Used: 11974MB

[44:46:30] Step 116600/125000:
  Training Loss: 1.324
  Perplexity: 3.759
  Learning Rate: 2.22e-06
  GPU Memory Used: 12132MB

[44:47:38] Step 116650/125000:
  Training Loss: 1.355
  Perplexity: 3.876
  Learning Rate: 2.19e-06
  GPU Memory Used: 11888MB

[44:48:47] Step 116700/125000:
  Training Loss: 1.379
  Perplexity: 3.970
  Learning Rate: 2.17e-06
  GPU Memory Used: 12081MB

[44:49:54] Step 116750/125000:
  Training Loss: 1.331
  Perplexity: 3.786
  Learning Rate: 2.14e-06
  GPU Memory Used: 11828MB

[44:51:03] Step 116800/125000:
  Training Loss: 1.330
  Perplexity: 3.782
  Learning Rate: 2.12e-06
  GPU Memory Used: 11802MB

[44:52:12] Step 116850/125000:
  Training Loss: 1.329
  Perplexity: 3.777
  Learning Rate: 2.09e-06
  GPU Memory Used: 11818MB

[44:53:20] Step 116900/125000:
  Training Loss: 1.369
  Perplexity: 3.932
  Learning Rate: 2.06e-06
  GPU Memory Used: 12161MB

[44:54:29] Step 116950/125000:
  Training Loss: 1.331
  Perplexity: 3.786
  Learning Rate: 2.04e-06
  GPU Memory Used: 12138MB

[44:55:39] Step 117000/125000:
  Training Loss: 1.339
  Perplexity: 3.813
  Learning Rate: 2.01e-06
  GPU Memory Used: 12132MB


Evaluation Results:
  Validation Loss: 1.455
  Validation Perplexity: 4.286

Saving model checkpoint: proposal_model/checkpoint-117000


Epoch 235/250
==================================================
[44:56:49] Step 117050/125000:
  Training Loss: 1.302
  Perplexity: 3.678
  Learning Rate: 1.99e-06
  GPU Memory Used: 11892MB

[44:58:01] Step 117100/125000:
  Training Loss: 1.357
  Perplexity: 3.884
  Learning Rate: 1.96e-06
  GPU Memory Used: 11999MB

[44:59:11] Step 117150/125000:
  Training Loss: 1.366
  Perplexity: 3.920
  Learning Rate: 1.94e-06
  GPU Memory Used: 11946MB

[45:00:21] Step 117200/125000:
  Training Loss: 1.332
  Perplexity: 3.789
  Learning Rate: 1.92e-06
  GPU Memory Used: 11996MB

[45:01:29] Step 117250/125000:
  Training Loss: 1.325
  Perplexity: 3.763
  Learning Rate: 1.89e-06
  GPU Memory Used: 11941MB

[45:02:39] Step 117300/125000:
  Training Loss: 1.369
  Perplexity: 3.932
  Learning Rate: 1.87e-06
  GPU Memory Used: 11968MB

[45:03:46] Step 117350/125000:
  Training Loss: 1.392
  Perplexity: 4.021
  Learning Rate: 1.84e-06
  GPU Memory Used: 11994MB

[45:04:57] Step 117400/125000:
  Training Loss: 1.370
  Perplexity: 3.937
  Learning Rate: 1.82e-06
  GPU Memory Used: 11826MB

[45:06:07] Step 117450/125000:
  Training Loss: 1.338
  Perplexity: 3.813
  Learning Rate: 1.79e-06
  GPU Memory Used: 11851MB

[45:07:16] Step 117500/125000:
  Training Loss: 1.289
  Perplexity: 3.628
  Learning Rate: 1.77e-06
  GPU Memory Used: 11935MB


Evaluation Results:
  Validation Loss: 1.496
  Validation Perplexity: 4.464

Saving model checkpoint: proposal_model/checkpoint-117500


Epoch 236/250
==================================================
[45:08:26] Step 117550/125000:
  Training Loss: 1.339
  Perplexity: 3.817
  Learning Rate: 1.75e-06
  GPU Memory Used: 12055MB

[45:09:35] Step 117600/125000:
  Training Loss: 1.340
  Perplexity: 3.821
  Learning Rate: 1.72e-06
  GPU Memory Used: 11803MB

[45:10:43] Step 117650/125000:
  Training Loss: 1.348
  Perplexity: 3.848
  Learning Rate: 1.70e-06
  GPU Memory Used: 11945MB

[45:11:51] Step 117700/125000:
  Training Loss: 1.333
  Perplexity: 3.791
  Learning Rate: 1.68e-06
  GPU Memory Used: 12094MB

[45:13:00] Step 117750/125000:
  Training Loss: 1.354
  Perplexity: 3.874
  Learning Rate: 1.66e-06
  GPU Memory Used: 11840MB

[45:14:11] Step 117800/125000:
  Training Loss: 1.370
  Perplexity: 3.936
  Learning Rate: 1.63e-06
  GPU Memory Used: 11968MB

[45:15:21] Step 117850/125000:
  Training Loss: 1.353
  Perplexity: 3.868
  Learning Rate: 1.61e-06
  GPU Memory Used: 12192MB

[45:16:32] Step 117900/125000:
  Training Loss: 1.339
  Perplexity: 3.814
  Learning Rate: 1.59e-06
  GPU Memory Used: 11981MB

[45:17:42] Step 117950/125000:
  Training Loss: 1.295
  Perplexity: 3.653
  Learning Rate: 1.57e-06
  GPU Memory Used: 11879MB

[45:18:53] Step 118000/125000:
  Training Loss: 1.335
  Perplexity: 3.800
  Learning Rate: 1.54e-06
  GPU Memory Used: 11837MB


Evaluation Results:
  Validation Loss: 1.493
  Validation Perplexity: 4.451

Saving model checkpoint: proposal_model/checkpoint-118000


Epoch 237/250
==================================================
[45:20:03] Step 118050/125000:
  Training Loss: 1.332
  Perplexity: 3.787
  Learning Rate: 1.52e-06
  GPU Memory Used: 11912MB

[45:21:10] Step 118100/125000:
  Training Loss: 1.354
  Perplexity: 3.874
  Learning Rate: 1.50e-06
  GPU Memory Used: 11990MB

[45:22:19] Step 118150/125000:
  Training Loss: 1.317
  Perplexity: 3.734
  Learning Rate: 1.48e-06
  GPU Memory Used: 11995MB

[45:23:28] Step 118200/125000:
  Training Loss: 1.341
  Perplexity: 3.824
  Learning Rate: 1.46e-06
  GPU Memory Used: 12172MB

[45:24:37] Step 118250/125000:
  Training Loss: 1.322
  Perplexity: 3.752
  Learning Rate: 1.44e-06
  GPU Memory Used: 12016MB

[45:25:46] Step 118300/125000:
  Training Loss: 1.368
  Perplexity: 3.926
  Learning Rate: 1.41e-06
  GPU Memory Used: 12051MB

[45:26:56] Step 118350/125000:
  Training Loss: 1.342
  Perplexity: 3.826
  Learning Rate: 1.39e-06
  GPU Memory Used: 12107MB

[45:28:05] Step 118400/125000:
  Training Loss: 1.332
  Perplexity: 3.788
  Learning Rate: 1.37e-06
  GPU Memory Used: 11959MB

[45:29:14] Step 118450/125000:
  Training Loss: 1.335
  Perplexity: 3.798
  Learning Rate: 1.35e-06
  GPU Memory Used: 12019MB

[45:30:25] Step 118500/125000:
  Training Loss: 1.340
  Perplexity: 3.820
  Learning Rate: 1.33e-06
  GPU Memory Used: 11812MB


Evaluation Results:
  Validation Loss: 1.472
  Validation Perplexity: 4.359

Saving model checkpoint: proposal_model/checkpoint-118500


Epoch 238/250
==================================================
[45:31:35] Step 118550/125000:
  Training Loss: 1.307
  Perplexity: 3.696
  Learning Rate: 1.31e-06
  GPU Memory Used: 12001MB

[45:32:46] Step 118600/125000:
  Training Loss: 1.363
  Perplexity: 3.908
  Learning Rate: 1.29e-06
  GPU Memory Used: 11824MB

[45:33:54] Step 118650/125000:
  Training Loss: 1.353
  Perplexity: 3.871
  Learning Rate: 1.27e-06
  GPU Memory Used: 11917MB

[45:35:03] Step 118700/125000:
  Training Loss: 1.271
  Perplexity: 3.565
  Learning Rate: 1.25e-06
  GPU Memory Used: 12123MB

[45:36:12] Step 118750/125000:
  Training Loss: 1.319
  Perplexity: 3.738
  Learning Rate: 1.23e-06
  GPU Memory Used: 12033MB

[45:37:21] Step 118800/125000:
  Training Loss: 1.349
  Perplexity: 3.852
  Learning Rate: 1.21e-06
  GPU Memory Used: 12061MB

[45:38:29] Step 118850/125000:
  Training Loss: 1.327
  Perplexity: 3.769
  Learning Rate: 1.19e-06
  GPU Memory Used: 11936MB

[45:39:37] Step 118900/125000:
  Training Loss: 1.314
  Perplexity: 3.719
  Learning Rate: 1.17e-06
  GPU Memory Used: 12024MB

[45:40:46] Step 118950/125000:
  Training Loss: 1.364
  Perplexity: 3.913
  Learning Rate: 1.15e-06
  GPU Memory Used: 11896MB

[45:41:56] Step 119000/125000:
  Training Loss: 1.321
  Perplexity: 3.747
  Learning Rate: 1.13e-06
  GPU Memory Used: 12042MB


Evaluation Results:
  Validation Loss: 1.417
  Validation Perplexity: 4.123

Saving model checkpoint: proposal_model/checkpoint-119000


Epoch 239/250
==================================================
[45:43:05] Step 119050/125000:
  Training Loss: 1.313
  Perplexity: 3.716
  Learning Rate: 1.12e-06
  GPU Memory Used: 12008MB

[45:44:13] Step 119100/125000:
  Training Loss: 1.336
  Perplexity: 3.804
  Learning Rate: 1.10e-06
  GPU Memory Used: 11963MB

[45:45:22] Step 119150/125000:
  Training Loss: 1.299
  Perplexity: 3.665
  Learning Rate: 1.08e-06
  GPU Memory Used: 11846MB

[45:46:31] Step 119200/125000:
  Training Loss: 1.363
  Perplexity: 3.907
  Learning Rate: 1.06e-06
  GPU Memory Used: 12035MB

[45:47:40] Step 119250/125000:
  Training Loss: 1.336
  Perplexity: 3.805
  Learning Rate: 1.04e-06
  GPU Memory Used: 12072MB

[45:48:49] Step 119300/125000:
  Training Loss: 1.299
  Perplexity: 3.666
  Learning Rate: 1.02e-06
  GPU Memory Used: 11854MB

[45:49:59] Step 119350/125000:
  Training Loss: 1.351
  Perplexity: 3.861
  Learning Rate: 1.01e-06
  GPU Memory Used: 12133MB

[45:51:09] Step 119400/125000:
  Training Loss: 1.291
  Perplexity: 3.635
  Learning Rate: 9.89e-07
  GPU Memory Used: 11841MB

[45:52:18] Step 119450/125000:
  Training Loss: 1.338
  Perplexity: 3.810
  Learning Rate: 9.71e-07
  GPU Memory Used: 11984MB

[45:53:28] Step 119500/125000:
  Training Loss: 1.323
  Perplexity: 3.753
  Learning Rate: 9.54e-07
  GPU Memory Used: 11837MB


Evaluation Results:
  Validation Loss: 1.468
  Validation Perplexity: 4.342

Saving model checkpoint: proposal_model/checkpoint-119500


Epoch 240/250
==================================================
[45:54:37] Step 119550/125000:
  Training Loss: 1.337
  Perplexity: 3.809
  Learning Rate: 9.37e-07
  GPU Memory Used: 11923MB

[45:55:46] Step 119600/125000:
  Training Loss: 1.384
  Perplexity: 3.991
  Learning Rate: 9.20e-07
  GPU Memory Used: 12000MB

[45:56:56] Step 119650/125000:
  Training Loss: 1.293
  Perplexity: 3.643
  Learning Rate: 9.03e-07
  GPU Memory Used: 12083MB

[45:58:06] Step 119700/125000:
  Training Loss: 1.368
  Perplexity: 3.926
  Learning Rate: 8.86e-07
  GPU Memory Used: 12097MB

[45:59:14] Step 119750/125000:
  Training Loss: 1.335
  Perplexity: 3.801
  Learning Rate: 8.69e-07
  GPU Memory Used: 11887MB

[46:00:26] Step 119800/125000:
  Training Loss: 1.381
  Perplexity: 3.979
  Learning Rate: 8.53e-07
  GPU Memory Used: 11840MB

[46:01:35] Step 119850/125000:
  Training Loss: 1.308
  Perplexity: 3.700
  Learning Rate: 8.36e-07
  GPU Memory Used: 12067MB

[46:02:43] Step 119900/125000:
  Training Loss: 1.345
  Perplexity: 3.838
  Learning Rate: 8.20e-07
  GPU Memory Used: 11906MB

[46:03:52] Step 119950/125000:
  Training Loss: 1.312
  Perplexity: 3.714
  Learning Rate: 8.04e-07
  GPU Memory Used: 11884MB

[46:05:01] Step 120000/125000:
  Training Loss: 1.305
  Perplexity: 3.688
  Learning Rate: 7.89e-07
  GPU Memory Used: 11959MB


Evaluation Results:
  Validation Loss: 1.469
  Validation Perplexity: 4.344

Saving model checkpoint: proposal_model/checkpoint-120000


Epoch 241/250
==================================================
[46:06:11] Step 120050/125000:
  Training Loss: 1.306
  Perplexity: 3.690
  Learning Rate: 7.73e-07
  GPU Memory Used: 11863MB

[46:07:21] Step 120100/125000:
  Training Loss: 1.386
  Perplexity: 3.999
  Learning Rate: 7.57e-07
  GPU Memory Used: 11880MB

[46:08:31] Step 120150/125000:
  Training Loss: 1.318
  Perplexity: 3.736
  Learning Rate: 7.42e-07
  GPU Memory Used: 12165MB

[46:09:40] Step 120200/125000:
  Training Loss: 1.328
  Perplexity: 3.774
  Learning Rate: 7.27e-07
  GPU Memory Used: 11825MB

[46:10:50] Step 120250/125000:
  Training Loss: 1.306
  Perplexity: 3.691
  Learning Rate: 7.12e-07
  GPU Memory Used: 12094MB

[46:12:01] Step 120300/125000:
  Training Loss: 1.367
  Perplexity: 3.923
  Learning Rate: 6.97e-07
  GPU Memory Used: 12103MB

[46:13:11] Step 120350/125000:
  Training Loss: 1.286
  Perplexity: 3.619
  Learning Rate: 6.82e-07
  GPU Memory Used: 12056MB

[46:14:22] Step 120400/125000:
  Training Loss: 1.314
  Perplexity: 3.721
  Learning Rate: 6.68e-07
  GPU Memory Used: 11989MB

[46:15:30] Step 120450/125000:
  Training Loss: 1.347
  Perplexity: 3.845
  Learning Rate: 6.53e-07
  GPU Memory Used: 12184MB

[46:16:41] Step 120500/125000:
  Training Loss: 1.347
  Perplexity: 3.845
  Learning Rate: 6.39e-07
  GPU Memory Used: 12118MB


Evaluation Results:
  Validation Loss: 1.469
  Validation Perplexity: 4.347

Saving model checkpoint: proposal_model/checkpoint-120500


Epoch 242/250
==================================================
[46:17:49] Step 120550/125000:
  Training Loss: 1.317
  Perplexity: 3.731
  Learning Rate: 6.25e-07
  GPU Memory Used: 11942MB

[46:18:58] Step 120600/125000:
  Training Loss: 1.342
  Perplexity: 3.828
  Learning Rate: 6.11e-07
  GPU Memory Used: 12170MB

[46:20:06] Step 120650/125000:
  Training Loss: 1.347
  Perplexity: 3.845
  Learning Rate: 5.97e-07
  GPU Memory Used: 11915MB

[46:21:16] Step 120700/125000:
  Training Loss: 1.329
  Perplexity: 3.779
  Learning Rate: 5.83e-07
  GPU Memory Used: 12166MB

[46:22:23] Step 120750/125000:
  Training Loss: 1.360
  Perplexity: 3.898
  Learning Rate: 5.70e-07
  GPU Memory Used: 12025MB

[46:23:33] Step 120800/125000:
  Training Loss: 1.330
  Perplexity: 3.780
  Learning Rate: 5.57e-07
  GPU Memory Used: 12105MB

[46:24:43] Step 120850/125000:
  Training Loss: 1.352
  Perplexity: 3.866
  Learning Rate: 5.43e-07
  GPU Memory Used: 12158MB

[46:25:52] Step 120900/125000:
  Training Loss: 1.311
  Perplexity: 3.712
  Learning Rate: 5.30e-07
  GPU Memory Used: 12093MB

[46:27:01] Step 120950/125000:
  Training Loss: 1.312
  Perplexity: 3.714
  Learning Rate: 5.18e-07
  GPU Memory Used: 11836MB

[46:28:09] Step 121000/125000:
  Training Loss: 1.297
  Perplexity: 3.659
  Learning Rate: 5.05e-07
  GPU Memory Used: 12037MB


Evaluation Results:
  Validation Loss: 1.503
  Validation Perplexity: 4.496

Saving model checkpoint: proposal_model/checkpoint-121000


Epoch 243/250
==================================================
[46:29:16] Step 121050/125000:
  Training Loss: 1.379
  Perplexity: 3.971
  Learning Rate: 4.92e-07
  GPU Memory Used: 11958MB

[46:30:25] Step 121100/125000:
  Training Loss: 1.326
  Perplexity: 3.764
  Learning Rate: 4.80e-07
  GPU Memory Used: 12102MB

[46:31:32] Step 121150/125000:
  Training Loss: 1.377
  Perplexity: 3.963
  Learning Rate: 4.68e-07
  GPU Memory Used: 11900MB

[46:32:38] Step 121200/125000:
  Training Loss: 1.375
  Perplexity: 3.956
  Learning Rate: 4.56e-07
  GPU Memory Used: 11846MB

[46:33:47] Step 121250/125000:
  Training Loss: 1.332
  Perplexity: 3.789
  Learning Rate: 4.44e-07
  GPU Memory Used: 12166MB

[46:34:56] Step 121300/125000:
  Training Loss: 1.317
  Perplexity: 3.732
  Learning Rate: 4.32e-07
  GPU Memory Used: 11902MB

[46:36:05] Step 121350/125000:
  Training Loss: 1.310
  Perplexity: 3.706
  Learning Rate: 4.20e-07
  GPU Memory Used: 12085MB

[46:37:15] Step 121400/125000:
  Training Loss: 1.373
  Perplexity: 3.946
  Learning Rate: 4.09e-07
  GPU Memory Used: 12161MB

[46:38:24] Step 121450/125000:
  Training Loss: 1.343
  Perplexity: 3.832
  Learning Rate: 3.98e-07
  GPU Memory Used: 11826MB

[46:39:36] Step 121500/125000:
  Training Loss: 1.361
  Perplexity: 3.899
  Learning Rate: 3.87e-07
  GPU Memory Used: 12098MB


Evaluation Results:
  Validation Loss: 1.471
  Validation Perplexity: 4.356

Saving model checkpoint: proposal_model/checkpoint-121500


Epoch 244/250
==================================================
[46:40:45] Step 121550/125000:
  Training Loss: 1.346
  Perplexity: 3.840
  Learning Rate: 3.76e-07
  GPU Memory Used: 12161MB

[46:41:52] Step 121600/125000:
  Training Loss: 1.305
  Perplexity: 3.688
  Learning Rate: 3.65e-07
  GPU Memory Used: 11905MB

[46:43:02] Step 121650/125000:
  Training Loss: 1.349
  Perplexity: 3.853
  Learning Rate: 3.54e-07
  GPU Memory Used: 11960MB

[46:44:11] Step 121700/125000:
  Training Loss: 1.342
  Perplexity: 3.826
  Learning Rate: 3.44e-07
  GPU Memory Used: 12157MB

[46:45:19] Step 121750/125000:
  Training Loss: 1.293
  Perplexity: 3.645
  Learning Rate: 3.33e-07
  GPU Memory Used: 11947MB

[46:46:30] Step 121800/125000:
  Training Loss: 1.383
  Perplexity: 3.985
  Learning Rate: 3.23e-07
  GPU Memory Used: 12183MB

[46:47:40] Step 121850/125000:
  Training Loss: 1.335
  Perplexity: 3.800
  Learning Rate: 3.13e-07
  GPU Memory Used: 11936MB

[46:48:49] Step 121900/125000:
  Training Loss: 1.334
  Perplexity: 3.796
  Learning Rate: 3.03e-07
  GPU Memory Used: 12027MB

[46:49:58] Step 121950/125000:
  Training Loss: 1.340
  Perplexity: 3.818
  Learning Rate: 2.94e-07
  GPU Memory Used: 11810MB

[46:51:08] Step 122000/125000:
  Training Loss: 1.343
  Perplexity: 3.831
  Learning Rate: 2.84e-07
  GPU Memory Used: 11969MB


Evaluation Results:
  Validation Loss: 1.464
  Validation Perplexity: 4.321

Saving model checkpoint: proposal_model/checkpoint-122000


Epoch 245/250
==================================================
[46:52:17] Step 122050/125000:
  Training Loss: 1.352
  Perplexity: 3.864
  Learning Rate: 2.75e-07
  GPU Memory Used: 11895MB

[46:53:28] Step 122100/125000:
  Training Loss: 1.332
  Perplexity: 3.790
  Learning Rate: 2.65e-07
  GPU Memory Used: 12094MB

[46:54:38] Step 122150/125000:
  Training Loss: 1.311
  Perplexity: 3.709
  Learning Rate: 2.56e-07
  GPU Memory Used: 11900MB

[46:55:47] Step 122200/125000:
  Training Loss: 1.334
  Perplexity: 3.797
  Learning Rate: 2.48e-07
  GPU Memory Used: 11910MB

[46:56:55] Step 122250/125000:
  Training Loss: 1.329
  Perplexity: 3.776
  Learning Rate: 2.39e-07
  GPU Memory Used: 12134MB

[46:58:04] Step 122300/125000:
  Training Loss: 1.336
  Perplexity: 3.803
  Learning Rate: 2.30e-07
  GPU Memory Used: 12002MB

[46:59:13] Step 122350/125000:
  Training Loss: 1.331
  Perplexity: 3.785
  Learning Rate: 2.22e-07
  GPU Memory Used: 12011MB

[47:00:24] Step 122400/125000:
  Training Loss: 1.358
  Perplexity: 3.889
  Learning Rate: 2.13e-07
  GPU Memory Used: 12181MB

[47:01:33] Step 122450/125000:
  Training Loss: 1.331
  Perplexity: 3.784
  Learning Rate: 2.05e-07
  GPU Memory Used: 12194MB

[47:02:41] Step 122500/125000:
  Training Loss: 1.341
  Perplexity: 3.823
  Learning Rate: 1.97e-07
  GPU Memory Used: 12030MB


Evaluation Results:
  Validation Loss: 1.431
  Validation Perplexity: 4.184

Saving model checkpoint: proposal_model/checkpoint-122500


Epoch 246/250
==================================================
[47:03:50] Step 122550/125000:
  Training Loss: 1.327
  Perplexity: 3.771
  Learning Rate: 1.90e-07
  GPU Memory Used: 12073MB

[47:04:58] Step 122600/125000:
  Training Loss: 1.318
  Perplexity: 3.737
  Learning Rate: 1.82e-07
  GPU Memory Used: 12161MB

[47:06:06] Step 122650/125000:
  Training Loss: 1.351
  Perplexity: 3.862
  Learning Rate: 1.74e-07
  GPU Memory Used: 12190MB

[47:07:17] Step 122700/125000:
  Training Loss: 1.317
  Perplexity: 3.733
  Learning Rate: 1.67e-07
  GPU Memory Used: 11859MB

[47:08:26] Step 122750/125000:
  Training Loss: 1.323
  Perplexity: 3.754
  Learning Rate: 1.60e-07
  GPU Memory Used: 11952MB

[47:09:34] Step 122800/125000:
  Training Loss: 1.352
  Perplexity: 3.865
  Learning Rate: 1.53e-07
  GPU Memory Used: 12128MB

[47:10:44] Step 122850/125000:
  Training Loss: 1.337
  Perplexity: 3.809
  Learning Rate: 1.46e-07
  GPU Memory Used: 11861MB

[47:11:53] Step 122900/125000:
  Training Loss: 1.330
  Perplexity: 3.780
  Learning Rate: 1.39e-07
  GPU Memory Used: 12107MB

[47:13:01] Step 122950/125000:
  Training Loss: 1.330
  Perplexity: 3.781
  Learning Rate: 1.33e-07
  GPU Memory Used: 11872MB

[47:14:09] Step 123000/125000:
  Training Loss: 1.352
  Perplexity: 3.865
  Learning Rate: 1.26e-07
  GPU Memory Used: 12049MB


Evaluation Results:
  Validation Loss: 1.475
  Validation Perplexity: 4.372

Saving model checkpoint: proposal_model/checkpoint-123000


Epoch 247/250
==================================================
[47:15:17] Step 123050/125000:
  Training Loss: 1.319
  Perplexity: 3.738
  Learning Rate: 1.20e-07
  GPU Memory Used: 12088MB

[47:16:27] Step 123100/125000:
  Training Loss: 1.303
  Perplexity: 3.682
  Learning Rate: 1.14e-07
  GPU Memory Used: 11989MB

[47:17:34] Step 123150/125000:
  Training Loss: 1.297
  Perplexity: 3.658
  Learning Rate: 1.08e-07
  GPU Memory Used: 12077MB

[47:18:44] Step 123200/125000:
  Training Loss: 1.348
  Perplexity: 3.850
  Learning Rate: 1.02e-07
  GPU Memory Used: 12147MB

[47:19:54] Step 123250/125000:
  Training Loss: 1.334
  Perplexity: 3.794
  Learning Rate: 9.67e-08
  GPU Memory Used: 12037MB

[47:21:02] Step 123300/125000:
  Training Loss: 1.353
  Perplexity: 3.869
  Learning Rate: 9.13e-08
  GPU Memory Used: 12198MB

[47:22:13] Step 123350/125000:
  Training Loss: 1.340
  Perplexity: 3.818
  Learning Rate: 8.60e-08
  GPU Memory Used: 11920MB

[47:23:20] Step 123400/125000:
  Training Loss: 1.286
  Perplexity: 3.617
  Learning Rate: 8.08e-08
  GPU Memory Used: 12028MB

[47:24:27] Step 123450/125000:
  Training Loss: 1.333
  Perplexity: 3.792
  Learning Rate: 7.59e-08
  GPU Memory Used: 11800MB

[47:25:39] Step 123500/125000:
  Training Loss: 1.344
  Perplexity: 3.834
  Learning Rate: 7.11e-08
  GPU Memory Used: 12076MB


Evaluation Results:
  Validation Loss: 1.453
  Validation Perplexity: 4.278

Saving model checkpoint: proposal_model/checkpoint-123500


Epoch 248/250
==================================================
[47:26:48] Step 123550/125000:
  Training Loss: 1.326
  Perplexity: 3.766
  Learning Rate: 6.64e-08
  GPU Memory Used: 12195MB

[47:27:58] Step 123600/125000:
  Training Loss: 1.362
  Perplexity: 3.902
  Learning Rate: 6.19e-08
  GPU Memory Used: 12098MB

[47:29:07] Step 123650/125000:
  Training Loss: 1.303
  Perplexity: 3.679
  Learning Rate: 5.76e-08
  GPU Memory Used: 12097MB

[47:30:16] Step 123700/125000:
  Training Loss: 1.328
  Perplexity: 3.774
  Learning Rate: 5.34e-08
  GPU Memory Used: 12091MB

[47:31:25] Step 123750/125000:
  Training Loss: 1.323
  Perplexity: 3.753
  Learning Rate: 4.93e-08
  GPU Memory Used: 12060MB

[47:32:35] Step 123800/125000:
  Training Loss: 1.352
  Perplexity: 3.866
  Learning Rate: 4.55e-08
  GPU Memory Used: 12061MB

[47:33:46] Step 123850/125000:
  Training Loss: 1.355
  Perplexity: 3.877
  Learning Rate: 4.18e-08
  GPU Memory Used: 11914MB

[47:34:54] Step 123900/125000:
  Training Loss: 1.381
  Perplexity: 3.981
  Learning Rate: 3.82e-08
  GPU Memory Used: 12026MB

[47:36:04] Step 123950/125000:
  Training Loss: 1.344
  Perplexity: 3.835
  Learning Rate: 3.48e-08
  GPU Memory Used: 11867MB

[47:37:12] Step 124000/125000:
  Training Loss: 1.328
  Perplexity: 3.772
  Learning Rate: 3.16e-08
  GPU Memory Used: 12158MB


Evaluation Results:
  Validation Loss: 1.451
  Validation Perplexity: 4.269

Saving model checkpoint: proposal_model/checkpoint-124000


Epoch 249/250
==================================================
[47:38:22] Step 124050/125000:
  Training Loss: 1.306
  Perplexity: 3.690
  Learning Rate: 2.85e-08
  GPU Memory Used: 11846MB

[47:39:30] Step 124100/125000:
  Training Loss: 1.364
  Perplexity: 3.911
  Learning Rate: 2.56e-08
  GPU Memory Used: 11919MB

[47:40:40] Step 124150/125000:
  Training Loss: 1.346
  Perplexity: 3.840
  Learning Rate: 2.28e-08
  GPU Memory Used: 12146MB

[47:41:49] Step 124200/125000:
  Training Loss: 1.301
  Perplexity: 3.673
  Learning Rate: 2.02e-08
  GPU Memory Used: 12018MB

[47:42:59] Step 124250/125000:
  Training Loss: 1.302
  Perplexity: 3.678
  Learning Rate: 1.78e-08
  GPU Memory Used: 11916MB

[47:44:08] Step 124300/125000:
  Training Loss: 1.313
  Perplexity: 3.718
  Learning Rate: 1.55e-08
  GPU Memory Used: 11847MB

[47:45:17] Step 124350/125000:
  Training Loss: 1.338
  Perplexity: 3.810
  Learning Rate: 1.33e-08
  GPU Memory Used: 11969MB

[47:46:25] Step 124400/125000:
  Training Loss: 1.328
  Perplexity: 3.775
  Learning Rate: 1.14e-08
  GPU Memory Used: 11801MB

[47:47:35] Step 124450/125000:
  Training Loss: 1.312
  Perplexity: 3.712
  Learning Rate: 9.55e-09
  GPU Memory Used: 11955MB

[47:48:41] Step 124500/125000:
  Training Loss: 1.292
  Perplexity: 3.640
  Learning Rate: 7.90e-09
  GPU Memory Used: 12008MB


Evaluation Results:
  Validation Loss: 1.463
  Validation Perplexity: 4.319

Saving model checkpoint: proposal_model/checkpoint-124500


Epoch 250/250
==================================================
[47:49:50] Step 124550/125000:
  Training Loss: 1.286
  Perplexity: 3.617
  Learning Rate: 6.40e-09
  GPU Memory Used: 11833MB

[47:51:01] Step 124600/125000:
  Training Loss: 1.304
  Perplexity: 3.684
  Learning Rate: 5.05e-09
  GPU Memory Used: 11803MB

[47:52:10] Step 124650/125000:
  Training Loss: 1.319
  Perplexity: 3.741
  Learning Rate: 3.87e-09
  GPU Memory Used: 12152MB

[47:53:18] Step 124700/125000:
  Training Loss: 1.296
  Perplexity: 3.653
  Learning Rate: 2.84e-09
  GPU Memory Used: 12065MB

[47:54:27] Step 124750/125000:
  Training Loss: 1.317
  Perplexity: 3.733
  Learning Rate: 1.97e-09
  GPU Memory Used: 11812MB

[47:55:36] Step 124800/125000:
  Training Loss: 1.298
  Perplexity: 3.663
  Learning Rate: 1.26e-09
  GPU Memory Used: 12036MB

[47:56:45] Step 124850/125000:
  Training Loss: 1.332
  Perplexity: 3.787
  Learning Rate: 7.11e-10
  GPU Memory Used: 11967MB

[47:57:54] Step 124900/125000:
  Training Loss: 1.299
  Perplexity: 3.666
  Learning Rate: 3.16e-10
  GPU Memory Used: 12009MB

[47:59:02] Step 124950/125000:
  Training Loss: 1.310
  Perplexity: 3.708
  Learning Rate: 7.90e-11
  GPU Memory Used: 11905MB

[48:00:14] Step 125000/125000:
  Training Loss: 1.310
  Perplexity: 3.706
  Learning Rate: 0.00e+00
  GPU Memory Used: 11914MB


Evaluation Results:
  Validation Loss: 1.458
  Validation Perplexity: 4.298

Saving model checkpoint: proposal_model/checkpoint-125000


Training completed!
Final Training Loss: 1.310
Final Validation Loss: 1.458
Total Training Time: 48:00:16
